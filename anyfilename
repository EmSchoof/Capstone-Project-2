 1/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Access Directory Structure
import os

# Machine Learning
# scikit-learn k-fold cross-validation
from numpy import array
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
import time 
from sklearn.externals import joblib

# Ploting Decision Boundaries
from matplotlib.colors import ListedColormap

# Scaling
from sklearn.preprocessing import StandardScaler

# Recommending System
%run Recommenders.ipynb
import networkx as nx
 1/2:
# install watermark extension
!pip install --upgrade pip
!pip install watermark
 1/3:
# Use a future note
%load_ext watermark
 1/4:
# Use a future note
%load_ext watermark
 1/5:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Access Directory Structure
import os

# Machine Learning
# scikit-learn k-fold cross-validation
from numpy import array
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
import time 
from sklearn.externals import joblib

# Ploting Decision Boundaries
from matplotlib.colors import ListedColormap

# Scaling
from sklearn.preprocessing import StandardScaler

# Recommending System
%run Recommenders.ipynb
import networkx as nx
 1/6: %watermark -a "Emily Schoof" -d -t -v -p numpy,pandas,seaborn,matplotlib,sklearn -g
 1/7:
# Install requirements.txt
!pip install -r requirements.txt
 1/8:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
 1/9: print(len(beats.columns)), len(beats.Genre.unique()), len(beats)
1/10: print(beats.columns)
1/11: len(beats.columns), len(beats.Genre.unique()), len(beats)
1/12: list(beats.columns)
1/13: beats.info()
1/14: beats = beats.dropna()
1/15: len(beats.columns), len(beats.Genre.unique()), len(beats)
1/16: beats.info()
1/17:
# Test output
beats.info()
1/18:
# Define columns of interest
genres = beats[['Name', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode',
       'Speechness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
       'Tempo','Duration_ms', 'time_signature', 'Genre']].copy()
genres.head()
1/19:
# Create a correlation dataframe
feature_corr = genres.corr()
feature_corr
1/20:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
1/21:
# Reduce dimensions in dataset to match where correlations were detected
genres = genres.drop(columns=['Key','Speechness', 'Mode', 'Instrumentalness', 'Tempo',
                              'Liveness', 'time_signature','Duration_ms'])
genres.head()
1/22:
# Encode categorical features as an integer array
from sklearn import preprocessing
1/23:
# Define columns of interest
genres = beats[['Name', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode',
       'Speechness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
       'Tempo','Duration_ms', 'time_signature', 'Genre']].copy()
genres.head()
1/24:
# Reduce dimensions in dataset to match where correlations were detected
genres = genres.drop(columns=['Name', 'Key','Speechness', 'Mode', 'Instrumentalness', 'Tempo',
                              'Liveness', 'time_signature','Duration_ms'])
genres.head()
1/25:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
print(list(le.classes_))

# Transform column with label encoder
ls.transform(genres.Genre)
1/26:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
#print(list(le.classes_))

# Transform column with label encoder
le.transform(genres.Genre)
1/27:
# Encode categorical integer features as a one-hot numeric array
ohe = preprocessing.OneHotEncoder()
1/28:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
#print(list(le.classes_))

# Transform column with label encoder
int_genres = le.transform(genres.Genre)

# Add to genres dataset
genres['Genre-Int'] = pd.Series(int_genres)
1/29:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
#print(list(le.classes_))

# Transform column with label encoder
int_genres = le.transform(genres.Genre)

# Add to genres dataset
genres['Genre-Int'] = pd.Series(int_genres)
genres.head()
1/30:
# Encode categorical integer features as a one-hot numeric array
ohe = preprocessing.OneHotEncoder(handle_unknown='ignore')

# Fit to encoder
ohe.fit()
1/31:
# Encode categorical integer features as a one-hot numeric array
ohe = preprocessing.OneHotEncoder(handle_unknown='ignore')

# Fit to encoder
ohe.fit(genres['Genre-Int'])
1/32:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
#print(list(le.classes_))

# Transform column with label encoder
int_genres = le.transform(genres.Genre)

# Add to genres dataset
genres['Genre-Int'] = pd.Series(int_genres)
genres.head(), genre.tail()
1/33:
# Define a label encoding instance
le = preprocessing.LabelEncoder()

# Fit to label encoder
le.fit(genres.Genre)
#print(list(le.classes_))

# Transform column with label encoder
int_genres = le.transform(genres.Genre)

# Add to genres dataset
genres['Genre-Int'] = pd.Series(int_genres)
genres.head(), genres.tail()
1/34:
# Reduce dimensions in dataset to match where correlations were detected
genres = genres.drop(columns=['Name', 'Key','Speechness', 'Mode', 'Instrumentalness', 'Tempo',
                              'Liveness', 'time_signature','Duration_ms'])
genres.head()
1/35:
# Define columns of interest
genres = beats[['Name', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode',
       'Speechness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
       'Tempo','Duration_ms', 'time_signature', 'Genre']].copy()
genres.head()
1/36:
# Reduce dimensions in dataset to match where correlations were detected
genres = genres.drop(columns=['Name', 'Key','Speechness', 'Mode', 'Instrumentalness', 'Tempo',
                              'Liveness', 'time_signature','Duration_ms'])
genres.head()
1/37:
# Import necessary modules
from sklearn.preprocessing import OneHotEncoder()
1/38:
# Import necessary modules
from sklearn.preprocessing import OneHotEncoder
1/39:
# Encode categorical integer features as a one-hot numeric array
ohe = OneHotEncoder(handle_unknown='ignore').fit_transform(genres)
ohe
1/40:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
1/41:
# limit to categorical data using df.select_dtypes()
categorical_genres = genres.select_dtypes(include=[object])
categorical_genres.head(3)
1/42:
# Check data shape
categorical_genres.shape()
1/43:
# Check data shape
categorical_genres.shape
1/44:
# Limit to categorical data using df.select_dtypes()
categorical_genres = genres.select_dtypes(include=[object])
categorical_genres.head(3), categorical_genres.shape
1/45:
# Limit to categorical data using df.select_dtypes()
categorical_genres = genres.select_dtypes(include=[object])
categorical_genres.head(3)
1/46:
# Create a LabelEncoder object and fit it to each feature in categorical_genres

# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_genres_2 = categorical_genres.apply(le.fit_transform)
categorical_genres_2.head()
1/47:
# Create a LabelEncoder object and fit it to each feature in categorical_genres

# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_genres_2 = categorical_genres.apply(le.fit_transform)
categorical_genres_2.head(), categorical_genres_2.tail()
1/48:
# Create a OneHotEncoder object, and fit it to categorical_genres_2
ohe = OneHotEncoder(handle_unknown='ignore').fit(categorical_genres_2)

# Transform data
onehotlabels = ohe.transform(categorical_genres_2).toarray()
onehotlabels.shape
1/49:
# Create a OneHotEncoder object, and fit it to categorical_genres_2
ohe = OneHotEncoder(handle_unknown='ignore').fit(categorical_genres_2)

# Transform data
onehotlabels = ohe.transform(categorical_genres_2).toarray()
onehotlabels.shape, onehotlabels
1/50:
# Create a LabelEncoder object and fit it to each feature in categorical_genres

# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_encoded = categorical_genres.apply(le.fit_transform)
categorical_encoded.head(), categorical_encoded.tail()
1/51:
# Create a OneHotEncoder object, and fit it to categorical_encoded

# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(categorical_encoded)

# Transform data
onehotlabels = ohe.transform(categorical_encoded).toarray()
onehotlabels.shape, onehotlabels
1/52:
# Define columns of interest
genres = beats[['Name', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode',
       'Speechness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
       'Tempo','Duration_ms', 'time_signature', 'Genre']].copy()
genres.head()
1/53:
# Limit to categorical data using df.select_dtypes()
categorical_genres = genres.select_dtypes(include=[object])
categorical_genres.head(3)
1/54: beats.head(1)
1/55:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats = beats.infer_objects()
1/56:
# Test output
beats.info()
1/57:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats = beats.astype('int64')
1/58:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats = beats[['Temp', 'time_signature']].astype('int64')
1/59:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats['Temp', 'time_signature'] = beats['Temp', 'time_signature'].astype('int64')
1/60:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats[['Temp', 'time_signature']] = beats[['Temp', 'time_signature']].astype('int64')
1/61:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats['Temp', 'time_signature'] = beats['Temp', 'time_signature'].astype('int64')
1/62:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats['Temp', 'time_signature'] = pd.to_numeric(beats['Temp', 'time_signature'])
1/63:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats['Tempo', 'time_signature'] = pd.to_numeric(beats['Tempo', 'time_signature'])
1/64:
# Drop NaN values
beats = beats.dropna()

# Convert 
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
1/65:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])
1/66:
# Test output
beats.info()
1/67:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['ID','Uri','Ref_Track','URL_features'], axis=1)
1/68:
# Test output
beats.info()
1/69:
# Test output
print(beats.head(1))
beats.info()
1/70:
# Test output
beats.info()
1/71:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
1/72:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
1/73:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
1/74:
# Limit to categorical data using df.select_dtypes()
categorical_genres = genres.select_dtypes(include=[object])
categorical_genres.head(3)
1/75:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.head(3)
1/76:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
1/77:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
1/78:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['ID','Uri','Ref_Track','URL_features','Type'], axis=1)
1/79:
# Test output
beats.info()
1/80:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
1/81:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
1/82:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
1/83:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
1/84:
# Check data shape
categorical_genres.shape
1/85:
# Create a LabelEncoder object and fit it to each feature in categorical_genres

# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_encoded = categorical_genres.apply(le.fit_transform)
categorical_encoded.head(), categorical_encoded.tail()
1/86:
# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(categorical_encoded)

# Transform data
onehotlabels = ohe.transform(categorical_encoded).toarray()
onehotlabels.shape
1/87:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
1/88:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
1/89:
# Check data shape
categorical_genres.shape
1/90:
# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_encoded = categorical_genres.apply(le.fit_transform)
categorical_encoded.head(), categorical_encoded.tail()
1/91:
# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
categorical_encoded = categorical_genres.apply(le.fit_transform)
categorical_encoded.head()
1/92: categorical_encoded.tail()
1/93: onehotlabels
1/94:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
1/95:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
1/96:
# Test output
beats.info()
1/97:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
1/98:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
1/99:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
1/100:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
1/101:
# Check data shape
categorical_genres.shape
1/102:
# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
genres_encoded = categorical_genres.apply(le.fit_transform)
genres_encoded.head()
1/103: genres_encoded.tail()
1/104:
# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(genres_encoded)

# Transform data
onehotlabels = ohe.transform(genres_encoded).toarray()
onehotlabels.shape
1/105: onehotlabels
1/106:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
1/107: list(beats.columns)
1/108: beats.columns
1/109:
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
x = beats.loc[:, features].values

# Separate out the target
y = beats.loc[:,['Genre']].values

# Standardizing the features
x = StandardScaler().fit_transform(x)
1/110:
# Import necessary modules
from sklearn.decomposition import PCA
1/111:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genre']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(x)
1/112:
# Test output
standard_x.head()
1/113:
# Test output
standard_x
1/114:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
1/115:
# Test output
principal_df.head()
1/116:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
print(principal_components)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
1/117:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
1/118:
# Test output
principal_df.head()
1/119:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
1/120:
# Concatenat DataFrame along axis = 1. genre_principals is the final DataFrame before plotting the data
genre_principals = pd.concat([principalDf, beats[['Genre']]], axis = 1)
genre_principals.head()
1/121:
# Concatenat DataFrame along axis = 1. genre_principals is the final DataFrame before plotting the data
genre_principals = pd.concat([principal_df, beats[['Genre']]], axis = 1)
genre_principals.head()
1/122:
# Test output
onehotlabels[1]
1/123:
# Test output
onehotlabels
1/124:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat([principal_df, onehotlabels], axis = 1)
genre_principals.head()
1/125:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat([principal_df, pd.Series(onehotlabels)], axis = 1)
genre_principals.head()
1/126:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame(onehotlabels)
binary_genres.head(1)
1/127:
# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(genres_encoded)

# Transform data
onehotlabels = ohe.transform(genres_encoded)
onehotlabels.shape
1/128:
# Test output
onehotlabels
1/129:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame(onehotlabels)
binary_genres.head(1)
1/130:
# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(genres_encoded)

# Transform data
onehotlabels = ohe.transform(genres_encoded).toarray()
onehotlabels.shape
1/131:
# Test output
onehotlabels
1/132:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame(onehotlabels)
binary_genres.head(1)
1/133:
# Import necessary modules
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
1/134:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
1/135:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat([principal_df, beats[['Genre']]], axis = 1)
genre_principals.head()
1/136: genre_principals.Genre.unique()
1/137:
# Determine the numbers of Genre
list(genre_principals.Genre.unique())
1/138: genre_principals.dropna()
1/139: genre_principals = genre_principals.dropna()
1/140:
# Determine the numbers of Genre
list(genre_principals.Genre.unique())
1/141:
# Determine the numbers of Genre
lenght(list(genre_principals.Genre.unique()))
1/142:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()))
1/143:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['target'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal component 1'], 
               genre_principals.loc[indices_to_keep, 'principal component 2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/144:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal component 1'], 
               genre_principals.loc[indices_to_keep, 'principal component 2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/145:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/146:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()[:,5]))
1/147:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()[: 5]))
1/148:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()[:5]))
1/149:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()[:,5]))
1/150:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()))
1/151:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()[]
colors = ['r', 'g', 'b', 'o', 'y']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/152:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b', 'o', 'y']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/153:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b', 'y']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/154:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/155:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'orange', 'yellow', 'g', 'b', 'purple', 'pink']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/156:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 50)
ax.legend(targets)
ax.grid()
1/157:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 1000)
ax.legend(targets)
ax.grid()
1/158:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 10)
ax.legend(targets)
ax.grid()
1/159:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 20)
ax.legend(targets)
ax.grid()
1/160:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 30)
ax.legend(targets)
ax.grid()
1/161: pca.explained_variance_ratio_
1/162:
train_data, test_data = train_test_split(genres, test_size=0.20, random_state=0)
train_data.head(5)
1/163:
train_data, test_data = train_test_split(beats, test_size=0.20, random_state=0)
train_data.head(5)
1/164:
train_data, test_data = train_test_split(genre_principals, test_size=0.20, random_state=0)
train_data.head(5)
1/165:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['Genre'])

# The response variable
y = train_data.copy().pop('Genre')
1/166:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
1/167:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['Genre'])

# The response variable
y = train_data.copy().pop('Genre')
1/168:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
1/169:
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel().reshape(-1, 1), xx2.ravel().reshape(-1, 1)]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=Y[ y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
1/170:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
1/171:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
1/172:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
1/173:
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel().reshape(-1, 1), xx2.ravel().reshape(-1, 1)]))
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=Y[ y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
1/174:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
1/175:
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=Y[ y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
1/176:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
1/177:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
 2/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
 2/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
 2/3:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
 2/4:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
 2/5:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
 2/6:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
 2/7:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
 2/8:
# Check data shape
categorical_genres.shape
 2/9:
# Encode labels with value between 0 and n_classes-1
le = preprocessing.LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
genres_encoded = categorical_genres.apply(le.fit_transform)
genres_encoded.head()
2/10:
# Encode labels with value between 0 and n_classes-1
le = LabelEncoder()

# Fit/Transform data user df.apply() on le.fit_transform to all columns
genres_encoded = categorical_genres.apply(le.fit_transform)
genres_encoded.head()
2/11: genres_encoded.tail()
2/12:
# Encode categorical integer features and fit to data
ohe = OneHotEncoder(handle_unknown='ignore').fit(genres_encoded)

# Transform data
onehotlabels = ohe.transform(genres_encoded).toarray()
onehotlabels.shape
2/13:
# Test output
onehotlabels
2/14:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame(onehotlabels)
binary_genres.head(1)
2/15:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
2/16: beats.columns
2/17:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genre']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(x)
2/18:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genre']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
2/19:
# Test output
standard_x
2/20:
# Import necessary modules
from sklearn.decomposition import PCA
2/21:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
2/22:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat([principal_df, beats[['Genre']]], axis = 1)
genre_principals.head()
2/23: genre_principals = genre_principals.dropna()
2/24:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()))
2/25:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 30)
ax.legend(targets)
ax.grid()
2/26:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat([principal_df, binary_genres, axis = 1)
genre_principals.head()
2/27:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat(principal_df, binary_genres, axis = 1)
genre_principals.head()
2/28:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat(principal_df, binary_genres)
genre_principals.head()
2/29:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame(columns=onehotlabels)
binary_genres.head(1)
2/30:
# Convert onehotlabels to dataframe
binary_genres = pd.DataFrame()
binary_genres['vector_genres'] = pd.Series(onehotlabels)
binary_genres.head(1)
 3/1: import pyAudioAnalysis
 3/2:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 3/3:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 4/1:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 3/4: !pip3
 3/5: ls
 3/6:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 3/7:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 5/1:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 6/1:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 6/2:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 7/1:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 8/1:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 8/2:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 8/3:
# install watermark extension
!pip install --upgrade pip
!pip install watermark
 8/4: %watermark -a "Emily Schoof" -d -t -v -p matplotlib,pyAudioAnalysis,soundscrape,libmagic,FFMPEG -g
 8/5:
# Use a future note
%load_ext watermark
%watermark -a "Emily Schoof" -d -t -v -p matplotlib,pyAudioAnalysis,soundscrape,libmagic,FFMPEG -g
 8/6:
from pydub import AudioSegment
from pydub.playback import play

sound = AudioSegment.from_file("music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3", format="mp3")
play(sound)
 9/1:
# Load necessary modules
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
 9/2:
from pydub import AudioSegment
from pydub.playback import play

sound = AudioSegment.from_file("music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3", format="mp3")
play(sound)
 9/3:
# Load necessary modules
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
10/1:
# Load necessary modules
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
10/2:
from pydub import AudioSegment
from pydub.playback import play

sound = AudioSegment.from_file("music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3", format="mp3")
play(sound)
10/3:
from pydub import AudioSegment
from pydub.playback import play
10/4:
# Load necessary modules
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
10/5:
from pydub import AudioSegment
from pydub.playback import play
10/6:

sound = AudioSegment.from_file("music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3", format="mp3")
play(sound)
10/7:
sound = 

from os import path
from pydub import AudioSegment

# files                                                                         
src = "music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3")
dst = "test.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")
10/8:
from os import path
from pydub import AudioSegment

# files                                                                         
src = "music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3")
dst = "test.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")
10/9:
from os import path
from pydub import AudioSegment

# files                                                                         
src = "music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3"
dst = "test.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")
10/10: ls
10/11:
import subprocess

subprocess.call(['ffmpeg', '-i', '/input/music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3',
                   '/output/file.wav'])
11/1:
from pydub import AudioSegment
from pydub.playback import play
11/2:
import subprocess

subprocess.call(['ffmpeg', '-i', '/input/music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3',
                   '/output/file.wav'])
11/3:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/4:
from pydub import AudioSegment
from pydub.playback import play
11/5:
import subprocess
subprocess.call(['ffmpeg', '-i', '/input/music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3',
                   '/output/file.wav'])
11/6:
import matplotlib.pyplot as plt

[Fs, x] = audioBasicIO.readAudioFile("sample.wav")

F, f_names = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs)

plt.subplot(2,1,1); plt.plot(F[0,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[0])

plt.subplot(2,1,2); plt.plot(F[1,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[1])

plt.show()
11/7:
from os import listdir
from os.path import isfile, join

mypath = "music_data"

mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
11/8:
# Import mp3 music file
mypath = "music_data"
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
mp3_files
11/9:
# Import mp3 music file
mypath = "music_data"
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
11/10:
# Import necessary modules
import os
import pandas as pd
11/11:
# Read mp3 list files in pandas then concat together in a dataframe

#concatenate them together
big_df = pd.concat(mp3_list)
11/12:
# Read mp3 list files in pandas then concat together in a dataframe

#concatenate them together
big_df = pd.concat(mp3_files)
11/13: # Read mp3 list files in pandas then concat together in a dataframe
11/14:
# files                                                                         
src = "transcript.mp3"
dst = "test.wav"

# convert wav to mp3 

for file in mp3_file:
    print(file)

#sound = AudioSegment.from_mp3(src)
#sound.export(dst, format="wav")
11/15:
# files                                                                         
src = "transcript.mp3"
dst = "test.wav"

# convert wav to mp3 

for file in mp3_files:
    print(file)

#sound = AudioSegment.from_mp3(src)
#sound.export(dst, format="wav")
11/16:
# files                                                                         
src = "transcript.mp3"
dst = "test.wav"

wav_files = []
# convert wav to mp3 

for file in mp3_files:
    sound = AudioSegment.from_mp3(src)
    wav_form = "new.wav"
    sound.export(dst, format="wav")
    print(sound)
11/17:
# files                                                                         
src = "transcript.mp3"
dst = "test.wav"

wav_files = []
# convert wav to mp3 

for file in mp3_files:
    sound = AudioSegment.from_mp3(file)
    wav_form = "new.wav"
    sound.export(dst, format="wav")
    print(sound)
11/18:
# Import mp3 music files
mypath = "music_data"
mypath
11/19:
# files                                                                         


wav_files = []
# convert wav to mp3 

for file in mp3_files:
    sound = AudioSegment.from_mp3(file)
    wav_form = "new.wav"
    sound.export(dst, format="wav")
    print(sound)
11/20:
# Import mp3 music files
directory = "music_data" # music source Directory
d = path(directory) # path

mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
11/21:
# Import necessary modules
import os
from os import listdir
from os.path import isfile, join
11/22:
# Import mp3 music files
directory = "music_data" # music source Directory
d = path(directory) # path

mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
11/23:
# Import necessary modules
import os
from path import path
from os import listdir
from os.path import isfile, join
11/24:
# Import necessary modules
import os
import path
from os import listdir
from os.path import isfile, join
11/25:
# Import mp3 music files
directory = "music_data" # music source Directory
d = path(directory) # path

mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
11/26:
# Import necessary modules
import os
from path import path
from os import listdir
from os.path import isfile, join
11/27:
# Import necessary modules
from os import listdir
from os.path import isfile, join
11/28:
# Import mp3 music files
directory = "music_data" # music source Directory
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
11/29:
# Import mp3 music files
directory = "music_data" # music source Directory
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
mp3_files
11/30:
# Import mp3 music files
directory = "music_data/Tony H 🙌 - The Drive-Thru 045 -- Miss Min.D.mp3" # music source Directory
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
directory
11/31:
# files                                                                         


wav_files = []
# convert wav to mp3 

for file in directory:
    sound = AudioSegment.from_mp3(file)
    wav_form = "new.wav"
    sound.export(dst, format="wav")
    print(sound)
11/32:
# files                                                                         


wav_files = []
# convert wav to mp3 

for file in directory:
    print(file)
    #sound = AudioSegment.from_mp3(file)
    #wav_form = "new.wav"
    #sound.export(dst, format="wav")
    #print(sound)
11/33:
# files                                                                         


wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
11/34:
# Load necessary modules
import ffmpeg
import ffprobe
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/35:
# Load necessary modules
import ffmpeg
from ffmpeg import ffprobe
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/36:
# Load necessary modules
import ffmpeg
from .ffprobe ...
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/37:
# Load necessary modules
import ffmpeg
from .ffprobe import ffprobe
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/38:
# Load necessary modules
import ffmpeg
from ffprobe import ffprobe
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/39:
# Load necessary modules
import ffmpeg
from .ffprobe import FFProbe
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
11/40:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
12/1:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
12/2:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
12/3:
from pydub import AudioSegment
from pydub.playback import play
12/4:
from pydub import AudioSegment
from pydub.playback import play
12/5:
# Import necessary modules
from os import listdir
from os.path import isfile, join
12/6:
# Import mp3 music files
directory = "music_data/Tony H 🙌 - The Drive-Thru 045 -- Miss Min.D.mp3" # music source Directory
mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
directory
12/7:
# Import mp3 music files
directory = "music_data/Tony H 🙌 - The Drive-Thru 045 -- Miss Min.D.mp3" # music source Directory
#mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
directory
12/8:
# Import necessary modules
import os
import pandas as pd
12/9: # Read mp3 list files in pandas then concat together in a dataframe
12/10:
# files                                                                         


wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
12/11:
# files                                                                         
from .ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
12/12:
# files                                                                         
from __init__.ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
12/13:
# files                                                                         
from ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
12/14:
# files                                                                         
from .ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/1:
# files                                                                         
from .ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/2:
# files                                                                         
from ffprobe import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/3:
# files                                                                         
from ffprobes3.exceptions import FFProbeError

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/4:
# files                                                                         
from ffprobes3.exceptions import FFProbeError

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/5:
# files                                                                         
from ffprobes3 import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/6:
# files                                                                         
from ffprobe3 import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
13/7:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
13/8:
# Load necessary modules
import ffmpeg
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
13/9:
from pydub import AudioSegment
from pydub.playback import play
13/10:
from pydub import AudioSegment
from pydub.playback import play
13/11: ls
13/12:
# Import necessary modules
from os import listdir
from os.path import isfile, join
13/13:
# Import mp3 music files
directory = "music_data/Tony H 🙌 - The Drive-Thru 045 -- Miss Min.D.mp3" # music source Directory
#mp3_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]
#mp3_files
directory
13/14:
# Import necessary modules
import os
import pandas as pd
13/15: # Read mp3 list files in pandas then concat together in a dataframe
13/16:
# files                                                                         
from ffprobe3 import FFProbe

wav_files = []
# convert wav to mp3 


sound = AudioSegment.from_mp3(directory)
wav_form = "new.wav"
sound.export(dst, format="wav")
print(sound)
14/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
14/2:
# Load json dataset
data = pd.read_json(ultimate_data_challenge.json)
data.head()
14/3:
# Load json dataset
data = pd.read_json('ultimate_data_challenge.json')
data.head()
14/4:
# Load json dataset
data = pd.read_json('ultimate_data_challenge.json', orient='split')
data.head()
14/5:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import json
14/6:
# Load json dataset
with open('ultimate_data_challenge.json') as f:
   data = json.load(f)
print data
14/7:
# Load json dataset
with open('ultimate_data_challenge.json') as f:
   data = json.load(f)
print(data)
15/1:
# Load json dataset
with open('logins.json') as f:
   data = json.load(f)
print data
15/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import json
15/3:
# Load json dataset
with open('logins.json') as f:
   data = json.load(f)
print(data)
15/4:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)
data.head()
15/5:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)
data
15/6:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)
15/7: data.info()
15/8: data
15/9: data[0:5]
15/10: data[0]
15/11: data.items()
15/12:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)
data.type()
15/13:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)
type(data)
15/14:
keys = dishes.keys()
print(keys)
values = dishes.values()
print(values)
15/15:
# Assess keys and values of data dictionary
keys = data.keys()
print(keys)
values = data.values()
print(values)
15/16:
# Assess keys and values of data dictionary
keys = data.keys()
print(keys)
values = data.values()
print(values[1])
15/17:
# Convert dict data to dataframe
data_df = pd.DataFrame.from_dict(data)
data_df.head()
15/18:
# Label Dataframe columns
data_df.columns = ['date', 'login_time']
data_df.head()
15/19:
# Assess dataframe
data_df.info()
15/20:
# Split column values into dates and login_time
from dateutil import parser
date_time = []

for date in data_df['login_time']:
    time_series = parser.parse(date)
    date_time.append(time_series)
15/21:
# Test output
date_time
15/22:
# Test output
#date_time
15/23:
# Add time series column to data_df
data_df['time_series_login'] = pd.Series(date_time)
data_df.head()
15/24:
# Assess the dataframe
data_df.info()
15/25:
# Create new dataframe for time_series
login_data['time_series_login'] = pd.Series(date_time)
login_data.head()
15/26:
# Create new dataframe for time_series
login_data = = pd.DataFrame()
login_data['time_series_login'] = pd.Series(date_time)
login_data.head()
15/27:
# Create new dataframe for time_series
login_data = pd.DataFrame()
login_data['time_series_login'] = pd.Series(date_time)
login_data.head()
15/28:
# Assess the dataframe
login_data.info()
15/29:
# Use dates and times as indices to intuitively organize and access data 
from pandas_datareader import data
15/30:
# Use dates and times as indices to intuitively organize and access data 
conda install pandas-datareader
from pandas_datareader import data
15/31:
# Use dates and times as indices to intuitively organize and access data 
from pandas_datareader import data
15/32:
# Visualize Data
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
15/33: login_data.plot()
15/34:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
15/35:
# Visualize Data
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style="darkgrid")
15/36: login_data.plot()
15/37:
# Determine number of days in dataset
login_data.date
15/38:
# Determine number of days in dataset
login_data.time_series_login.date
15/39:
# Determine number of days in dataset
for date in login_data.time_series_login:
    print(date)
15/40:
# Determine number of days in dataset
for datetime in login_data.time_series_login:
    date = datetime.date
    print(date)
15/41:
# Determine number of days in dataset
for datetime in login_data.time_series_login:
    print(datetime)
15/42: # Determine number of days in dataset
15/43:
for datetime in login_data.time_series_login:
    datetime.plot()
15/44:
# Split login_time strings in to date and time columns
for time in data_df.login_time:
    print(time)
15/45:
# Split login_time strings in to date and time columns
for time in data_df.login_time:
    new_time = time.split(' ', 1)
    print(new_time)
15/46:
# Split login_time strings in to date and time columns
date = []
time = []

for time in data_df.login_time:
    new_time = time.split(' ', 1)
    date = new_time[0]
    time = new_time[1]
    print(date, time)
15/47:
# Split login_time strings in to date and time columns
date_list = []
time_time = []

for time in data_df.login_time:
    new_time = time.split(' ', 1) #split strings
    date = new_time[0] #create date
    date_list.append(data)#append to date list
    time = new_time[1] #create time
    time_list.append(time)#append to time list
15/48:
# Split login_time strings in to date and time columns
date_list = []
time_list = []

for time in data_df.login_time:
    new_time = time.split(' ', 1) #split strings
    date = new_time[0] #create date
    date_list.append(data)#append to date list
    time = new_time[1] #create time
    time_list.append(time)#append to time list
15/49:
# Test output
date_list
15/50:
# Split login_time strings in to date and time columns
date_list = []
time_list = []

for time in data_df.login_time:
    new_time = time.split(' ', 1) #split strings
    date = new_time[0] #create date
    date_list.append(date)#append to date list
    time = new_time[1] #create time
    time_list.append(time)#append to time list
15/51:
# Test output
date_list
15/52:
# Test output
#date_list
time_list
15/53:
# Test output
#date_list
#time_list
15/54:
# Add date and time columns
data_df['date'] = pd.Series(date_list)
data_df['time'] = pd.Series(time_list)
data_df.head()
15/55:
# Convert dict data to dataframe
data_df = pd.DataFrame.from_dict(data)
data_df.head()
15/56:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)

# Define Data Type
type(data)
15/57:
# Convert dict data to dataframe
data_df = pd.DataFrame.from_dict(data)
data_df.head()
15/58:
# Assess the dataframe
data_df.info()
15/59:
# Split login_time strings in to date and time columns
date_list = []
time_list = []

for time in data_df.login_time:
    new_time = time.split(' ', 1) #split strings
    date = new_time[0] #create date
    date_list.append(date)#append to date list
    time = new_time[1] #create time
    time_list.append(time)#append to time list
15/60:
# Test output
#date_list
#time_list
15/61:
# Add date and time columns
data_df['date'] = pd.Series(date_list)
data_df['time'] = pd.Series(time_list)
data_df.head()
15/62:
# Drop login_time
data_df = data_df.drop(columns='login_time')
data_df.head()
15/63:
# Define the type of objects within the date and time columns
data_df.date[0].dtype, data_df.time[0].dtype
15/64:
# Define the type of objects within the date and time columns
data_df.date.dtype, data_df.time.dtype
15/65:
# Create a general function that converts a column's entries to Datetime objects
def column_val_to_datetime(df, col):
    
    #define the column vars
    col_1 = df.col
    col_name = str(col) + ' Datetime'
    
    #define list vars for column datetime
    datetime = []
    
    #iternate over each column val
    for date in col_1:
        
        #convert entry to datetime object
        to_datetime = parser.parse(date)
        
        #append to column datetime list
        datetime.append(to_datetime)
    
    # convert datetime list to Series and add Series to df as column
    df[col_name] = pd.Series(datetime)
15/66:
# Import necessary modules
from dateutil import parser
15/67:
# Create a general function that converts a column's entries to Datetime objects
def column_val_to_datetime(df, col):
    
    #define the column vars
    col_1 = df.col
    col_name = str(col) + ' Datetime'
    
    #define list vars for column datetime
    datetime = []
    
    #iternate over each column val
    for date in col_1:
        
        #convert entry to datetime object
        to_datetime = parser.parse(date)
        
        #append to column datetime list
        datetime.append(to_datetime)
    
    # convert datetime list to Series and add Series to df as column
    df[col_name] = pd.Series(datetime)
15/68:
# Use function on date and time columns
column_val_to_datetime(data_df, date)
15/69:
# Use function on date and time columns
column_val_to_datetime(data_df, df.date)
15/70:
# Use function on date and time columns
column_val_to_datetime(data_df, data_df.date)
15/71:
# Create a general function that converts a column's entries to Datetime objects
def column_val_to_datetime(df, df.col):
    
    #define the column vars
    col_name = str(df.col) + ' Datetime'
    
    #define list vars for column datetime
    datetime = []
    
    #iternate over each column val
    for date in col_1:
        
        #convert entry to datetime object
        to_datetime = parser.parse(date)
        
        #append to column datetime list
        datetime.append(to_datetime)
    
    # convert datetime list to Series and add Series to df as column
    df[col_name] = pd.Series(datetime)
15/72:
# Create a general function that converts a column's entries to Datetime objects
def column_val_to_datetime(df, col):
    
    #define the column vars
    col_1 = df[col]
    col_name = str(col) + ' Datetime'
    
    #define list vars for column datetime
    datetime = []
    
    #iternate over each column val
    for date in col_1:
        
        #convert entry to datetime object
        to_datetime = parser.parse(date)
        
        #append to column datetime list
        datetime.append(to_datetime)
    
    # convert datetime list to Series and add Series to df as column
    df[col_name] = pd.Series(datetime)
15/73:
# Use function on date and time columns
column_val_to_datetime(data_df, 'date')
15/74:
# Create a function that converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)
15/75:
# Test output
to_datetime('2014-02-12')
15/76:
# Test output
to_datetime('2014-02-12'), to_datetime('20:16:10')
15/77:
# Convert dict data to dataframe
data_df = pd.DataFrame.from_dict(data)
data_df.head()
15/78:
# Assess the dataframe
data_df.info()
15/79:
# Define the type of objects within the login_time column
data_df.login_time.dtype
15/80:
# Test output
value = data_df.login_time[0]
to_datetime(value)
15/81:
# Test output
li = [1,2,3,4,5]
list_to_series(li)
15/82:
# Create a general function that converts a column's entries to Datetime objects
def list_to_series(li):
    return pd.Series(li)
15/83:
# Test output
li = [1,2,3,4,5]
list_to_series(li)
15/84:
# Create a general function that converts all columns to datetime and adds new column to df
def datetime_column(df, col):
    
    #define datetime list
    datetime_list = []
    
    for date in col:
        value = to_datetime(date)
        datetime_list.append(value)
    
    return df['new_col'] = list_to_series(datetime)
15/85:
# Create a general function that converts all columns to datetime and adds new column to df
def datetime_column(df, col):
    
    #define datetime list
    datetime_list = []
    
    for date in col:
        value = to_datetime(date)
        datetime_list.append(value)
    
    df['new_col'] = list_to_series(datetime)
    return df
15/86:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
    #define datetime list
    datetime_list = []
    
    for date in col:
        value = to_datetime(date)
        datetime_list.append(value)
    
    df['new_col'] = list_to_series(datetime)
    return df
15/87:
# Test output
create_datetime_column(data_df, login_data)
15/88:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    print(col)
    #define datetime list
    datetime_list = []
    
    for date in col:
        print(date)
        value = to_datetime(date)
        datetime_list.append(value)
    
    df['new_col'] = list_to_series(datetime)
    return df
15/89:
# Test output
create_datetime_column(data_df, login_data)
15/90:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    print(col)
    #define datetime list
    datetime_list = []
    
    for idx in col.index:
        print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/91:
# Test output
create_datetime_column(data_df, login_data)
15/92:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    print(col)
    #define datetime list
    datetime_list = []
    
    for idx in df.index:
        print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/93:
# Test output
create_datetime_column(data_df, login_data)
15/94:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
     #define datetime list
    datetime_list = []
    
    #select column data
    col_data = df.loc[ : , 'col']
    
    for idx in df.index:
        print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/95:
# Test output
create_datetime_column(data_df, login_data)
15/96:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
     #define datetime list
    datetime_list = []
    
    #select column data
    col_data = df.loc[ : , col]
    
    for idx in df.index:
        print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/97:
# Test output
create_datetime_column(data_df, login_data)
15/98:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
     #define datetime list
    datetime_list = []
    
    #select column data
    col_data = df.loc[ : , str(col)]
    
    for idx in df.index:
        print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/99:
# Test output
create_datetime_column(data_df, login_data)
15/100:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
     #define datetime list
    datetime_list = []
    
    #select column data
    col_data = df.loc[ : , str(col)]
    print(col_data)
    #for idx in df.index:
        #print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    return df
15/101:
# Test output
create_datetime_column(data_df, login_data)
15/102:
# Create a general function that converts all columns to datetime and adds new column to df
def create_datetime_column(df, col):
    
     #define datetime list
    datetime_list = []
    
    #select column data
    col_data = df.loc[ : , str(col)]
    return col_data
    #for idx in df.index:
        #print(idx)
        #value = to_datetime(date)
        #datetime_list.append(value)
    
    #df['new_col'] = list_to_series(datetime)
    #return df
15/103:
# iterate over login_time column
login_datetime = []

for date in data_df.login_time:
    print(date)
15/104:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/105:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/106:
# Test output (1)
value = data_df.login_time[0]
to_datetime(value)
15/107:
# Test output (2)
val = 6
li = [1,2,3,4,5]
add_to_list(li, val)
15/108:
# Test output (3)
list_to_series(li)
15/109:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/110:
# iterate over login_time column
login_datetime = []

for date in data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # append to list
    add_to_list(login_datetime, datetime)
15/111:
# Test output
login_datetime[0]
15/112:
# Test output
login_datetime
15/113:
# Test output
#login_datetime
15/114:
# Add datetime values to data_df
data_df['login_datetime'] = list_to_series(login_datetime)
15/115:
# Add datetime values to data_df
data_df['login_datetime'] = list_to_series(login_datetime)
data_df.head()
15/116:
# Add datetime values to data_df
data_df['login_datetime'] = list_to_series(login_datetime)

# Drop login_time column
data_df = data_df.drop(columns='login_time')
data_df.head()
15/117: data_df.plot()
15/118:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/119:
# Assess the dataframe
login_data_df.info()
15/120:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/121:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/122:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/123:
# iterate over login_time column
login_datetime = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # append to list
    add_to_list(login_datetime, datetime)
15/124:
# Add datetime values to data_df
login_data_df['login_datetime'] = list_to_series(login_datetime)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
15/125: login_data_df.plot()
15/126: login_datetime[0].time
15/127: login_datetime.time
15/128: login_datetime[0]
15/129: login_datetime[0].time
15/130: login_datetime[0].date
15/131: login_datetime[0].year
15/132: login_datetime[0].hour
15/133: login_datetime[0].minute
15/134: login_datetime[0].second
15/135: login_datetime.tail()
15/136: login_data_df.tail()
15/137: login_datetime[0].date.values
15/138: login_datetime[0].values
15/139: login_datetime[0].date
15/140: login_datetime[0].time
15/141:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax= plt.subplots()

# add the x-axis and the y-axis to the plot
ax.plot(x=login_datetime.time, 
        y=login_datetime.year, 
        color = 'red')

# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Temperature (Fahrenheit)",
       title="Precipitation\nBoulder, Colorado in July 2018");
15/142:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax= plt.subplots()

# add the x-axis and the y-axis to the plot
ax.plot(x=login_datetime.hour, 
        y=login_datetime.year, 
        color = 'red')

# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Temperature (Fahrenheit)",
       title="Precipitation\nBoulder, Colorado in July 2018");
15/143: login_datetime.time
15/144: login_data_df.login_datetime.time
15/145:
# iterate over login_time column
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/146:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/147:
# Assess the dataframe
login_data_df.info()
15/148:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/149:
# Import necessary modules
from dateutil import parser
15/150:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/151:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/152:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/153:
# iterate over login_time column
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/154:
# Test output
login_yr
15/155:
# Test output
#login_yr
15/156:
# Add datetime values to data_df
login_data_df['login_year'] = list_to_series(login_yr)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
15/157: login_data_df.tail()
15/158:
# Drop login_year column
login_data_df = login_data_df.drop(columns='login_year')
login_data_df.head()
15/159:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax= plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
axs[0, 0].plot(x=login_day, 
        y=login_month, 
        color = 'red')
#hour by day
axs[0, 1].plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
axs[1, 0].plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
axs[0, 0].set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
axs[0, 1].set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
axs[0, 1].set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/160:
# Visualize Data

# create the plot space upon which to plot the data
fig, axs = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
axs[0, 0].plot(x=login_day, 
        y=login_month, 
        color = 'red')
#hour by day
axs[0, 1].plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
axs[1, 0].plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
axs[0, 0].set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
axs[0, 1].set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
axs[0, 1].set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/161:
# Visualize Data

# create the plot space upon which to plot the data
fig, axs = plt.subplots(2,2)

# add the x-axis and the y-axis to the plot

#day by month
axs[0, 0].plot(x=login_day, 
        y=login_month, 
        color = 'red')
#hour by day
axs[0, 1].plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
axs[1, 0].plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
axs[0, 0].set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
axs[0, 1].set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
axs[0, 1].set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/162:
# Visualize Data

# create the plot space upon which to plot the data
fig, axs = plt.subplots(2,2)

# add the x-axis and the y-axis to the plot

#day by month
axs[0, 0].plot(x=login_day, 
        y=login_mon, 
        color = 'red')
#hour by day
axs[0, 1].plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
axs[1, 0].plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
axs[0, 0].set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
axs[0, 1].set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
axs[0, 1].set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/163:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1, ax2, ax3 = plt.subplots(2,2)

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_day, 
        y=login_mon, 
        color = 'red')
#hour by day
ax2.plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
ax3.plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
ax2.set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
ax3.set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/164:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1, ax2, ax3 = plt.subplots(4)

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_day, 
        y=login_mon, 
        color = 'red')
#hour by day
ax2.plot(x=login_hr, 
        y=login_day, 
        color = 'blue')
#minute by hour
ax3.plot(x=login_min, 
        y=login_hr, 
        color = 'green')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
#hour by day
ax2.set(xlabel="Hour",
       ylabel="Day",
       title="Trend of Customer Login by Hour for Each Day of the Week")
#minute by hour
ax3.set(xlabel="Minute",
       ylabel="Day",
       title="Trend of Customer Login by Minute for Each Hour of the Day")
15/165:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots(4)

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_day, 
        y=login_mon, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/166:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_day, 
        y=login_mon, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/167:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(data=login_data_df, x=login_day, 
        y=login_mon, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/168:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_day, 
        y=login_mon, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/169:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_data_df.login_day, 
        y=login_data_df.login_mon, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/170:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_data_df.login_day, 
        y=login_data_df.login_month, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/171:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_data_df.login_month, 
        y=login_data_df.login_hr, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/172:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_hr')
15/173:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
15/174:
# Visualize Data

# create the plot space upon which to plot the data
fig, ax1 = plt.subplots()

# add the x-axis and the y-axis to the plot

#day by month
ax1.plot(x=login_data_df.login_month, 
        y=login_data_df.login_day, 
        color = 'red')


# rotate tick labels
plt.setp(ax.get_xticklabels(), rotation=45)

# set title and labels for axes

#day by month
ax1.set(xlabel="Day",
       ylabel="Month",
       title="Trend of Customer Login by Day for Each Calendar Month")
15/175: login_data_df.plot(x='login_day', y='login_hr')
15/176:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(x), max(x)+1, 1.0))
15/177:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month), max(login_data_df.login_month)+1, 1.0))
15/178:
login_data_df.plot(x='login_day', y='login_hr')
plt.xticks(np.arange(min(login_data_df.login_day), max(login_data_df.login_day)+1, 1.0))
15/179:
login_data_df.plot(x='login_day', y='login_hr')
plt.xticks(np.arange(min(login_data_df.login_day), max(login_data_df.login_day)+1, 1.0))
plt.show()
15/180:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month), max(login_data_df.login_month)+1, 1.0))
plt.show()
15/181: max(login_data_df.login_day)
15/182:
login_data_df.plot(x='login_day', y='login_hr')
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+2, 1.0))
plt.show()
15/183:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+2, 1.0))
plt.show()
15/184:
# Basic Visualization of Data
login_data_df.plot(x='login_day', y='login_month')
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+1, 1.0))
plt.show()
15/185:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1, 1.0))
plt.show()
15/186:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+2, 1.0))
plt.show()
15/187:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month)-0.5, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/188:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/189:
login_data_df.plot(x='login_day', y='login_hr')
plt.yticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+1, 1.0))
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+2, 1.0))
plt.show()
15/190:
login_data_df.plot(x='login_day', y='login_hr')
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+2, 1.0))
plt.show()
15/191:
login_data_df.plot(x='login_hr', y='login_day')
plt.xticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+2, 1.0))
plt.show()
15/192:
login_data_df.plot(x='login_hr', y='login_minute')
plt.xticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+2, 1.0))
plt.show()
15/193:
login_data_df.plot(x='login_hr', y='login_min')
plt.xticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+2, 1.0))
plt.show()
15/194:
login_data_df.plot(x='login_day', y='login_hr', color='r')
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+2, 1.0))
plt.show()
15/195:
login_data_df.plot(x='login_hr', y='login_min', color='g')
plt.xticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+2, 1.0))
plt.show()
15/196:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='login_day', color='r')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/197:
login_data_df.plot(x='login_day', y='login_hr', color='b')
plt.xticks(np.arange(min(login_data_df.login_day)-1, max(login_data_df.login_day)+2, 1.0))
plt.show()
15/198:
login_data_df.plot(x='login_hr', y='login_min', color='g')
plt.xticks(np.arange(min(login_data_df.login_hr)-1, max(login_data_df.login_hr)+2, 1.0))
plt.show()
15/199:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
15/200:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
challendge_data
15/201:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
challenge_data
15/202:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
15/203: challenge_data[0]
15/204: challenge_data[0], challenge_data[1]
15/205:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/206:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/207:
# Assess the dataframe
login_data_df.info()
15/208:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/209:
# Import necessary modules
from dateutil import parser
15/210:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/211:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/212:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/213:
# iterate over login_time column
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/214:
# Test output
#login_yr
15/215:
# Add datetime values to data_df
login_data_df['login_year'] = list_to_series(login_yr)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
15/216: login_data_df.tail()
15/217:
# Drop login_year column
login_data_df = login_data_df.drop(columns='login_year')
login_data_df.head()
15/218:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/219:
# Assess the dataframe
login_data_df.info()
15/220:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/221:
# Import necessary modules
from dateutil import parser
15/222:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/223:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/224:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/225:
# iterate over login_time column
timestamp = []
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(timestamp, datetime)
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/226:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)
login_data_df['login_year'] = list_to_series(login_yr)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
15/227: login_data_df.tail()
15/228:
login_data_df = login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
15/229:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
15/230:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/231:
# Assess the dataframe
login_data_df.info()
15/232:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/233:
# Import necessary modules
from dateutil import parser
15/234:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return parser.parse(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/235:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/236:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/237:
# iterate over login_time column
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/238:
# Test output
#login_yr
15/239:
# Add datetime values to data_df
login_data_df['login_year'] = list_to_series(login_yr)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)
15/240: login_data_df.tail()
15/241:
login_data_df.set_index('login_time', inplace=True)
login_data_df.head()
15/242: Step 2: Add a numeric column to track counts
15/243:
login_data_df['counts'] = 1
login_data_df.head()
15/244: login_data_df.resample(rule='15T', how='sum')
15/245:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/246:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/247:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
15/248:
# Assess the dataframe
login_data_df.info()
15/249:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
15/250:
# Import necessary modules
from dateutil import parser
15/251:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
15/252:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
15/253:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
15/254:
# iterate over login_time column
timestamp = []
login_yr = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(timestamp, datetime)
    add_to_list(login_yr, yr)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
15/255:
# Test output
#login_yr
15/256:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)
login_data_df['login_year'] = list_to_series(login_yr)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
15/257: login_data_df.tail()
15/258:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
15/259:
login_data_df['counts'] = 1
login_data_df.head()
15/260: login_data_df.resample(rule='15T', how='sum')
15/261: login_data_df.resample(rule='15T').sum()
15/262:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
15/263:
# Basic Visualization of Data
login_data_df.plot(x='counts', y='login_day', color='r', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/264:
# Basic Visualization of Data
login_data_df.plot(x='login_day', y='counts', color='r', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/265:
login_plot = sns.PairGrid(login_data_df)
login_plot.map_diag(plt.hist)
login_plot.map_offdiag(plt.scatter)
15/266:
# Basic Visualization of Data
login_data_df.plot(x='login_month', y='counts', color='r', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
15/267:
# Use seaborn style defaults and set the default figure size
sns.set(rc={'figure.figsize':(11, 4)})
15/268: login_data_df.tail()
17/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
17/2:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)

# Define Data Type
type(data)
17/3:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
17/4: challenge_data[0], challenge_data[1]
17/5:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
17/6: login_data_df.tail()
17/7:
# Assess the dataframe
login_data_df.info()
17/8:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
17/9:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
17/10:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
17/11:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
17/12:
# iterate over login_time column
timestamp = []
login_mon = []
login_day = []
login_hr = []
login_min = []
login_sec = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # select each datetime description
    yr = datetime.year
    mon = datetime.month
    day = datetime.day
    hr = datetime.hour
    minu = datetime.minute
    sec = datetime.second
    
    # append to corr lists
    add_to_list(timestamp, datetime)
    add_to_list(login_mon, mon)
    add_to_list(login_day, day)
    add_to_list(login_hr, hr)
    add_to_list(login_min, minu)
    add_to_list(login_sec, sec)
17/13:
# Test output
#login_yr
17/14:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)
login_data_df['login_month'] = list_to_series(login_mon)
login_data_df['login_day'] = list_to_series(login_day)
login_data_df['login_hr'] = list_to_series(login_hr)
login_data_df['login_min'] = list_to_series(login_min)
login_data_df['login_sec'] = list_to_series(login_sec)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
17/15: login_data_df.tail()
17/16:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
17/17:
login_data_df['counts'] = 1
login_data_df.head()
17/18:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
17/19:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
17/20: login_data_df.tail()
17/21:
# Assess the dataframe
login_data_df.info()
17/22:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
17/23:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
17/24:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
17/25:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
17/26:
# iterate over login_time column
timestamp = []

for date in login_data_df.login_time:
    
    # convert date to datetime object
    datetime = to_datetime(date)
    
    # append to corr lists
    add_to_list(timestamp, datetime)
17/27:
# Test output
#timestamp
17/28:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
17/29:
# iterate over login_time column
timestamp = []

for date in login_data_df.login_time:
    # convert date to datetime object and append to corr lists
    add_to_list(timestamp, to_datetime(date))
17/30:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
17/31: login_data_df.tail()
17/32:
# Assess the dataframe
login_data_df.info()
17/33:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
17/34:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
17/35:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
17/36:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
17/37:
# iterate over login_time column
timestamp = []

for date in login_data_df.login_time:
    # convert date to datetime object and append to corr lists
    add_to_list(timestamp, to_datetime(date))
17/38:
# Test output
#timestamp
17/39:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
17/40: login_data_df.tail()
17/41:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
17/42:
login_data_df['counts'] = 1
login_data_df.head()
17/43:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
17/44:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style="darkgrid")
17/45:
login_data_df = login_data_df.reset_index()
login_data_df.head()
17/46:
# Basic Visualization of Data
login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
17/47:
# Basic Visualization of Data
login_data_df.plot(x='counts', y='timestamp', color='g', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
17/48:
# Basic Visualization of Data
login_data_df.plot(x='counts', y='timestamp', color='g', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
17/49:
# Basic Visualization of Data
login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
#plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
17/50:
# Basic Visualization of Data
login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
plt.xticks(np.arange(min(login_data_df.login_month)-1, max(login_data_df.login_month)+1.5, 1.0))
plt.show()
17/51:
# Basic Visualization of Data
login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
plt.xticks(np.arange(min(login_data_df.timestamp)-1, max(login_data_df.timestamp)+1.5, 1.0))
plt.show()
17/52:
# Basic Visualization of Data
login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
plt.set_xlabel("x label")
plt.set_ylabel("y label")
plt.show()
17/53:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("x label")
ax.set_ylabel("y label")
plt.show()
17/54:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970")
ax.set_ylabel("Frequency of Logins")
plt.show()
17/55:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within a Geographic Location")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/56:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/57:
# Visualization of Data
ax = login_data_df.plot(x='timestamp.month', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/58: days = timestamp.day
17/59: days = timestamp[0].day
17/60:
days = timestamp[0].day
days
17/61:
days = timestamp[10].day
days
17/62:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)

# adds new datetime objects to list
def datetime_to_list(li, datetime):
    return add_to_list(timestamp, to_datetime(date))
17/63:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)

# adds new datetime objects to list
def datetime_to_list(li, date):
    return add_to_list(timestamp, to_datetime(date))
17/64:
# define empty list
timestamp = []

# add datetime objects to list
for date in login_data_df.login_time:
    datetime_to_list(timestamp, date)
17/65:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
17/66: login_data_df.tail()
17/67:
# Assess the dataframe
login_data_df.info()
17/68:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
17/69:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)

# adds new datetime objects to list
def datetime_to_list(li, date):
    return add_to_list(timestamp, to_datetime(date))
17/70:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
17/71:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
17/72:
# define empty list
timestamp = []

# add datetime objects to list
for date in login_data_df.login_time:
    datetime_to_list(timestamp, date)
17/73:
# Test output
timestamp
17/74:
# Test output
#timestamp
17/75:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
17/76:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
17/77:
login_data_df['counts'] = 1
login_data_df.head()
17/78:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
17/79:
login_data_df = login_data_df.reset_index()
login_data_df.head()
17/80:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style="darkgrid")
17/81:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/82:
# define empty list
time_day = []

# add datetime objects to list
for date in login_data_df.timestamp:
    print(date)
17/83:
# define empty list
time_day = []

# add datetime objects to list
for date in login_data_df.timestamp:
    print(date.day)
17/84:
# define empty list
time_day = []
time_mon = []

# add datetime objects to list
for date in login_data_df.timestamp:
    
    #define vars
    day = date.day
    month = date.month
    
    #append to lists
    add_to_list(time_day, day)
    add_to_list(time_month, month)
17/85:
# define empty list
time_day = []
time_month = []

# add datetime objects to list
for date in login_data_df.timestamp:
    
    #define vars
    day = date.day
    month = date.month
    
    #append to lists
    add_to_list(time_day, day)
    add_to_list(time_month, month)
17/86:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df.head()
17/87:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df.head(), login_data_df.tail()
17/88:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df.head()
17/89:
# Visualization of Month and Date Data
fig, ax = plt.subplot(1,1)

ax[0,0] = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax1.set_ylabel("Frequency of User Logins")
plt.show()
17/90:
# Visualization of Month and Date Data
fig, ax = plt.subplot(2,1)

ax[0,0] = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax1.set_ylabel("Frequency of User Logins")
plt.show()
17/91:
# Visualization of Month and Date Data
fig, ax = plt.subplot(2,2)

ax[0,0] = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax1.set_ylabel("Frequency of User Logins")
plt.show()
17/92:
# Visualization of Month and Date Data
fig, axs = plt.subplot(2)

axs[0] = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
ax[0].set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax[0].set_ylabel("Frequency of User Logins")
plt.show()
17/93:
# Visualization of Month and Date Data
fig, axs = plt.subplot(2)

axs[0] = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist')
axs[0].set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
axs[0].set_ylabel("Frequency of User Logins")
plt.show()
17/94:
# Visualization of Month and Date Data
fig, axs = plt.subplot(2)

axs[0].plot(x='timestamp', y='counts', color='g', kind='hist')
axs[0].set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
axs[0].set_ylabel("Frequency of User Logins")
plt.show()
17/95:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplot(2)

axs1.plot(x='timestamp', y='counts', color='g', kind='hist')
axs1.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
axs1.set_ylabel("Frequency of User Logins")
plt.show()
17/96:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplot(2)

ax1.plot(x='timestamp', y='counts', color='g', kind='hist')
ax1.set_xlabel("Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax1.set_ylabel("Frequency of User Logins")
plt.show()
17/97:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(2)
fig.suptitle('User Login Information of one Geographic Location Between Jan and April of 1970')
ax1.plot(x=login_data_df.month, y=login_data_df.counts, color='g', kind='hist')
ax2.plot(x=login_data_df.day, y=login_data_df.counts, color='b', kind='hist')
plt.show()
17/98:
month = login_data_df.month
month
17/99:
month = login_data_df.month[1]
month
17/100:
month = login_data_df.month
month
17/101:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
ax1.plot(x=login_data_df.month, y=login_data_df.counts, color='g', kind='hist')
ax2.plot(x=login_data_df.day, y=login_data_df.counts, color='b', kind='hist')
plt.show()
17/102:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='hist', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='hist', ax=ax2)
plt.show()
17/103:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df.tail()
17/104:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/105:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,1)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/106:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(2,1)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/107:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/108:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='pie', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='pie', ax=ax2)
plt.show()
17/109:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='bar', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='bar', ax=ax2)
plt.show()
17/110:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='line', ax=ax2)
plt.show()
17/111:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/112:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month',xticks='month', y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/113:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month',xticks=1', y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/114:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month',xticks=1, y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/115:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='line', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/116:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='hist', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/117:
# Visualization of Month and Date Data
fig, (ax1, ax2) = plt.subplots(1,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
plt.show()
17/118:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel()
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/119:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("X Label")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/120:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("Recorded Timestamps")
ax.set_ylabel("Frequency of User Logins")
plt.show()
17/121:
# define empty list
time_hour = []
time_day = []
time_month = []

# add datetime objects to list
for date in login_data_df.timestamp:
    
    #define vars
    hour = date.hour
    day = date.day
    month = date.month
    
    #append to lists
    add_to_list(time_hour, hour)
    add_to_list(time_day, day)
    add_to_list(time_month, month)
17/122:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df['hour'] = list_to_series(time_hour)
login_data_df.tail()
17/123:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/124:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,6)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/125:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax5)
plt.show()
17/126:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,5)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/127:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/128:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/129:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(2,1)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/130:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(2,2)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/131:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1, 3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.show()
17/132:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.subplots_adjust(left=0.2, bottom=0.2, right=0.2, wspace=0.3)
plt.show()
17/133:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.subplots_adjust(left=0.3, bottom=0.2, right=0.2, wspace=0.3)
plt.show()
17/134:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.subplots_adjust(left=0.3, bottom=0.2, right=0.5, wspace=0.3)
plt.show()
17/135:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.subplots_adjust(wspace=0.3)
plt.show()
17/136:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)
plt.subplots_adjust(wspace=0.8)
plt.show()
17/137:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt, figure
import seaborn as sns; sns.set(style="darkgrid")
17/138:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style="darkgrid")
from matplotlib.pyplot import figure
17/139:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')

plt.show()
17/140:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(1, 3), dpi=80, facecolor='w', edgecolor='k')

plt.show()
17/141:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(1, 3), dpi=200)

plt.show()
17/142:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/143:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.8)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.8)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.8)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/144:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.5)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.5)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.5)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/145:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/146:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='line', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/147:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/148:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Information of One Geographic Location Between Jan and April of 1970')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/149:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Between Jan and April of 1970')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', ylabel='Number User Logins', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', ylabel='Number User Logins', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', ylabel='Number User Logins', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/150:
# Visualization of Month and Date Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)
fig.suptitle('User Login Between Jan and April of 1970')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/151:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('User Login Between Jan and April of 1970')
fig.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/152:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('User Login Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/153:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('User Login Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_xlabel('common xlabel')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/154:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('User Login Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('common xlabel')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/155:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('User Login Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/156:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('Trend of User Login in Geographic Region Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
17/157: challenge_data[0]
18/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
18/2:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)

# Define Data Type
type(data)
18/3:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
18/4: login_data_df.tail()
18/5:
# Assess the dataframe
login_data_df.info()
18/6:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
18/7:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)

#adds new datetime objects to list
def datetime_to_list(li, date):
    return add_to_list(timestamp, to_datetime(date))
18/8:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
18/9:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
18/10:
# define empty list
timestamp = []

# add datetime objects to list
for date in login_data_df.login_time:
    datetime_to_list(timestamp, date)
18/11:
# Test output
#timestamp
18/12:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
18/13:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
18/14:
login_data_df['counts'] = 1
login_data_df.head()
18/15:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
18/16:
login_data_df = login_data_df.reset_index()
login_data_df.head()
18/17:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
18/18:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("Recorded Timestamps")
ax.set_ylabel("Frequency of User Logins")
plt.show()
18/19:
# define empty list
time_hour = []
time_day = []
time_month = []

# add datetime objects to list
for date in login_data_df.timestamp:
    
    #define vars
    hour = date.hour
    day = date.day
    month = date.month
    
    #append to lists
    add_to_list(time_hour, hour)
    add_to_list(time_day, day)
    add_to_list(time_month, month)
18/20:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df['hour'] = list_to_series(time_hour)
login_data_df.tail()
18/21:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('Trend of User Login in Geographic Region Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
18/22:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
18/23: challenge_data[0]
18/24: print('The total length of this dataset list is: ', len(challenge_data))
18/25: challenge_data[0].keys
18/26: list(challenge_data[0].keys)
18/27: challenge_data[0][0]
18/28: challenge_data[0]
18/29:
for each in challenge_data[0]:
    print(each)
18/30:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)
18/31:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
18/32:
ultimate_df = pd.DataFrame(columns=column_names)
ultimate_df.head()
18/33:
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
18/34:
# Assess Dataframe
ultimate_df.info()
18/35:
# Assess Dataframe
ultimate_df.describe()
18/36:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
18/37:
# Assess Dataframe
ultimate_df.describe()
18/38: print(ultimate_df.shape)
18/39:
# Total number of unique values of the columns
print(ultimate_df.nunique())
18/40:
# Categorical Columns
ultimate_df['signup_date'].value_counts().plot.bar(title="Frequency of Distribution of Sign-Up Date")
18/41:
# Categorical Columns
ultimate_df['signup_date'].value_counts().plot.bar(title="Frequency of Distribution of Sign-Up Date")
18/42: ultimate_df['last_trip_date'].value_counts().plot.bar(title="Frequency of Distribution of Sign-Up Date")
18/43:
# Import necessary modules
from datetime import datetime
from dateutil.parser import parse
18/44:
# Convert date strings to datetime objects
ultimate_df = pd.to_datetime(ultimate_df[['signup_date','last_trip_date']])
18/45:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%m/%d/%Y') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%m/%d/%Y') for x in ultimate_df['last_trip_date']]
18/46:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y/%m/%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y/%m/%d') for x in ultimate_df['last_trip_date']]
18/47:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]
18/48:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df[['signup_date','last_trip_date']].head()
18/49:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df[['signup_date','last_trip_date']].dtype()
18/50:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df['last_trip_date'].dtype(), ultimate_df['signup_date'].dtype()
18/51:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df['last_trip_date'][0].dtype(), ultimate_df['signup_date'][0].dtype()
18/52:
# Convert date strings to datetime objects

# signup
[datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df['last_trip_date'][0].dtype(), ultimate_df['signup_date'][0].dtype()
18/53:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

ultimate_df['last_trip_date'][0].dtype(), ultimate_df['signup_date'][0].dtype()
18/54:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

 ultimate_df['signup_date'][0].dtype()
18/55:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]
ultimate_df['signup_date'][0].dtype()
18/56:
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
18/57:
# Assess Dataframe
ultimate_df.info()
18/58:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
18/59:
# Import necessary modules
from datetime import datetime
18/60:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]
18/61:
# Assess Dataframe
ultimate_df.describe()
18/62:
# Test output
ultimate_df.info()
18/63:
# Test output
ultimate_df[['signup_date','last_trip_date']].info()
18/64:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
18/65:
# Total number of unique values within each column
print(ultimate_df.nunique())
18/66:
# Grab key names for numeric column
numeric_cols = []

for col in utlimate_df:
    
    if utlimate_df.col.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/67:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    
    if ultimate_df.col.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/68:
# Grab key names for numeric column
numeric_cols = []

for column in ultimate_df:
    
    
    if ultimate_df.column.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/69:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(col)
    
    #if ultimate_df.column.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        #print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/70:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(ultimate_df.col)
    
    #if ultimate_df.column.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        #print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/71:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(ultimate_df[col])
    
    #if ultimate_df.column.dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        #print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/72:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(ultimate_df[col].dtype)
    
    #if ultimate_df[column].dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        #print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/73:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(ultimate_df[col].dtype)
    
    if ultimate_df[col].dtype == 'int64' or utlimate_df.col.dtype == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/74:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    print(ultimate_df[col].dtype)
    
    if ultimate_df[col].dtype == 'int64' or ultimate_df.col.dtype == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/75:
# Grab key names for numeric column
numeric_cols = []

for col in ultimate_df:
    
    data_type = ultimate_df[col].dtype
    
    if data_type == 'int64' or data_type == 'float64':
        print(col)
    #add_to_list(column_names, each)

# Test output
#column_names
18/76:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    else if data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
numeric_cols, datetime_cols
18/77:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
numeric_cols, datetime_cols
18/78:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols) 
print(datetime_cols)
18/79:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols) 
print('.')
print(datetime_cols)
18/80:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
18/81:
fig, ax = plt.subplots(len(numeric_cols), figsize=(4,3))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/82:
fig, ax = plt.subplots(len(numeric_cols), figsize=(16,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/83:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,8))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/84:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,10))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/85:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/86:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/87:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], hist_kws={"rwidth":0.1,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/88:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], hist_kws={"rwidth":0.85,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/89:
fig, ax = plt.subplots(len(numeric_cols), figsize=(12,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/90:
fig, ax = plt.subplots(len(numeric_cols), figsize=(8,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/91:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
18/92:
# Study the relationship between two variables by analyzing the scatter plot
sns.pairplot(ultimate_df)
18/93:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Driver Status of Ultimate Black User")
18/94:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
18/95:
# Study the relationship between two variables by analyzing the scatter plot

# Drop boolean column - Boolean values shouldn't enter describe_numeric_1d function. It must be treated as Categorical
ultimate_df_snsplot = ultimate_df.drop(columns='ultimate_black_user', axis=1)

# Plot remaining variables
sns.pairplot(ultimate_df_snsplot)
18/96:
fig, ax = plt.subplots(len(column_names), figsize=(8,40))

for i, col_val in enumerate(column_names):

    sns.boxplot(y=ultimate_df[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
18/97:
fig, ax = plt.subplots(len(numeric_cols), figsize=(8,40))

for i, col_val in enumerate(numeric_cols):

    sns.boxplot(y=ultimate_df[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
18/98:
fig, ax = plt.subplots(len(numeric_cols), figsize=(8,40))

for i, col_val in enumerate(numeric_cols):

    sns.boxplot(y=ultimate_df[col_val], ax=ax[i], vertical=False)
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
18/99:
fig, ax = plt.subplots(len(numeric_cols), figsize=(8,30))

for i, col_val in enumerate(numeric_cols):

    sns.boxplot(y=ultimate_df[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
21/1:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
21/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
21/3:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
21/4:
# Assess Contents of Data
challenge_data[0]
21/5:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
21/6:
# Create a general function that:

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
21/7:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
21/8:
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
21/9:
# Assess Dataframe
ultimate_df.info()
21/10:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
21/11:
# Import necessary modules
from datetime import datetime
21/12:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]
21/13:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
21/14:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]
21/15:
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
21/16:
# Assess Dataframe
ultimate_df.info()
21/17:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
21/18:
# Import necessary modules
from datetime import datetime
21/19:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
21/20:
# Assess Dataframe
ultimate_df.describe()
21/21:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
21/22:
# Total number of unique values within each column
print(ultimate_df.nunique())
21/23:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
21/24:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
21/25:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
21/26:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import matplotlib as plt
import numpy as np
import json
21/27:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
21/28:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
21/29:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
21/30:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
21/31:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
21/32:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
21/33:
# Study the relationship between two variables by analyzing the scatter plot

# Drop boolean column - Boolean values shouldn't enter describe_numeric_1d function. It must be treated as Categorical
ultimate_df_snsplot = ultimate_df.drop(columns='ultimate_black_user', axis=1)

# Plot remaining variables
sns.pairplot(ultimate_df_snsplot)
21/34:
fig, ax = plt.subplots(len(numeric_cols), figsize=(8,30))

for i, col_val in enumerate(numeric_cols):

    sns.boxplot(y=ultimate_df[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
22/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
22/2:
# Load json dataset
json_data = open('logins.json')
data = json.load(json_data)

# Define Data Type
type(data)
22/3:
# Convert dict data to dataframe
login_data_df = pd.DataFrame.from_dict(data)
login_data_df.head()
22/4: login_data_df.tail()
22/5:
# Assess the dataframe
login_data_df.info()
22/6:
# Define the type of objects within the login_time column
login_data_df.login_time.dtype
22/7:
# Create a general function that:

#converts a datapoint into a Datetime object
def to_datetime(value):
    return pd.to_datetime(value)

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)

#adds new datetime objects to list
def datetime_to_list(li, date):
    return add_to_list(timestamp, to_datetime(date))
22/8:
# Test output (1)
value = login_data_df.login_time[0]
to_datetime(value)
22/9:
# Test output
val = 6
li = [1,2,3,4,5]
# (2)
add_to_list(li, val)
# (3)
list_to_series(li)
22/10:
# define empty list
timestamp = []

# add datetime objects to list
for date in login_data_df.login_time:
    datetime_to_list(timestamp, date)
22/11:
# Test output
#timestamp
22/12:
# Add datetime values to data_df
login_data_df['timestamp'] = list_to_series(timestamp)

# Drop login_time column
login_data_df = login_data_df.drop(columns='login_time')
login_data_df.head()
22/13:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
22/14:
login_data_df['counts'] = 1
login_data_df.head()
22/15:
login_data_df = login_data_df.resample(rule='15T').sum()
login_data_df.head()
22/16:
login_data_df = login_data_df.reset_index()
login_data_df.head()
22/17:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
22/18:
# Basic Visualization of Data
ax = login_data_df.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("Recorded Timestamps")
ax.set_ylabel("Frequency of User Logins")
plt.show()
22/19:
# define empty list
time_hour = []
time_day = []
time_month = []

# add datetime objects to list
for date in login_data_df.timestamp:
    
    #define vars
    hour = date.hour
    day = date.day
    month = date.month
    
    #append to lists
    add_to_list(time_hour, hour)
    add_to_list(time_day, day)
    add_to_list(time_month, month)
22/20:
# Add datetime values to data_df
login_data_df['month'] = list_to_series(time_month)
login_data_df['day'] = list_to_series(time_day)
login_data_df['hour'] = list_to_series(time_hour)
login_data_df.tail()
22/21:
# Visualization of Month, Date, and Hour Data
fig, (ax1, ax2, ax3) = plt.subplots(1,3)

# Set common labels
fig.suptitle('Trend of User Login in Geographic Region Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
24/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
24/2:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
24/3:
# Assess Contents of Data
challenge_data[0]
24/4:
# Create a general function that:

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
24/5:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
24/6:
# Convert the list of lists of key:value pairs to a dataframe
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
24/7:
# Assess Dataframe
ultimate_df.info()
24/8:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
24/9:
# Import necessary modules
from datetime import datetime
24/10:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
24/11:
# Assess Dataframe
ultimate_df.describe()
24/12:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
24/13:
# Total number of unique values within each column
print(ultimate_df.nunique())
24/14:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
24/15:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
24/16:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
24/17:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/18:
# Use datetime variables in dataetime_cols list

x, y
           
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

for i, col_val in enumerate(datetime_cols):

    plt.plot_date(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/19:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

for i, col_val in enumerate(datetime_cols):

    plt.plot_date(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/20:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

for datetime in datetime_cols:
    print(datetime)
    #plt.plot_date(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    #ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    #ax[i].set_xlabel(col_val, fontsize=8)
    #ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/21:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

print(datetime[0])
plt.plot_date(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
ax[i].set_xlabel(col_val, fontsize=8)
ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/22:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

print(datetime_cols[0])
plt.plot_date(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
ax[i].set_xlabel(col_val, fontsize=8)
ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/23:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
ax.set_title('Frequency Distribution '+ col_val, fontsize=10)
ax.set_xlabel(datetime_cols[0], fontsize=8)
ax.set_ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/24:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1], bins=34, ax=ax, vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
ax.set_title('Frequency Distribution '+ col_val, fontsize=10)
ax.set_xlabel(datetime_cols[0], fontsize=8)
ax.set_ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/25:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1], bins=34, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
ax.set_title('Frequency Distribution '+ col_val, fontsize=10)
ax.set_xlabel(datetime_cols[0], fontsize=8)
ax.set_ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/26:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1])
ax.set_title('Frequency Distribution '+ col_val, fontsize=10)
ax.set_xlabel(datetime_cols[0], fontsize=8)
ax.set_ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/27:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1])
ax.title('Frequency Distribution '+ col_val, fontsize=10)
ax.xlabel(datetime_cols[0], fontsize=8)
ax.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/28:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=datetime_cols[0], y=datetime_cols[1])


plt.show()
24/29:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot(x=datetime_cols[0], y=datetime_cols[1])

plt.show()
24/30:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(len(datetime_cols), figsize=(10,12))

ax1.plot(datetime_cols[0])
ax2.plot(datetime_cols[1])

plt.show()
24/31:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(len(datetime_cols), figsize=(10,12))

plt.plot_date(x=ultimate_df[datetime_cols[0]], y=ultimate_df[datetime_cols[1]])


plt.show()
24/32:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(1, figsize=(10,12))

plt.plot_date(x=ultimate_df[datetime_cols[0]], y=ultimate_df[datetime_cols[1]])


plt.show()
24/33:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(1, figsize=(10,12))

plt.plot_date(x=ultimate_df[datetime_cols[0]], y=ultimate_df[datetime_cols[1]])
ax.title('Frequency Distribution ', fontsize=10)
ax.xlabel(datetime_cols[0], fontsize=8)
ax.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/34:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(1, figsize=(10,12))

plt.plot_date(x=ultimate_df[datetime_cols[0]], y=len(ultimate_df[datetime_cols[0]])
ax.title('Frequency Distribution ', fontsize=10)
ax.xlabel(datetime_cols[0], fontsize=8)
ax.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/35:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(1, figsize=(10,12))

ax1.plot_date(x=ultimate_df[datetime_cols[0]], y=len(ultimate_df[datetime_cols[0]])
ax.title('Frequency Distribution ', fontsize=10)
ax.xlabel(datetime_cols[0], fontsize=8)
ax.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/36:
# Use datetime variables in dataetime_cols list
fig, ax = plt.subplots(1, figsize=(10,12))

plt.plot_date(x=ultimate_df[datetime_cols[0]], y=ultimate_df[datetime_cols[1]])
ax.title('Frequency Distribution '+ col_val, fontsize=10)
ax.xlabel(datetime_cols[0], fontsize=8)
ax.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/37:
# Create a Series holding the total size of each sign_up date
signup_group_size = ultimate_df.groupby('signup_date').first()
cohort_group_size.head()
24/38:
# Create a Series holding the total size of each sign_up date
signup_group_size = ultimate_df.groupby('signup_date').first()
signup_group_size.head()
24/39:
# Create a Series holding the total size of each sign_up date
signup_group_size = ultimate_df.groupby('signup_date').first()
signup_group_size.tail()
24/40:
customer_retention_df = ultimate_df.copy()
customer_retention_df.head()
24/41:
# Create a customer retention dataframe
customer_retention_df = ultimate_df[['signup_date','last_trip_date']]
customer_retention_df.head()
24/42:
# Create a customer retention dataframe
customer_retention_df = ultimate_df[['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['Counts'] 

# Test output
customer_retention_df.head()
24/43:
# Create a customer retention dataframe
customer_retention_df = ultimate_df[['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['Counts'] = 1

# Test output
customer_retention_df.head()
24/44:
# Create a customer retention dataframe
customer_retention_df = ultimate_df[['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['Counts'] = 1

# Test output
customer_retention_df.head()
24/45:
# Create a customer retention dataframe
customer_retention_df = ultimate_df.loc[columns='signup_date','last_trip_date']

# Add a counts column
customer_retention_df['Counts'] = 1

# Test output
customer_retention_df.head()
24/46:
# Create a customer retention dataframe
customer_retention_df = ultimate_df.loc[:,['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['Counts'] = 1

# Test output
customer_retention_df.head()
24/47:
# Create a Series holding the total size of each sign_up date
signup_group_size = ultimate_df.groupby('signup_date').first()
signup_group_size.tail()
24/48:
# Create a Series holding the total size of each sign_up date
signup_group_size = customer_retention_df.groupby('signup_date').first()
signup_group_size.tail()
24/49:
# Create a Series holding the total size of each sign_up date
signup_group_size = customer_retention_df.groupby('signup_date').first()
signup_group_size.head()
24/50:
# Create a Series holding the total size of each sign_up date
signup_group_size = customer_retention_df.groupby('signup_date')
signup_group_size.head()
24/51:
# Create a Series holding the total size of each sign_up date
signup_group_size = customer_retention_df.groupby('signup_date')

# Test output
signup_group_size.head()
24/52:
# Create a Series holding the total size of each sign_up date
signup_group_size = customer_retention_df.groupby('signup_date').sum()

# Test output
signup_group_size.head()
24/53:
# Create a Series holding the total size of each sign_up date 
signup_group_size = customer_retention_df.groupby('signup_date').sum()
# and last_trip
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Test output
signup_group_size.head() , last_date_group_size.head()
24/54:
# Create a Series holding the total size of each sign_up date 
signup_group_size = customer_retention_df.groupby('signup_date').sum()
# and last_trip
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Test output
print(signup_group_size.head())
print(last_date_group_size.head())
24/55:
# Create a Series holding the total size of each sign_up date 
signup_group_size = customer_retention_df.groupby('signup_date').sum()
# and last_trip
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Reset index 
signup_group_size = signup_group_size.reset_index()
last_date_group_size = last_date_group_size.reset_index()

# Test output
print(signup_group_size.head())
print(last_date_group_size.head())
24/56:
# Create a customer retention dataframe
customer_retention_df = ultimate_df.loc[:,['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['count'] = 1

# Test output
customer_retention_df.head()
24/57:
# Create a Series holding the total size of each sign_up date 
signup_group_size = customer_retention_df.groupby('signup_date').sum()
# and last_trip
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Reset index 
signup_group_size = signup_group_size.reset_index()
last_date_group_size = last_date_group_size.reset_index()

# Test output
print(signup_group_size.head())
print(last_date_group_size.head())
24/58:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size.signup_date, y=signup_group_size.count)
ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel(datetime_cols[0], fontsize=8)
ax1.ylabel(datetime_cols[1], fontsize=8)

# Last Trip Date
ax1.plot_date(x=last_date_group_size.signup_date, y=last_date_group_size.count)
ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel(datetime_cols[0], fontsize=8)
ax1.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/59:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size.signup_date, y=signup_group_size.count)
ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel(datetime_cols[0], fontsize=8)
ax1.ylabel(datetime_cols[1], fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size.signup_date, y=last_date_group_size.count)
ax2.title('Frequency Distribution ', fontsize=10)
ax2.xlabel(datetime_cols[0], fontsize=8)
ax2.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/60:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size.signup_date, y=signup_group_size.count)
ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel(datetime_cols[0], fontsize=8)
ax1.ylabel(datetime_cols[1], fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size.last_trip_date, y=last_date_group_size.count)
ax2.title('Frequency Distribution ', fontsize=10)
ax2.xlabel(datetime_cols[0], fontsize=8)
ax2.ylabel(datetime_cols[1], fontsize=8)

plt.show()
24/61:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel('Sign-Up Date', fontsize=8)
ax1.ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
ax2.title('Frequency Distribution ', fontsize=10)
ax2.xlabel('Sign-Up Date', fontsize=8)
ax2.ylabel('Count', fontsize=8)

plt.show()
24/62:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
#ax1.title('Frequency Distribution ', fontsize=10)
ax1.xlabel('Sign-Up Date', fontsize=8)
ax1.ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
#ax2.title('Frequency Distribution ', fontsize=10)
ax2.xlabel('Sign-Up Date', fontsize=8)
ax2.ylabel('Count', fontsize=8)

plt.show()
24/63:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
#ax1.title('Frequency Distribution ', fontsize=10)
ax1.set_xlabel('Sign-Up Date', fontsize=8)
ax1.set_ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
#ax2.title('Frequency Distribution ', fontsize=10)
ax2.set_xlabel('Sign-Up Date', fontsize=8)
ax2.set_ylabel('Count', fontsize=8)

plt.show()
24/64:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
ax1.set_title('Frequency Distribution ', fontsize=10)
ax1.set_xlabel('Sign-Up Date', fontsize=8)
ax1.set_ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
ax2.set_title('Frequency Distribution ', fontsize=10)
ax2.set_xlabel('Sign-Up Date', fontsize=8)
ax2.set_ylabel('Count', fontsize=8)

plt.show()
24/65:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
ax1.set_title('Frequency Distribution of User Sign-Up Dates', fontsize=10)
ax1.set_xlabel('Sign-Up Date', fontsize=8)
ax1.set_ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
ax2.set_title('Frequency Distribution of Last Trip of User', fontsize=10)
ax2.set_xlabel('Sign-Up Date', fontsize=8)
ax2.set_ylabel('Count', fontsize=8)

plt.show()
24/66:
# Max Last Trip
last_date_group_size['last_trip_date'].max()
24/67:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    print(last_trip)
24/68:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        customer_retained += 1
    else:
        customer_lost += 1
24/69:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/70:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        print('yes')
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/71:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    print(last_trip)
    
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/72:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    print(last_trip.dtype)
    
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/73:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == Timestamp('2014-07-01 00:00:00'):
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/74:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == '2014-07-01 00:00:00':
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/75:
# Max Last Trip
last_date = last_date_group_size['last_trip_date'].max()
last_date
24/76:
# Create variables for customer_retained and customer_lost
customer_retained = 0
customer_lost = 0

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        customer_retained += 1
    else:
        customer_lost += 1

# Test output
customer_retained, customer_lost
24/77:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 'yes')
    else:
        add_to_list(customer_lost, 'no')

# Test output
customer_retained, customer_lost
24/78:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 'yes')
    else:
        add_to_list(customer_lost, 'no')

# Test output
#customer_retained, customer_lost
24/79:
# Build a Dataframe for customers 
customers = pd.DataFrame()
customers['retained'] = list_to_series(customer_retained)
customers['lost'] = list_to_series(customer_lost)
customers.head()
24/80:
# Create a combined customer list
customers = customer_retained + customer_lost
customers
24/81:
# Create a combined customer list
customers = customer_retained + customer_lost

# Test output
#customers
24/82:
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, explode=explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/83:
# Import necessary modules
from pylab import rcParams
24/84:
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, explode=explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/85:
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/86:
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/87:
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, colors=colors,shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/88:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
customer_retained, customer_lost
24/89:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost
24/90:
labels = ["yes", "no"]
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/91:
# Create a combined customer list
customers = customer_retained + customer_lost

# Test output
#customers
24/92:
labels = ["yes", "no"]
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/93:
#labels = ["yes", "no"]
colors = ["grey","purple"] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/94:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, {1, 'yes'})
    else:
        add_to_list(customer_lost, {0, 'no'})

# Test output
#customer_retained, customer_lost
24/95:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, {1, 'yes'})
    else:
        add_to_list(customer_lost, {0, 'no'})

# Test output
customer_retained, customer_lost
24/96:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, {1, 'yes'})
    else:
        add_to_list(customer_lost, {0, 'no'})

# Test output
#customer_retained, customer_lost
24/97:
# Create a combined customer list
customers = customer_retained + customer_lost

# Test output
#customers
24/98: customers[0].key
24/99: customers[0].key()
24/100: customers.items()
24/101:
labels = [x.items() for x in customers]
labels
24/102:
labels = [x.values() for x in customers]
labels
24/103:
# Create a combined customer list
customers = customer_retained + customer_lost

# Convert list to dict
customers = dict(customers)

# Test output
customers
24/104:
# Create a combined customer list
customers = customer_retained + customer_lost

# Test output
customers
24/105:
# Create a combined customer list
customers = customer_retained + customer_lost

# Test output
#customers
24/106: customers[0]
24/107:
for pair in customers:
    print(pair)
24/108:
for pair in customers:
    print(pair.values())
24/109:
for pair in customers:
    print(pair.value)
24/110:
for key,val in customers.items()
    print(key, val)
24/111:
for key,val in customers.items():
    print(key, val)
24/112:
for pair in customers:
    for key,val in pair.items():
        print(key, val)
24/113:
for pair in customers:
    
    for key,val in pair:
        print(key, val)
24/114:
# Create a combined customer list
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Test output
#customers
24/115:
# Create a combined customer list
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Test output
len_retained, len_lost
24/116:
# Create variables for customer_retained and customer_lost
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost
24/117:
# Create a combined customer list
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Test output
len_retained, len_lost
24/118:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost
24/119:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
customer_retained, customer_lost, total_customers
24/120:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if last_trip == last_date:
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/121:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Calculate percentages
percent_retained = (len_retained / total_customers)*100
percent_lost = (len_lost / total_customers)*100

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/122:
# Create a general function for percent
def calculate_percentage(val, total):
    round((val / total)*100, 3)
24/123:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Calculate percentages
percent_retained = calculate_percentage(len_retained / total_customers)
percent_lost = calculate_percentage(len_lost / total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/124:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/125:
# Create a general function for percent
def calculate_percentage(val, total):
    round((val / total)*100, 3)

# Test output
calculate_percentage(30, 100)
24/126:
# Create a general function for percent
def calculate_percentage(val, total):
    round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
24/127:
# Create a general function for percent
def calculate_percentage(val, total):
    (val / total)*100, 3

# Test output
print(calculate_percentage(30, 100))
24/128:
# Create a general function for percent
def calculate_percentage(val, total):
    (val / total)*100

# Test output
print(calculate_percentage(30, 100))
24/129:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
24/130:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/131:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/132:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/133:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/134:
# Last Trip Ordered
last_7_trips = last_date_group_size['last_trip_date'].ordered()
last_7_trips
24/135:
# Last Trip Ordered
last_7_trips = last_date_group_size['last_trip_date'].sort_values()
last_7_trips
24/136:
# Last Trip Ordered
last_7_trips = last_date_group_size['last_trip_date'].sort_values().head(7)
last_7_trips
24/137:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    for trip in last_7_trips:
    
        # Select customers retained
        if last_trip == trip:
            add_to_list(customer_retained, 1)
        else:
            add_to_list(customer_lost, 0)

# Test output
customer_retained, customer_lost, total_customers
24/138:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    for trip in last_7_trips:
    
        # Select customers retained
        if last_trip == trip:
            add_to_list(customer_retained, 1)
        else:
            add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/139:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/140:
dti = pd.date_range('2014-01-01', periods=3, freq='D')
dti
24/141:
dti = pd.date_range('2014-01-01', periods=7, freq='D')
dti
24/142:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/143:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
customer_retained, customer_lost, total_customers
24/144:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/145:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/146:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/147:
labels = ['Retained', 'Lost']
colors = ['grey','yello'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/148:
labels = ['Retained', 'Lost']
colors = ['grey','yellow'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/149:
labels = ['Retained', 'Lost']
colors = ['grey','blue'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/150:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 5,5

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/151:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 10,10

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/152:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 10,10

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%3.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/153:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 10,10

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/154:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/155:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=90,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/156:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8

# Plot
plt.pie(customers_list, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/157:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/158:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.show()
24/159: ultimate_df.head()
24/160:
ultimate_df['active1_inactive0'] = 'none'
ultimate_df.head()
24/161:
# Last Trip Ordered
dti = pd.date_range('2014-01-01', periods=30, freq='D')
dti
24/162:
# Import necessary modules
from pylab import rcParams
24/163:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/164:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
24/165:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/166:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/167:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users
24/168:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.head()
24/169:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
24/170: ultimate_df.info()
24/171:
# Apply get_dummies to dataset
ultimate_df = pd.get_dummies(ultimate_df)
ultimate_df.head()
24/172:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
24/173:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
24/174:
# Assess Contents of Data
challenge_data[0]
24/175:
# Create a general function that:

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
24/176:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
24/177:
# Convert the list of lists of key:value pairs to a dataframe
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
24/178:
# Assess Dataframe
ultimate_df.info()
24/179:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
24/180:
# Import necessary modules
from datetime import datetime
24/181:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
24/182:
# Assess Dataframe
ultimate_df.describe()
24/183:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
24/184:
# Total number of unique values within each column
print(ultimate_df.nunique())
24/185:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
24/186:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
24/187:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
24/188:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
24/189:
# Study the relationship between two variables by analyzing the scatter plot

# Drop boolean column - Boolean values shouldn't enter describe_numeric_1d function. It must be treated as Categorical
ultimate_df_snsplot = ultimate_df.drop(columns='ultimate_black_user', axis=1)

# Plot remaining variables
sns.pairplot(ultimate_df_snsplot)
24/190:
# Create a customer retention dataframe
customer_retention_df = ultimate_df.loc[:,['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['count'] = 1

# Test output
customer_retention_df.head()
24/191:
# Create a Series holding the total size of each sign_up date and last_trip
signup_group_size = customer_retention_df.groupby('signup_date').sum()
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Reset index 
signup_group_size = signup_group_size.reset_index()
last_date_group_size = last_date_group_size.reset_index()

# Test output
print(signup_group_size.head())
print(last_date_group_size.head())
24/192:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
ax1.set_title('Frequency Distribution of User Sign-Up Dates', fontsize=10)
ax1.set_xlabel('Sign-Up Date', fontsize=8)
ax1.set_ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
ax2.set_title('Frequency Distribution of Last Trip of User', fontsize=10)
ax2.set_xlabel('Sign-Up Date', fontsize=8)
ax2.set_ylabel('Count', fontsize=8)

plt.show()
24/193:
# Max Last Trip
last_date_group_size['last_trip_date'].max()
24/194:
# Last Trip Ordered
dti = pd.date_range('2014-01-01', periods=30, freq='D')
dti
24/195:
# Import necessary modules
from pylab import rcParams
24/196:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
24/197:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
24/198:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
24/199:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
24/200: ultimate_df.head()
24/201:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users
24/202:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.head()
24/203: ultimate_df.info()
24/204:
# Create a copy of ultimate_df to modify
data = ultimate_df.copy()
data.head(2)
24/205:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in data.dtypes])):
    if column == "object":
        data[data.columns[i]] = data[data.columns[i]].fillna(data[data.columns[i]].mode())
        data[data.columns[i]] = data[data.columns[i]].astype("category").cat.codes
    else:
        data[data.columns[i]] = data[data.columns[i]].fillna(data[data.columns[i]].median())
24/206:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.tail()
24/207:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users

# Test output
active_inactive
24/208:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users

# Test output
#active_inactive
24/209:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.tail()
24/210:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users

# Test output
len(active_inactive)
26/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
26/2:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
26/3:
# Assess Contents of Data
challenge_data[0]
26/4:
# Create a general function that:

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
26/5:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
26/6:
# Convert the list of lists of key:value pairs to a dataframe
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
26/7:
# Assess Dataframe
ultimate_df.info()
26/8:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
26/9:
# Import necessary modules
from datetime import datetime
26/10:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
26/11:
# Assess Dataframe
ultimate_df.describe()
26/12:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
26/13:
# Total number of unique values within each column
print(ultimate_df.nunique())
26/14:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
26/15:
# Plot a histogram of categorical values
ultimate_df['ultimate_black_user'].value_counts().plot.bar(title="Frequency Distribution of Status of Ultimate Black User")
print('TRUE if the user took an Ultimate Black in their first 30 days; FALSE otherwise')
26/16:
# Grab key names for numeric column
numeric_cols = []
datetime_cols = []

for col in ultimate_df:
    
    # Define data type
    data_type = ultimate_df[col].dtype
    
    # If numeric
    if data_type == 'int64' or data_type == 'float64':
         add_to_list(numeric_cols, col)
    
    # If datetime
    elif data_type == 'datetime64[ns]':
        add_to_list(datetime_cols, col)

# Test output
print(numeric_cols, len(numeric_cols)) 
print('.')
print(datetime_cols)
26/17:
fig, ax = plt.subplots(len(numeric_cols), figsize=(10,12))

for i, col_val in enumerate(numeric_cols):

    sns.distplot(ultimate_df[col_val], bins=34, ax=ax[i], vertical=False, hist_kws={"rwidth":0.75,'edgecolor':'black', 'alpha':1.0})
    ax[i].set_title('Frequency Distribution '+ col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
26/18:
# Study the relationship between two variables by analyzing the scatter plot

# Drop boolean column - Boolean values shouldn't enter describe_numeric_1d function. It must be treated as Categorical
ultimate_df_snsplot = ultimate_df.drop(columns='ultimate_black_user', axis=1)

# Plot remaining variables
sns.pairplot(ultimate_df_snsplot)
26/19:
# Create a customer retention dataframe
customer_retention_df = ultimate_df.loc[:,['signup_date','last_trip_date']]

# Add a counts column
customer_retention_df['count'] = 1

# Test output
customer_retention_df.head()
26/20:
# Create a Series holding the total size of each sign_up date and last_trip
signup_group_size = customer_retention_df.groupby('signup_date').sum()
last_date_group_size = customer_retention_df.groupby('last_trip_date').sum()

# Reset index 
signup_group_size = signup_group_size.reset_index()
last_date_group_size = last_date_group_size.reset_index()

# Test output
print(signup_group_size.head())
print(last_date_group_size.head())
26/21:
# Use datetime variables in dataetime_cols list
fig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))

# SignUp Date
ax1.plot_date(x=signup_group_size['signup_date'], y=signup_group_size['count'])
ax1.set_title('Frequency Distribution of User Sign-Up Dates', fontsize=10)
ax1.set_xlabel('Sign-Up Date', fontsize=8)
ax1.set_ylabel('Count', fontsize=8)

# Last Trip Date
ax2.plot_date(x=last_date_group_size['last_trip_date'], y=last_date_group_size['count'])
ax2.set_title('Frequency Distribution of Last Trip of User', fontsize=10)
ax2.set_xlabel('Sign-Up Date', fontsize=8)
ax2.set_ylabel('Count', fontsize=8)

plt.show()
26/22:
# Max Last Trip
last_date_group_size['last_trip_date'].max()
26/23:
# Last Trip Ordered
dti = pd.date_range('2014-01-01', periods=30, freq='D')
dti
26/24:
# Import necessary modules
from pylab import rcParams
26/25:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
26/26:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
26/27:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
26/28:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
26/29: ultimate_df.head()
26/30:
# Create a list for binary column 
active_inactive = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if dti.contains(last_trip):
        add_to_list(active_inactive, 1) # Add 1 for Active users
    else:
        add_to_list(active_inactive, 0) # Add 0 for Inactive users

# Test output
len(active_inactive)
26/31:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.tail()
26/32: ultimate_df.info()
26/33:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = pd.Series(active_inactive, ax=1)
ultimate_df.tail()
26/34:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = pd.Series(active_inactive, axis=1)
ultimate_df.tail()
26/35:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive)
ultimate_df.tail()
26/36:
# Define types of phones
ultimate_df['phone'].unique()
26/37:
# Convert phone column to binary
bi_phone = []

for phone in ultimate_df['phone']:
    
    if phone == 'iPhone':
        add_to_list(bi_phone, 0)
    else:
        add_to_list(bi_phone, 1)
26/38:
# Replace contents of 'phone' column with bi_phone
ultimate_df['phone'] = list_to_series(bi_phone)
26/39:
# Replace contents of 'phone' column with bi_phone
ultimate_df['phone'] = list_to_series(bi_phone)
ultimate_df.tail()
26/40:
# Convert phone column to binary
bi_phone = []

for phone in ultimate_df['phone']:
    
    if phone == 'iPhone':
        add_to_list(bi_phone, 0)
    else:
        add_to_list(bi_phone, 1)

# Test Output
bi_phone[0]
26/41:
# Convert phone column to binary
bi_phone = []

for phone in ultimate_df['phone']:
    
    if phone == 'iPhone':
        add_to_list(bi_phone, 0)
    else:
        add_to_list(bi_phone, 1)

# Test Output
bi_phone[0]
26/42:
# Replace contents of 'phone' column with bi_phone
ultimate_df['phone'] = list_to_series(bi_phone)
ultimate_df.head()
26/43:
# Convert phone column to binary
bi_phone = []

for phone in ultimate_df['phone']:
    
    if phone == 'iPhone':
        add_to_list(bi_phone, 0)
    else:
        add_to_list(bi_phone, 1)

# Test Output
bi_phone
26/44:
# Convert phone column to binary
bi_phone = []

for phone in ultimate_df['phone']:
    
    print(phone)
    if phone == 'iPhone':
        add_to_list(bi_phone, 0)
    else:
        add_to_list(bi_phone, 1)

# Test Output
#bi_phone
26/45:
# Replace contents of 'phone' column with bi_phone
ultimate_df['phone'] = list_to_series(bi_phone)
ultimate_df.head()
26/46:
# Convert the list of lists of key:value pairs to a dataframe
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
26/47:
# Assess Dataframe
ultimate_df.info()
26/48:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
26/49:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
26/50:
# Import necessary modules
from datetime import datetime
28/1:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
28/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import math
import matplotlib.pyplot as plt
import numpy as np
from numpy import histogram
import random
import seaborn as sns
import scipy.stats as stats
import statistics
from statsmodels.distributions.empirical_distribution import ECDF
from __future__ import division
28/3:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
30/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import math
import matplotlib.pyplot as plt
import numpy as np
from numpy import histogram
import random
import seaborn as sns
import scipy.stats as stats
import statistics
from statsmodels.distributions.empirical_distribution import ECDF
from __future__ import division
30/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import math
import matplotlib.pyplot as plt
import numpy as np
from numpy import histogram
import random
import seaborn as sns
import scipy.stats as stats
import statistics
from statsmodels.distributions.empirical_distribution import ECDF
30/3:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
30/4:
# 1. Combine patient birthdate information into one column
birth = DataFrame(mortalitad_materna, columns=['Año de nacimiento', 'Mes de nacimiento', 'Día de nacimiento'])
birth = mortalitad_materna['Año de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str)
print(birth.sort_values(ascending=True).head(2))
len(birth)
30/5:
# Combine patient date of death information into one column
death = DataFrame(mortalitad_materna, columns=['Año de la defunción', 'Mes de la defunción', 'Día de la defunción'])
death = mortalitad_materna['Año de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str)
print(death.sort_values(ascending=True).head(2))
len(death)
30/6:
# Create variable to store:

#residence information 
residence_code = mortalitad_materna['Entidad de residencia']
residence_name = mortalitad_materna['Descripción de entidad de residencia']

#local community info
local_size = mortalitad_materna['Descripción del tamaño de localidad']

#educational level
edu_reached_code = mortalitad_materna['Escolaridad'] 
edu_reached = mortalitad_materna['Descripción de la escolaridad']

#age fulfilled by patient
last_age = mortalitad_materna['Edad cumplida']

#mortality reason
mortality_reason = mortalitad_materna['Razón de mortalidad materna']

#medical assistance
medical_received = mortalitad_materna['Descripción de la asistencia médica']
30/7:
# Create a sub-dataframe to hold all date- information 
materna = pd.concat([birth, 
                   death, 
                   residence_code,
                   residence_name,
                   local_size,
                   edu_reached_code,
                   edu_reached,
                   last_age,
                   mortality_reason,
                   medical_received], axis=1)
materna.columns = ['Date of Birth', 
                 'Date of Mortality', 
                 'Residence Code',
                 'Residence Name',
                 'Local Community Size',
                 'Education Code',
                 'Education Completed',
                 'Age at Death',
                 'Reason for Mortality',
                 'Medical Assistance Received']
    
materna.head(2)
30/8:
# Order dataframe to list in ascending order of approx. age at death
materna = materna.sort_values(by=['Age at Death'],ascending=True)
materna.head()
30/9:
# Reset Index 
materna = materna.reset_index(drop=True)
materna.head()
materna.tail()
30/10:
# Remove rows with NaN / '0-0-0' values in Date of Birth
materna = materna[materna['Date of Birth'] != '0-0-0']
materna.tail()
30/11: materna.head()
30/12:
# Create a variable for the description of Reason for Mortality Description
mortality_description = mortalitad_materna['Descripción de la razón de mortalidad materna']

# Create a sub-dataframe to show interaction of Reason for Mortality Code and Description
mortality = pd.concat([mortality_reason, mortality_description], axis=1)
mortality.columns = ['Reason Mortality Code', 'Reason Mortality Description']
mortality.head()
30/13: mortality.tail()
30/14:
print('0 Description:')
print('Spanish: Muertes Maternas excluidas para la razón de Mortalidad Materna')
print('English: Maternal deaths excluded for the reason of Maternal Mortality')
30/15:
print('1 Description:')
print('Spanish: Muertes Maternas para la razón de Mortalidad Materna')
print('English: Maternal deaths for the reason of Maternal Mortality')
30/16:
# Remove rows with 0 values in Reason for Mortality
materna = materna[materna['Reason for Mortality'] != 0 ]
materna.tail()
30/17:
# Create a sub-dataframe to show interaction of Education Code and Education Completed
education = materna[['Education Code', 'Education Completed']].sort_values(by='Education Code')
education = education.drop_duplicates()
print(len(education))
education
30/18:
# Overwriting column with replaced value of Education

# SE IGNORA / NINGUNA / NO ESPECIFICADO
materna["Education Completed"]= materna["Education Completed"].replace(['SE IGNORA', 'NINGUNA', 'NO ESPECIFICADO'], 0)

# PREESCOLAR
materna["Education Completed"]= materna["Education Completed"].replace('PREESCOLAR', 1)

# PRIMARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA INCOMPLETA', 2)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA COMPLETA', 3)

# SECUNDARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA INCOMPLETA', 4)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA COMPLETA', 5)

# BACHILLERATO O PREPARATORIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA INCOMPLETA', 6)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA COMPLETA', 7)

# PROFESIONAL
materna["Education Completed"]= materna["Education Completed"].replace('PROFESIONAL', 8)

#POSGRADO
materna["Education Completed"]= materna["Education Completed"].replace('POSGRADO', 9)
30/19:
# Test output
list(materna['Education Completed'].sort_values().unique())
30/20:
# Create a list item to hold comparison response
binary_medassist = []

# Create an iteration function to compare region mean to popupation mean
for medassist in materna['Medical Assistance Received']:
    
    #test for assistance
    if medassist == 'CON ATENCION MEDICA':
        binary_medassist.append(0)
    else:
        binary_medassist.append(1)
30/21:
# Test output
#binary_medassist
30/22:
# Convert the list to a Series and add as new column
materna['Received(0)/Not(1) Medical Assistance'] = pd.Series(binary_medassist)
materna.head()
30/23:
# Drop 'Metropolitan Areas' column as it is unnecessary
materna = materna.drop(columns=['Date of Birth', 'Date of Mortality', 'Medical Assistance Received', 'Education Code', 'Reason for Mortality'])
materna.head()
30/24:
# Analyze shape of cleaned data
materna.describe()
30/25:
# Store new dates as a global variable that can be uploaded to other Jupyter Notebooks
%store materna
29/1:
# Import the relevant python libraries for the analysis
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
from numpy import histogram
import random
import seaborn as sns
import scipy.stats as stats
import statistics
from statsmodels.distributions.empirical_distribution import ECDF
29/2:
# Load the materna dataset
%store -r  materna
29/3:
# Create variable for maternal death
age_mortality = materna['Age at Death']

# Determine sample size for maternal death 
sample_size = len(age_mortality)
sample_size
29/4:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(age_mortality, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(age_mortality, np.mean(age_mortality), np.std(age_mortality))
pl.plot(age_mortality, fit, '-o')
pl.hist(age_mortality, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Age of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Age Distribution Associated with the Incidence of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(sample_size) + ') population mean age of maternal death is ' + str(round(np.mean(age_mortality), 2)) + ' years old with a standard deviation of ' + str(round(np.std(age_mortality), 2)) + '.'
29/5:
# Create an Empirical Cumulative Distribution Function (ECDF)
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1, len(data)+1) / len(data)

    return x, y
29/6:
# Seed the random number generator
np.random.seed(15)

# Compute the theoretical CDF 
cdf_mean = np.mean(age_mortality)
cdf_std = np.std(age_mortality)

# Simulate a random sample with the same distribution and size of 10,000
cdf_samples = np.random.normal(cdf_mean, cdf_std, size=10000)
cdf_samples
29/7:
# Compute the CDFs
x_death, y_death = ecdf(age_mortality)
x_norm, y_norm = ecdf(cdf_samples)
29/8:
# Plot both ECDFs on same the same figure
fig = plt.plot(x_death, y_death, marker='.', linestyle='none', alpha=0.5)
fig = plt.plot(x_norm, y_norm, marker='.', linestyle='none', alpha=0.5)

# Label figure
fig = plt.xlabel('Age of Maternal Death')
fig = plt.ylabel('CDF')
fig = plt.legend(('Sample Population', 'Expected Norm'))
fig = plt.title('Distribution of Maternal-Associated Deaths in Mexico')

# Save plots
plt.show()
29/9: materna.head()
29/10:
print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
32/1:
# Create a bootstrap replicate function for repeatability
def bootstrap_replicate_1d(data, func):
    """Create a bootstrap replicates."""
    
    # Create bootstrap sample
    boot_sample = np.random.choice(data, size=len(data))
    
    # Apply function to the computed bootstrap sample
    return func(boot_sample)


# Create a function to apply the bootstrap replicate function 'n' and return an array
def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""

    # Initialize array of replicates: bs_replicates
    boot_rep = np.empty(size)

    # Generate 'n' number of replicates
    for i in range(size):
        boot_rep[i] = bootstrap_replicate_1d(data, func)

    return boot_rep
32/2:
# Import the relevant python libraries for the analysis
import pandas as pd
import numpy as np
import pylab as pl
import random
import seaborn as sns
import scipy.stats as stats
import statistics
32/3:
# Load the materna dataset
%store -r  materna
32/4:
# Create variables for sample statistical information
materna_age_std = age_mortality.std()
materna_sample_size = len(age_mortality)
materna_age_var = np.var(age_mortality)
mean_age = materna['Age at Death'].mean()

# Create an array of the sample mean that is equal to the boostrap array length
materna_mean_arr = np.full(10000, mean_age)

print('sample size: ', materna_sample_size)
print('sample mean age of death: ', mean_age)
print('sample standard deviation: ', materna_age_std)
print('sample variation: ', materna_age_var)
32/5:
# Load the materna dataset
%store -r  materna
materna.head(2)
32/6:
# Define a variable for the materna['Age at Death'] Series
age_mortality = materna['Age at Death']
32/7:
# Define a variable for the materna['Age at Death'] Series
age_mortality = materna['Age at Death']
32/8:
# Create variables for sample statistical information
materna_age_std = age_mortality.std()
materna_sample_size = len(age_mortality)
materna_age_var = np.var(age_mortality)
mean_age = materna['Age at Death'].mean()

# Create an array of the sample mean that is equal to the boostrap array length
materna_mean_arr = np.full(10000, mean_age)

print('sample size: ', materna_sample_size)
print('sample mean age of death: ', mean_age)
print('sample standard deviation: ', materna_age_std)
print('sample variation: ', materna_age_var)
32/9:
# Create 10000 bootstrap replicates of the mean and take the mean of the returned array
boot_tenthousand =  draw_bs_reps(age_mortality, np.mean, size=10000)
print('bootstrap mean age of death: ' + str(np.mean(boot_tenthousand)))
32/10:
# Compute p-value
p_val = np.sum(boot_tenthousand >= materna_mean_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_age-(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
conf_int_high = mean_age+(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
32/11:
# Import the relevant python libraries for the analysis
import math
import numpy as np
import pandas as pd
import pylab as pl
import random
import seaborn as sns
import scipy.stats as stats
import statistics
32/12:
# Compute p-value
p_val = np.sum(boot_tenthousand >= materna_mean_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_age-(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
conf_int_high = mean_age+(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
30/26:
print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
30/27:
# Remove unnecessary rows from region_ages sub-dataset
materna = materna[materna['Residence Name'] != 'Estados Unidos de Norteamérica' ]
materna = materna[materna['Residence Name'] != 'Otros paises latinoamericanos' ]
materna = materna[materna['Residence Name'] != 'No especificado' ]
materna = materna[materna['Residence Name'] != 'Otros paises' ]

print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
29/11:
# Create a bar graph to show distribution of incidences of maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(materna['Residence Name'])

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
29/12:
# Create a boxplot to show the distribution of each region compared to its mean
fig, ax = plt.subplots(figsize=(16, 8))
plt.xticks(rotation='vertical')
fig.subplots_adjust(bottom=0.2)
sns.boxplot(x=materna['Residence Name'], y=materna['Age at Death'], data=materna)

# Label axis 
pl.title('Age Distribution of Maternal Mortality within Each Providence of Mexico')
30/28:
# Create a sample region array variables to hold age distribution per region 
# as a model to construct a function to parse through subdataset, region_ages
aqua = materna[materna['Residence Name'] == 'Aguascalientes']
aqua = aqua['Age at Death']
aqua = np.array(aqua)

mex = materna[materna['Residence Name'] == 'México']
mex = mex['Age at Death']
mex = np.array(mex)

print('Aguascalientes Sample Length: '+ str(aqua))
print('México Sample Length: ' + str(mex))
30/29:
# Create a function to group all ages associated with materna death within a Province and store the ages in an array
def age_array(str):
    """Create arrays for all Ages of Maternal Death within a Region"""
    
    ages = materna[materna['Residence Name'] == str] # select the region 'str' from the 'Region' column
    ages = ages['Age at Death'] # select the ages within the region
    ages = np.array(ages) # store the ages in an array
    return ages # return the unique array
30/30:
# Test output
print('Aguascalientes', age_array('Aguascalientes'))
print('México', age_array('México'))
30/31:
# Create a variable for 'Region' names using np.unique()
list_regions = np.unique(materna['Residence Name'])

# Create an empty dictionary to hold the {Region : region_age_array} key pairs
age_by_state = {}
30/32:
# Use the age_array function with iteration over residence to create the {Region : region_age_array} key pairs
for region in list_regions:
    age_by_state[region] = age_array(region) # add arrays as values in dictionary with region-key
30/33:
# Test output
print('Aguascalientes', age_by_state['Aguascalientes'])
30/34:
# Store new dates as a global variable that can be uploaded to other Jupyter Notebooks
%store materna
29/13:
# Load the materna dataset
%store -r  materna
29/14:
# Create variable for maternal death
age_mortality = materna['Age at Death']

# Determine sample size for maternal death 
sample_size = len(age_mortality)
sample_size
29/15:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(age_mortality, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(age_mortality, np.mean(age_mortality), np.std(age_mortality))
pl.plot(age_mortality, fit, '-o')
pl.hist(age_mortality, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Age of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Age Distribution Associated with the Incidence of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(sample_size) + ') population mean age of maternal death is ' + str(round(np.mean(age_mortality), 2)) + ' years old with a standard deviation of ' + str(round(np.std(age_mortality), 2)) + '.'
29/16:
# Create an Empirical Cumulative Distribution Function (ECDF)
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1, len(data)+1) / len(data)

    return x, y
29/17:
# Seed the random number generator
np.random.seed(15)

# Compute the theoretical CDF 
cdf_mean = np.mean(age_mortality)
cdf_std = np.std(age_mortality)

# Simulate a random sample with the same distribution and size of 10,000
cdf_samples = np.random.normal(cdf_mean, cdf_std, size=10000)
cdf_samples
29/18:
# Compute the CDFs
x_death, y_death = ecdf(age_mortality)
x_norm, y_norm = ecdf(cdf_samples)
29/19:
# Plot both ECDFs on same the same figure
fig = plt.plot(x_death, y_death, marker='.', linestyle='none', alpha=0.5)
fig = plt.plot(x_norm, y_norm, marker='.', linestyle='none', alpha=0.5)

# Label figure
fig = plt.xlabel('Age of Maternal Death')
fig = plt.ylabel('CDF')
fig = plt.legend(('Sample Population', 'Expected Norm'))
fig = plt.title('Distribution of Maternal-Associated Deaths in Mexico')

# Save plots
plt.show()
29/20: materna.head()
29/21:
# Create a bar graph to show distribution of incidences of maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(materna['Residence Name'])

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
29/22:
# Create a boxplot to show the distribution of each region compared to its mean
fig, ax = plt.subplots(figsize=(16, 8))
plt.xticks(rotation='vertical')
fig.subplots_adjust(bottom=0.2)
sns.boxplot(x=materna['Residence Name'], y=materna['Age at Death'], data=materna)

# Label axis 
pl.title('Age Distribution of Maternal Mortality within Each Providence of Mexico')
30/35:
# Var for residence name 
residence_uniq = np.unique(materna['Residence Name'])

# Var for residence code
residence_code = np.unique(materna['Residence Code'])

# Create the sub-dateframe for region and region code
res_dataset = pd.DataFrame(residence_uniq, index=residence_code)
res_dataset = res_dataset.rename(columns={0:'Region'})
res_dataset.head()
30/36:
# Calculate the mean age per region
# Test Code
mean_death_list_trial = []

aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_mean = aguas['Age at Death'].mean()
aguas_mean = '{0:0.2f}'.format(aguas_mean)
print(aguas_mean)

mean_death_list_trial.append(aguas_mean)
print(mean_death_list_trial)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_mean = baja['Age at Death'].mean()
baja_mean = '{0:0.2f}'.format(baja_mean)
print(baja_mean)

mean_death_list_trial.append(baja_mean)
print(mean_death_list_trial)
30/37:
# Create an empty list to store region sample size and mean age of maternal death
region_mean = []
region_n = []

# Calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Length of Age Array and Mean Age per Region"""

    sub_df = materna[materna['Residence Code'] == (i - 1)] # select one region
    n = len(sub_df['Age at Death']) # calculate sample length
    mean = sub_df['Age at Death'].mean() # calculate mean of region
    region_n.append(round(n, 2)) # append n to list
    region_mean.append(round(mean, 2)) # append mean to list
30/38:
# Test output
print(region_mean[1], region_n[1], type(region_mean[1]))
print(region_mean[2], region_n[2], type(region_mean[2]))
30/39:
# Convert the list to a Series and add as new column
res_dataset['μ Age Maternal Mortality'] = pd.Series(region_mean)
res_dataset['Region (n)'] = pd.Series(region_n)
res_dataset
30/40:
# Calculate the mean Age of Death for region 'Zacatecas'
zaca = materna[materna['Residence Code'] == 32 ]
zaca = zaca['Age at Death']

# Calculate sample size
zaca_n = len(zaca)

# Calculate mean
zaca_mean = zaca.mean()
zaca_mean = round(mean, 2)
print(zaca_mean)

# Change contents of res_dataset NaN to calculated mean
res_dataset['μ Age Maternal Mortality'] = res_dataset['μ Age Maternal Mortality'].replace(np.nan, zaca_mean)
res_dataset['Region (n)'] = res_dataset['Region (n)'].replace(np.nan, zaca_n)
res_dataset.tail()
30/41:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
30/42:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store materna
29/23:
# Load datasets
%store -r  materna
%store -r  res_dataset
29/24:
# Create a bar graph to show mean age maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
plt.scatter(res_dataset['Region'], res_dataset['μ Age Maternal Mortality'])

# Label axis 
pl.title('Regions Compared to Mexico Mean Age Maternal Mortality')
32/13:
# Open metro_by_region with specific population values of each Mexican State by region
%store -r state_pop
state_pop
32/14:
# Create a variable to hold list of Regions with normally-distributed sample sizes
norm_distr_regions = []

# Create a variable to hold list of Regions without normally-distributed sample sizes
not_norm_distr_regions = []
32/15:
# Determine if each Province has a normally distributed sample population of ages
for region in age_by_state:
    """Determine if Region Age Distribution is Normal"""
    
    region_name = str(region)
    arr = age_by_state[region_name]
    
    if len(arr) > 8: # skewtest (k2): not valid with less than 8 samples 
        k2, p = stats.normaltest(arr)
        alpha = 0.05 # 95% confidence
        print("p = {:g}".format(p))
        print("n = " + str(len(arr)))
        
        if p < alpha: # if norm
            print(str(region)+ " IS normally distributed.")
            norm_distr_regions.append(region_name) # add region to norm list  
        else:
            print(str(region)+ " *IS NOT* normally distributed.")
            not_norm_distr_regions.append(region_name) # add region to norm list     
    else: 
        print(str(region)+ " *sample size is too small*")
        not_norm_distr_regions.append(region_name) # add region to non-norm list of regions
32/16: print('Not Normally Distributed: ', list(np.unique(not_norm_distr_regions)))
32/17:
# Test code
aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_var = statistics.pvariance(aguas['Age at Death'])
print('Aguas', aguas_var)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_var = statistics.pvariance(baja['Age at Death'])
print('Baja Cal', baja_var)
32/18:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
30/43:
# Test code
aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_var = statistics.pvariance(aguas['Age at Death'])
print('Aguas', aguas_var)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_var = statistics.pvariance(baja['Age at Death'])
print('Baja Cal', baja_var)
30/44:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
30/45:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
30/46:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
30/47:
# Test output - Make sure it matches Test Results
print('Test Results - Aguas', round(aguas_var,2))
print('Function Results - Aguas', region_var[0])
30/48:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
32/19:
# Load datasets
%store -r  materna
%store -r state_pop
32/20:
# Load datasets
%store -r  materna
%store -r state_pop
%store -r res_dataset
32/21: materna.head()
32/22: materna.head(2)
32/23:
# Create variables for minimum and maximum variation values in res_dataset
max_variance = res_dataset['μ Age Variance'].max()
print('Max Variance: ', max_variance)

min_variance = res_dataset['μ Age Variance'].min()
print('Min Variance: ', min_variance)
print('Double Min Variance: ', min_variance*2)

# Check if largest variance is more than double the smallest
if (2*min_variance) >= max_variance:
    print('Accept ANOVA: The max variance is less than double the min variance.')
else: 
    print('Reject ANOVA: The max variance is more than double the min variance.')
32/24: res_dataset.head(2)
30/49:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
30/50:
# Convert the list to a Series and add as new column
res_dataset['μ Age Variance'] = pd.Series(region_var, index=np.arange(1,33))
res_dataset.head()
30/51:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
32/25:
# Load datasets
%store -r  materna
%store -r state_pop
%store -r res_dataset
32/26:
# Create variables for minimum and maximum variation values in res_dataset
max_variance = res_dataset['μ Age Variance'].max()
print('Max Variance: ', max_variance)

min_variance = res_dataset['μ Age Variance'].min()
print('Min Variance: ', min_variance)
print('Double Min Variance: ', min_variance*2)

# Check if largest variance is more than double the smallest
if (2*min_variance) >= max_variance:
    print('Accept ANOVA: The max variance is less than double the min variance.')
else: 
    print('Reject ANOVA: The max variance is more than double the min variance.')
32/27:
# Define the number of conditions (k) based on Region/State
k = len(pd.unique(materna['Residence Name']))
print('Number of Conditions(k): ', k)

# Calculate the conditions times data points (N)
N = len(materna.values)
print('Number of Conditions times Data Points(N): ', N)

# Participants in each condition
n = materna.groupby('Residence Name').size()[0] 
print('Number of Participants in Each Condition(n): ', n)
32/28:
#Create a dict variable for key:value pairs of state:age_array
grps = pd.unique(materna['Residence Name'].values)
state_mean_ages = {grp:materna['Age at Death'][materna['Residence Name'] == grp] for grp in grps}
32/29:
F, p = stats.f_oneway(state_mean_ages['Aguascalientes'], 
                      state_mean_ages['Baja California'], 
                      state_mean_ages['Baja California Sur'],
                      state_mean_ages['Campeche'],
                      state_mean_ages['Chiapas'],
                      state_mean_ages['Chihuahua'],
                      state_mean_ages['Coahuila de Zaragoza'],
                      state_mean_ages['Colima'],
                      state_mean_ages['Distrito Federal'],
                      state_mean_ages['Durango'],
                      state_mean_ages['Guanajuato'],
                      state_mean_ages['Guerrero'],
                      state_mean_ages['Hidalgo'],
                      state_mean_ages['Jalisco'],
                      state_mean_ages['Michoacán de Ocampo'],
                      state_mean_ages['Morelos'],
                      state_mean_ages['México'],
                      state_mean_ages['Nayarit'],
                      state_mean_ages['Nuevo León'],
                      state_mean_ages['Oaxaca'],
                      state_mean_ages['Puebla'],
                      state_mean_ages['Querétaro Arteaga'],
                      state_mean_ages['Quintana Roo'],
                      state_mean_ages['San Luis Potosí'],
                      state_mean_ages['Sinaloa'],
                      state_mean_ages['Sonora'],
                      state_mean_ages['Tabasco'],
                      state_mean_ages['Tamaulipas'],
                      state_mean_ages['Tlaxcala'],
                      state_mean_ages['Veracruz de Ignacio de la Llave'],
                      state_mean_ages['Yucatán'],
                      state_mean_ages['Zacatecas'])

print('ANOVA F-value: ', str(F))
print('ANOVA p-value: ', str(p))
print('alpha level: 0.05')
30/52: print('The population mean is: ', round(mean_age, 2))
30/53: print('The population mean is: ', round(res_dataset['μ Age Maternal Mortality   '], 2))
30/54: print('The population mean is: ', round(res_dataset['μ Age Maternal Mortality'], 2))
30/55:
res_mean_age = res_dataset['μ Age Maternal Mortality'].mean()
print('The population mean is: ', round(, 2))
30/56:
res_mean_age = res_dataset['μ Age Maternal Mortality'].mean()
print('The population mean is: ', round(res_mean_age, 2))
30/57:
# Create a dictionary item to hold comparison response
binary_mean = []

# Compare region mean to population mean
for mean in res_dataset['μ Age Maternal Mortality']:
    if mean >= res_mean_age:
        binary_mean.append(0)
    else:
        binary_mean.append(1)
30/58:
# Test output
#binary_mean
30/59:
# Convert the list to a Series and add as new column
res_dataset['Above(0) or Below(1) Average'] = pd.Series(binary_mean, index=np.arange(1,33))
res_dataset.head()
30/60:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
29/25:
# Load datasets
%store -r  materna
%store -r  res_dataset
29/26:
# Create a bar graph to show mean age maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(res_dataset['Above(0) or Below(1) Average'])

# Label axis 
pl.title('Mean Age Maternal Mortality in Each Providence of Mexico')
29/27:
# List Regions with a mean maternal mortality lower than population mean
len(res_dataset[res_dataset['Above(0) or Below(1) Average'] == 1])
33/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import matplotlib.pyplot as plt
import numpy as np
33/2:
# Load datasets
%store -r  materna
%store -r res_dataset
33/3:
# Create an adolescent sub dataframe from materna
adolescent_matern_mortality = materna[materna['Age at Death'] <= 20 ]
adolescent_matern_mortality.head()
33/4:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']

# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
33/5:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']
print(adolescent_ages)
# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
33/6:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']

# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
29/28:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']

# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
33/7:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']

# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
33/8:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(adolescent_ages, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(adolescent_ages, np.mean(adolescent_ages), np.std(adolescent_ages))
pl.plot(adolescent_ages, fit, '-o')
pl.hist(adolescent_ages, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Adolescent Ages of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Adolescent Age Distribution of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(adolsecent_sample_size) + ') population mean age of adolescent maternal mortality is ' + str(round(np.mean(adolescent_ages), 2)) + ' years old with a standard deviation of ' + str(round(np.std(adolescent_ages), 2)) + '.'
33/9:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
33/10:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(adolescent_ages, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(adolescent_ages, np.mean(adolescent_ages), np.std(adolescent_ages))
pl.plot(adolescent_ages, fit, '-o')
pl.hist(adolescent_ages, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Adolescent Ages of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Adolescent Age Distribution of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(adolsecent_sample_size) + ') population mean age of adolescent maternal mortality is ' + str(round(np.mean(adolescent_ages), 2)) + ' years old with a standard deviation of ' + str(round(np.std(adolescent_ages), 2)) + '.'
33/11:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import stats
33/12:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import statistics as stats
33/13:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(adolescent_ages, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(adolescent_ages, np.mean(adolescent_ages), np.std(adolescent_ages))
pl.plot(adolescent_ages, fit, '-o')
pl.hist(adolescent_ages, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Adolescent Ages of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Adolescent Age Distribution of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(adolsecent_sample_size) + ') population mean age of adolescent maternal mortality is ' + str(round(np.mean(adolescent_ages), 2)) + ' years old with a standard deviation of ' + str(round(np.std(adolescent_ages), 2)) + '.'
33/14:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import scipy.stats as stats
import statistics
33/15:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(adolescent_ages, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(adolescent_ages, np.mean(adolescent_ages), np.std(adolescent_ages))
pl.plot(adolescent_ages, fit, '-o')
pl.hist(adolescent_ages, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Adolescent Ages of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Adolescent Age Distribution of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(adolsecent_sample_size) + ') population mean age of adolescent maternal mortality is ' + str(round(np.mean(adolescent_ages), 2)) + ' years old with a standard deviation of ' + str(round(np.std(adolescent_ages), 2)) + '.'
33/16:
# Create variables for sample statistical information
adolescent_ages_std = adolescent_ages.std()
mean_adolescent_ages = adolescent_ages.mean()

# Create an array of the sample mean that is equal to the boostrap array length
adolescent_ages_arr = np.full(10000, mean_adolescent_ages)

print('sample size: ', adolsecent_sample_size)
print('sample mean age of death: ', mean_adolescent_ages)
print('sample standard deviation: ', adolescent_ages_std)
33/17:
# Create 10000 bootstrap replicates of the mean and take the mean of the returned array
boot_tenthousand =  draw_bs_reps(adolescent_ages, np.mean, size=10000)
print('bootstrap mean adolescent age of death: ' + str(np.mean(boot_tenthousand)))
33/18:
# Bootstrap replicate function for repeatability
def bootstrap_replicate_1d(data, func):
    """Create a bootstrap replicates."""
    
    boot_sample = np.random.choice(data, size=len(data))  # create bootstrap sample
    return func(boot_sample) # apply function to bootstrap

# Apply bootstrap replicate function 'n' and return an array
def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""

    boot_rep = np.empty(size) # initialize array of replicates: bs_replicates
    for i in range(size):  # generate 'n' number of replicates
        boot_rep[i] = bootstrap_replicate_1d(data, func)
    return boot_rep
33/19:
# Create 10000 bootstrap replicates of the mean and take the mean of the returned array
boot_tenthousand =  draw_bs_reps(adolescent_ages, np.mean, size=10000)
print('bootstrap mean adolescent age of death: ' + str(np.mean(boot_tenthousand)))
33/20:
# Compute p-value
p_val = np.sum(boot_tenthousand >= adolescent_ages_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_adolescent_ages-(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
conf_int_high = mean_adolescent_ages+(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
33/21:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import math
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import scipy.stats as stats
import statistics
33/22:
# Compute p-value
p_val = np.sum(boot_tenthousand >= adolescent_ages_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_adolescent_ages-(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
conf_int_high = mean_adolescent_ages+(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
33/23:
#Create a boxplot
adolescent_matern_mortality.boxplot('Age at Death', by='Residence Name', figsize=(12, 8))
plt.xticks(rotation='vertical')
33/24:
#Create a dict variable for key:value pairs of state:age_array
grps = pd.unique(adolescent_matern_mortality['Residence Name'].values)
state_mean_adolescent_ages = {grp:adolescent_matern_mortality['Age at Death'][adolescent_matern_mortality['Residence Name'] == grp] for grp in grps}
33/25:
# Create an empty list to store mean age and sample size of maternal death per region
ado_region_mean = []
ado_region_n = []

# Create an iteration function
for i in adolescent_matern_mortality['Residence Code'].sort_values().unique():
    """Calculate Mean Age Adolescent Maternal Mortality per Region"""
    
    sub_df = adolescent_matern_mortality[adolescent_matern_mortality['Residence Code'] == i]
    age = sub_df['Age at Death']
    ado_n = len(age)  # sample length
    ado_mean = age.mean() # calculate mean
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        ado_region_mean.append(round(ado_mean, 2)) #  append mean to region list
        ado_region_n.append(round(ado_n, 2)) # append ado_n to region list
33/26:
# Test output
len(ado_region_mean), ado_region_mean[31], len(ado_region_n), ado_region_n[31]
33/27:
# Convert the list to a Series and add as new column
res_dataset['μ Age Adolescent Maternal Death'] = pd.Series(ado_region_mean, index=np.arange(1,33))
res_dataset['Region Ado (n)'] = pd.Series(ado_region_n, index=np.arange(1,33))
res_dataset.head()
33/28:
# Test output
res_dataset.tail()
30/61:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    education = sub_df['Education Completed'].mean()
    region_education.append(round(education, 2))
30/62:
# Test output
print(len(region_education))
#region_education
30/63:
# Convert the list to a Series and add as new column
res_dataset['μ Region Education Level'] = pd.Series(region_education, index=np.arange(1,33))
res_dataset.tail()
27/1:
# Create a column for active or inactive
ultimate_df['active1_inactive0'] = list_to_series(active_inactive, index=np.arange(1,41445))
ultimate_df.tail()
30/64:
# Create an empty list to store region sample size and mean age of maternal death
region_medical = []

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""
    
    sub_df = materna[materna['Residence Code'] == i ]
    med_assist = sub_df['Received(0)/Not(1) Medical Assistance'].mean()
    region_medical.append(round(med_assist, 2))
30/65:
# Test output
print(len(region_medical))
#region_medical
30/66:
# Convert the list to a Series and add as new column
res_dataset['μ Presence(0)/Not(1) of Medical Assistance ATD'] = pd.Series(region_medical, index=np.arange(1,33))
res_dataset.head()
30/67:
# Store new dates as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
36/1:
# Load datasets
%store -r  materna
%store -r  res_dataset
%store -r  adolescent_matern_mortality
36/2:
# Create a bar graph to show distribution of incidences of maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(res_dataset['μ Presence(0)/Not(1) of Medical Assistance ATD'])

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
36/3:
# Import the relevant python libraries for the analysis
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
from numpy import histogram
import seaborn as sns
from statsmodels.distributions.empirical_distribution import ECDF
36/4:
# Create a bar graph to show distribution of incidences of maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(res_dataset['μ Presence(0)/Not(1) of Medical Assistance ATD'])

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
30/68:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
education_dict

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    education_dict[i] = mean_edu
30/69:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
education_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    education_dict[i] = mean_edu
30/70:
# Test output
print(len(region_education))
#region_education
#mean_edu

# Store as a global variable
%store mean_edu
30/71:
# Test output
print(len(region_education))
#region_education
mean_edu

# Store as a global variable
%store mean_edu
30/72:
# Test output
print(len(region_education))
#region_education
print(mean_edu)

# Store as a global variable
%store mean_edu
30/73:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[i] = mean_edu
30/74:
# Test output
print(len(region_education))
#region_education
print(mean_edu)

# Store as a global variable
%store mean_edu
30/75:
# Test output
print(len(region_education))
#region_education
print(edu_dict)

# Store as a global variable
%store edu_dict
30/76:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Region ']
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/77:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Region']
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/78:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Region']
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/79:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Residence Name']
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/80:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Residence Name'].unique()
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/81:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Residence Name'].unique().str
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/82:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Residence Name']str.unique()
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/83:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = sub_df['Residence Name'].str.unique()
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/84:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/85:
# Test output
print(len(region_education))
#region_education
print(edu_dict)

# Store as a global variable
%store edu_dict
30/86:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = np.array2string(sub_df['Residence Name'].unique())
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
30/87:
# Test output
print(len(region_education))
#region_education
print(edu_dict)

# Store as a global variable
%store edu_dict
36/5:
# Plot medical care by region
x, y = zip(*edu_dict)
plt.plot(x, y)
plt.show()

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
36/6: %store -r edu_dict
36/7:
# Plot medical care by region
x, y = zip(*edu_dict)
plt.plot(x, y)
plt.show()

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
30/88:
# Create an empty list to store region sample size and mean age of maternal death
region_medical = []
medical_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""
    
    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    med_assist = round(sub_df['Received(0)/Not(1) Medical Assistance'].mean(), 2)
    region_medical.append(med_assist) 
    medical_dict[region] = med_assist
30/89:
# Test output
print(len(region_medical))
#region_medical

# Store as a global variable
%store edu_dict
30/90:
# Test output
print(len(region_medical))
#region_medical

# Store as a global variable
%store medical_dict
30/91:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store res_dataset
38/1:
# Import the relevant python libraries for the analysis
import pandas as pd
import numpy as np
38/2:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
38/3:
# Store as a global variable 
%store res_dataset
38/4:
# Import the relevant python libraries for the analysis
import pandas as pd
import numpy as np
38/5:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
38/6:
# 1. Combine patient birthdate information into one column
birth = DataFrame(mortalitad_materna, columns=['Año de nacimiento', 'Mes de nacimiento', 'Día de nacimiento'])
birth = mortalitad_materna['Año de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str)
print(birth.sort_values(ascending=True).head(2))
len(birth)
38/7:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
38/8:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
38/9:
# 1. Combine patient birthdate information into one column
birth = DataFrame(mortalitad_materna, columns=['Año de nacimiento', 'Mes de nacimiento', 'Día de nacimiento'])
birth = mortalitad_materna['Año de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str)
print(birth.sort_values(ascending=True).head(2))
len(birth)
38/10:
# Combine patient date of death information into one column
death = DataFrame(mortalitad_materna, columns=['Año de la defunción', 'Mes de la defunción', 'Día de la defunción'])
death = mortalitad_materna['Año de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str)
print(death.sort_values(ascending=True).head(2))
len(death)
38/11:
# Create variable to store:

#residence information 
residence_code = mortalitad_materna['Entidad de residencia']
residence_name = mortalitad_materna['Descripción de entidad de residencia']

#local community info
local_size = mortalitad_materna['Descripción del tamaño de localidad']

#educational level
edu_reached_code = mortalitad_materna['Escolaridad'] 
edu_reached = mortalitad_materna['Descripción de la escolaridad']

#age fulfilled by patient
last_age = mortalitad_materna['Edad cumplida']

#mortality reason
mortality_reason = mortalitad_materna['Razón de mortalidad materna']

#medical assistance
medical_received = mortalitad_materna['Descripción de la asistencia médica']
38/12:
# Create a sub-dataframe to hold all date- information 
materna = pd.concat([birth, 
                   death, 
                   residence_code,
                   residence_name,
                   local_size,
                   edu_reached_code,
                   edu_reached,
                   last_age,
                   mortality_reason,
                   medical_received], axis=1)
materna.columns = ['Date of Birth', 
                 'Date of Mortality', 
                 'Residence Code',
                 'Residence Name',
                 'Local Community Size',
                 'Education Code',
                 'Education Completed',
                 'Age at Death',
                 'Reason for Mortality',
                 'Medical Assistance Received']
    
materna.head(2)
38/13:
# Order dataframe to list in ascending order of approx. age at death
materna = materna.sort_values(by=['Age at Death'],ascending=True)
materna.head()
38/14:
# Reset Index 
materna = materna.reset_index(drop=True)
materna.head()
materna.tail()
38/15:
# Remove rows with NaN / '0-0-0' values in Date of Birth
materna = materna[materna['Date of Birth'] != '0-0-0']
materna.tail()
38/16: materna.head()
38/17:
# Create a variable for the description of Reason for Mortality Description
mortality_description = mortalitad_materna['Descripción de la razón de mortalidad materna']

# Create a sub-dataframe to show interaction of Reason for Mortality Code and Description
mortality = pd.concat([mortality_reason, mortality_description], axis=1)
mortality.columns = ['Reason Mortality Code', 'Reason Mortality Description']
mortality.head()
38/18: mortality.tail()
38/19:
print('0 Description:')
print('Spanish: Muertes Maternas excluidas para la razón de Mortalidad Materna')
print('English: Maternal deaths excluded for the reason of Maternal Mortality')
38/20:
print('1 Description:')
print('Spanish: Muertes Maternas para la razón de Mortalidad Materna')
print('English: Maternal deaths for the reason of Maternal Mortality')
38/21:
# Remove rows with 0 values in Reason for Mortality
materna = materna[materna['Reason for Mortality'] != 0 ]
materna.tail()
38/22:
# Create a sub-dataframe to show interaction of Education Code and Education Completed
education = materna[['Education Code', 'Education Completed']].sort_values(by='Education Code')
education = education.drop_duplicates()
print(len(education))
education
38/23:
# Overwriting column with replaced value of Education

# SE IGNORA / NINGUNA / NO ESPECIFICADO
materna["Education Completed"]= materna["Education Completed"].replace(['SE IGNORA', 'NINGUNA', 'NO ESPECIFICADO'], 0)

# PREESCOLAR
materna["Education Completed"]= materna["Education Completed"].replace('PREESCOLAR', 1)

# PRIMARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA INCOMPLETA', 2)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA COMPLETA', 3)

# SECUNDARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA INCOMPLETA', 4)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA COMPLETA', 5)

# BACHILLERATO O PREPARATORIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA INCOMPLETA', 6)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA COMPLETA', 7)

# PROFESIONAL
materna["Education Completed"]= materna["Education Completed"].replace('PROFESIONAL', 8)

#POSGRADO
materna["Education Completed"]= materna["Education Completed"].replace('POSGRADO', 9)
38/24:
# Test output
list(materna['Education Completed'].sort_values().unique())
38/25:
# Create a list item to hold comparison response
binary_medassist = []

# Create an iteration function to compare region mean to popupation mean
for medassist in materna['Medical Assistance Received']:
    
    #test for assistance
    if medassist == 'CON ATENCION MEDICA':
        binary_medassist.append(0)
    else:
        binary_medassist.append(1)
38/26:
# Test output
#binary_medassist
38/27:
# Convert the list to a Series and add as new column
materna['Received(0)/Not(1) Medical Assistance'] = pd.Series(binary_medassist)
materna.head()
38/28:
# Drop 'Metropolitan Areas' column as it is unnecessary
materna = materna.drop(columns=['Date of Birth', 'Date of Mortality', 'Medical Assistance Received', 'Education Code', 'Reason for Mortality'])
materna.head()
38/29:
# Analyze shape of cleaned data
materna.describe()
38/30:
print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
38/31:
# Remove unnecessary rows from region_ages sub-dataset
materna = materna[materna['Residence Name'] != 'Estados Unidos de Norteamérica' ]
materna = materna[materna['Residence Name'] != 'Otros paises latinoamericanos' ]
materna = materna[materna['Residence Name'] != 'No especificado' ]
materna = materna[materna['Residence Name'] != 'Otros paises' ]

print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
38/32:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store materna
38/33:
# Test code to create function
aqua = materna[materna['Residence Name'] == 'Aguascalientes']
aqua = aqua['Age at Death']
aqua = np.array(aqua)

mex = materna[materna['Residence Name'] == 'México']
mex = mex['Age at Death']
mex = np.array(mex)

print('Aguascalientes Sample Length: '+ str(aqua))
print('México Sample Length: ' + str(mex))
38/34:
# Create a function to group all ages associated with materna death within a Province and store the ages in an array
def age_array(str):
    
    """Create arrays for all Ages of Maternal Death within a Region"""
    
    ages = materna[materna['Residence Name'] == str] # select the region 'str' from the 'Region' column
    ages = ages['Age at Death'] # select the ages within the region
    ages = np.array(ages) # store the ages in an array
    return ages # return the unique array
38/35:
# Test output
print('Aguascalientes', age_array('Aguascalientes'))
print('México', age_array('México'))
38/36:
# Create a variable for 'Region' names using np.unique()
list_regions = np.unique(materna['Residence Name'])

# Create an empty dictionary to hold the {Region : region_age_array} key pairs
age_by_state = {}
38/37:
# Use the age_array function with iteration over residence to create the {Region : region_age_array} key pairs
for region in list_regions:
    age_by_state[region] = age_array(region) # add arrays as values in dictionary with region-key
38/38:
# Test output
print('Aguascalientes', age_by_state['Aguascalientes'])
38/39:
# Var for residence name 
residence_uniq = np.unique(materna['Residence Name'])

# Var for residence code
residence_code = np.unique(materna['Residence Code'])

# Create the sub-dateframe for region and region code
res_dataset = pd.DataFrame(residence_uniq, index=residence_code)
res_dataset = res_dataset.rename(columns={0:'Region'})
res_dataset.head()
38/40:
# Test Code
mean_death_list_trial = []

aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_mean = aguas['Age at Death'].mean()
aguas_mean = '{0:0.2f}'.format(aguas_mean)
print(aguas_mean)
mean_death_list_trial.append(aguas_mean)
print(mean_death_list_trial)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_mean = baja['Age at Death'].mean()
baja_mean = '{0:0.2f}'.format(baja_mean)
print(baja_mean)
mean_death_list_trial.append(baja_mean)
print(mean_death_list_trial)
38/41:
# Create an empty list to store region sample size and mean age of maternal death
region_mean = []
region_n = []

# Calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Length of Age Array and Mean Age per Region"""

    sub_df = materna[materna['Residence Code'] == (i - 1)] # select one region
    n = len(sub_df['Age at Death']) # calculate sample length
    mean = sub_df['Age at Death'].mean() # calculate mean of region
    region_n.append(round(n, 2)) # append n to list
    region_mean.append(round(mean, 2)) # append mean to list
38/42:
# Test output
print(region_mean[1], region_n[1], type(region_mean[1]))
print(region_mean[2], region_n[2], type(region_mean[2]))
38/43:
# Convert the list to a Series and add as new column
res_dataset['μ Age Maternal Mortality'] = pd.Series(region_mean)
res_dataset['Region (n)'] = pd.Series(region_n)
res_dataset.tail()
38/44:
# Calculate the mean Age of Death for region 'Zacatecas'
zaca = materna[materna['Residence Code'] == 32 ]
zaca = zaca['Age at Death']

# Calculate sample size
zaca_n = len(zaca)

# Calculate mean
zaca_mean = zaca.mean()
zaca_mean = round(mean, 2)
print(zaca_mean)

# Change contents of res_dataset NaN to calculated mean
res_dataset['μ Age Maternal Mortality'] = res_dataset['μ Age Maternal Mortality'].replace(np.nan, zaca_mean)
res_dataset['Region (n)'] = res_dataset['Region (n)'].replace(np.nan, zaca_n)
res_dataset.tail()
38/45:
# Test code
aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_var = statistics.pvariance(aguas['Age at Death'])
print('Aguas', aguas_var)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_var = statistics.pvariance(baja['Age at Death'])
print('Baja Cal', baja_var)
38/46:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
38/47:
# Test output - Make sure it matches Test Results
print('Test Results - Aguas', round(aguas_var,2))
print('Function Results - Aguas', region_var[0])
38/48:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import statistics
38/49:
# Test code
aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_var = statistics.pvariance(aguas['Age at Death'])
print('Aguas', aguas_var)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_var = statistics.pvariance(baja['Age at Death'])
print('Baja Cal', baja_var)
38/50:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
38/51:
# Test output - Make sure it matches Test Results
print('Test Results - Aguas', round(aguas_var,2))
print('Function Results - Aguas', region_var[0])
38/52:
# Convert the list to a Series and add as new column
res_dataset['μ Age Variance'] = pd.Series(region_var, index=np.arange(1,33))
res_dataset.head()
38/53:
res_mean_age = res_dataset['μ Age Maternal Mortality'].mean()
print('The population mean is: ', round(res_mean_age, 2))
38/54:
# Create a dictionary item to hold comparison response
binary_mean = []

# Compare region mean to population mean
for mean in res_dataset['μ Age Maternal Mortality']:
    if mean >= res_mean_age:
        binary_mean.append(0)
    else:
        binary_mean.append(1)
38/55:
# Test output
#binary_mean
38/56:
# Convert the list to a Series and add as new column
res_dataset['Above(0) or Below(1) Average'] = pd.Series(binary_mean, index=np.arange(1,33))
res_dataset.head()
38/57:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
38/58:
# Test output
print(len(region_education))
#region_education
print(edu_dict)

# Store as a global variable
%store edu_dict
38/59:
# Convert the list to a Series and add as new column
res_dataset['μ Region Education Level'] = pd.Series(region_education, index=np.arange(1,33))
res_dataset.tail()
38/60:
# Create an empty list to store region sample size and mean age of maternal death
region_medical = []
medical_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""
    
    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    med_assist = round(sub_df['Received(0)/Not(1) Medical Assistance'].mean(), 2)
    region_medical.append(med_assist) 
    medical_dict[region] = med_assist
38/61:
# Test output
print(len(region_medical))
#region_medical

# Store as a global variable
%store medical_dict
38/62:
# Convert the list to a Series and add as new column
res_dataset['μ Presence(0)/Not(1) of Medical Assistance ATD'] = pd.Series(region_medical, index=np.arange(1,33))
res_dataset.head()
38/63:
# Store as a global variable 
%store res_dataset
39/1:
# Import the relevant python libraries
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
from numpy import histogram
import seaborn as sns
from statsmodels.distributions.empirical_distribution import ECDF
39/2:
# Load datasets
%store -r  materna
%store -r  res_dataset
%store -r  adolescent_matern_mortality
39/3:
# Load datasets
%store -r  materna
%store -r  res_dataset
39/4:
# Create variable for maternal death
age_mortality = materna['Age at Death']

# Determine sample size for maternal death 
sample_size = len(age_mortality)
sample_size
39/5:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(age_mortality, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(age_mortality, np.mean(age_mortality), np.std(age_mortality))
pl.plot(age_mortality, fit, '-o')
pl.hist(age_mortality, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Age of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Age Distribution Associated with the Incidence of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(sample_size) + ') population mean age of maternal death is ' + str(round(np.mean(age_mortality), 2)) + ' years old with a standard deviation of ' + str(round(np.std(age_mortality), 2)) + '.'
39/6:
# Import the relevant python libraries
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
from numpy import histogram
import seaborn as sns
import statistics as stats
from statsmodels.distributions.empirical_distribution import ECDF
39/7:
# Load datasets
%store -r  materna
%store -r  res_dataset
39/8:
# Create variable for maternal death
age_mortality = materna['Age at Death']

# Determine sample size for maternal death 
sample_size = len(age_mortality)
sample_size
39/9:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(age_mortality, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(age_mortality, np.mean(age_mortality), np.std(age_mortality))
pl.plot(age_mortality, fit, '-o')
pl.hist(age_mortality, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Age of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Age Distribution Associated with the Incidence of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(sample_size) + ') population mean age of maternal death is ' + str(round(np.mean(age_mortality), 2)) + ' years old with a standard deviation of ' + str(round(np.std(age_mortality), 2)) + '.'
39/10:
# Import the relevant python libraries
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
from numpy import histogram
import seaborn as sns
import scipy.stats as stats
from statsmodels.distributions.empirical_distribution import ECDF
39/11:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(age_mortality, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(age_mortality, np.mean(age_mortality), np.std(age_mortality))
pl.plot(age_mortality, fit, '-o')
pl.hist(age_mortality, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Age of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Age Distribution Associated with the Incidence of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(sample_size) + ') population mean age of maternal death is ' + str(round(np.mean(age_mortality), 2)) + ' years old with a standard deviation of ' + str(round(np.std(age_mortality), 2)) + '.'
39/12:
# Create an Empirical Cumulative Distribution Function (ECDF)
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
   
    x = np.sort(data)
    y = np.arange(1, len(data)+1) / len(data)
    return x, y
39/13:
# Seed the random number generator
np.random.seed(15)

# Compute the theoretical CDF 
cdf_mean = np.mean(age_mortality)
cdf_std = np.std(age_mortality)

# Simulate a random sample with the same distribution and size of 10,000
cdf_samples = np.random.normal(cdf_mean, cdf_std, size=10000)
cdf_samples
39/14:
# Compute the CDFs
x_death, y_death = ecdf(age_mortality)
x_norm, y_norm = ecdf(cdf_samples)
39/15:
# Plot both ECDFs on same the same figure
fig = plt.plot(x_death, y_death, marker='.', linestyle='none', alpha=0.5)
fig = plt.plot(x_norm, y_norm, marker='.', linestyle='none', alpha=0.5)

# Label figure
fig = plt.xlabel('Age of Maternal Death')
fig = plt.ylabel('CDF')
fig = plt.legend(('Sample Population', 'Expected Norm'))
fig = plt.title('Distribution of Maternal-Associated Deaths in Mexico')

# Save plots
plt.show()
39/16: materna.head()
39/17:
# Create a bar graph to show distribution of incidences of maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(materna['Residence Name'])

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
39/18:
# Create a boxplot to show the distribution of each region compared to its mean
fig, ax = plt.subplots(figsize=(16, 8))
plt.xticks(rotation='vertical')
fig.subplots_adjust(bottom=0.2)
sns.boxplot(x=materna['Residence Name'], y=materna['Age at Death'], data=materna)

# Label axis 
pl.title('Age Distribution of Maternal Mortality within Each Providence of Mexico')
39/19:
# Create a bar graph to show mean age maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.xticks(rotation='vertical')
plt.grid(True)
plt.scatter(res_dataset['Region'], res_dataset['μ Age Maternal Mortality'])

# Label axis 
pl.title('Regions Compared to Mexico Mean Age Maternal Mortality')
39/20:
# Create a bar graph to show mean age maternal death by region
fig, ax = plt.subplots(figsize=(16, 4))
plt.grid(True)
fig.subplots_adjust(bottom=0.2)
sns.countplot(res_dataset['Above(0) or Below(1) Average'])

# Label axis 
pl.title('Mean Age Maternal Mortality in Each Providence of Mexico')
39/21:
# List Regions with a mean maternal mortality lower than population mean
len(res_dataset[res_dataset['Above(0) or Below(1) Average'] == 1])
39/22: %store -r edu_dict
39/23:
# Plot medical care by region
x, y = zip(*edu_dict)
plt.plot(x, y)
plt.show()

# Label axis 
pl.title('Incidence of Maternal Mortality in Each Providence of Mexico')
40/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import pylab as pl
import math
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import scipy.stats as stats
import statistics
40/2:
# Load datasets
%store -r  materna
%store -r res_dataset
40/3:
# Create an adolescent sub dataframe from materna
adolescent_matern_mortality = materna[materna['Age at Death'] <= 20 ]
adolescent_matern_mortality.head()
40/4:
# Create a variable for adolescent_ages_maternal_mortality
adolescent_ages = adolescent_matern_mortality['Age at Death']

# Create a variable for adolsecent_sample_size
adolsecent_sample_size = len(adolescent_ages)
adolsecent_sample_size
40/5:
# Create a figure with two plots
fig, (boxplot, histogram) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})

# Add boxplot for maternal death
sns.boxplot(adolescent_ages, ax=boxplot)

# Remove x-axis label from boxplot
boxplot.set(xlabel='')

# Add histogram and normal curve for maternal death
fit = stats.norm.pdf(adolescent_ages, np.mean(adolescent_ages), np.std(adolescent_ages))
pl.plot(adolescent_ages, fit, '-o')
pl.hist(adolescent_ages, density=True, alpha=0.5, bins=20)

# Label axis 
pl.xlabel('Adolescent Ages of Maternal Mortality')
pl.ylabel('Probability Density Function')
pl.title('Adolescent Age Distribution of Maternal Mortality in Mexico')

# Show plot and add print mean and std sample information
plt.show()
'The sample(n=' + str(adolsecent_sample_size) + ') population mean age of adolescent maternal mortality is ' + str(round(np.mean(adolescent_ages), 2)) + ' years old with a standard deviation of ' + str(round(np.std(adolescent_ages), 2)) + '.'
40/6:
#Create a boxplot
adolescent_matern_mortality.boxplot('Age at Death', by='Residence Name', figsize=(12, 8))
plt.xticks(rotation='vertical')
40/7:
# Create variables for sample statistical information
adolescent_ages_std = adolescent_ages.std()
mean_adolescent_ages = adolescent_ages.mean()

# Create an array of the sample mean that is equal to the boostrap array length
adolescent_ages_arr = np.full(10000, mean_adolescent_ages)

print('sample size: ', adolsecent_sample_size)
print('sample mean age of death: ', mean_adolescent_ages)
print('sample standard deviation: ', adolescent_ages_std)
40/8:
# Bootstrap replicate function for repeatability
def bootstrap_replicate_1d(data, func):
    """Create a bootstrap replicates."""
    
    boot_sample = np.random.choice(data, size=len(data))  # create bootstrap sample
    return func(boot_sample) # apply function to bootstrap

# Apply bootstrap replicate function 'n' and return an array
def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""

    boot_rep = np.empty(size) # initialize array of replicates: bs_replicates
    for i in range(size):  # generate 'n' number of replicates
        boot_rep[i] = bootstrap_replicate_1d(data, func)
    return boot_rep
40/9:
# Create 10000 bootstrap replicates of the mean and take the mean of the returned array
boot_tenthousand =  draw_bs_reps(adolescent_ages, np.mean, size=10000)
print('bootstrap mean adolescent age of death: ' + str(np.mean(boot_tenthousand)))
40/10:
# Compute p-value
p_val = np.sum(boot_tenthousand >= adolescent_ages_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_adolescent_ages-(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
conf_int_high = mean_adolescent_ages+(1.98*(adolescent_ages_std/math.sqrt(adolsecent_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
40/11:
#Create a dict variable for key:value pairs of state:age_array
grps = pd.unique(adolescent_matern_mortality['Residence Name'].values)
state_mean_adolescent_ages = {grp:adolescent_matern_mortality['Age at Death'][adolescent_matern_mortality['Residence Name'] == grp] for grp in grps}
40/12:
# Create an empty list to store mean age and sample size of maternal death per region
ado_region_mean = []
ado_region_n = []

# Create an iteration function
for i in adolescent_matern_mortality['Residence Code'].sort_values().unique():
    """Calculate Mean Age Adolescent Maternal Mortality per Region"""
    
    sub_df = adolescent_matern_mortality[adolescent_matern_mortality['Residence Code'] == i]
    age = sub_df['Age at Death']
    ado_n = len(age)  # sample length
    ado_mean = age.mean() # calculate mean
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        ado_region_mean.append(round(ado_mean, 2)) #  append mean to region list
        ado_region_n.append(round(ado_n, 2)) # append ado_n to region list
40/13:
# Test output
len(ado_region_mean), ado_region_mean[31], len(ado_region_n), ado_region_n[31]
40/14:
# Convert the list to a Series and add as new column
res_dataset['μ Age Adolescent Maternal Death'] = pd.Series(ado_region_mean, index=np.arange(1,33))
res_dataset['Region Ado (n)'] = pd.Series(ado_region_n, index=np.arange(1,33))
res_dataset.head()
40/15:
# Test output
res_dataset.tail()
40/16:
# Store as a global variable 
%store res_dataset
42/1:
# Import the relevant python libraries for the analysis
import math
import numpy as np
import pandas as pd
import pylab as pl
import random
import seaborn as sns
import scipy.stats as stats
import statistics
42/2:
# Load datasets
%store -r  materna
%store -r state_pop
%store -r res_dataset
42/3:
# Load datasets
%store -r  materna
%store -r res_dataset
42/4: materna.head(2)
42/5:
# Bootstrap replicate function for repeatability
def bootstrap_replicate_1d(data, func):
    """Create a bootstrap replicates."""
    
    boot_sample = np.random.choice(data, size=len(data))  # create bootstrap sample
    return func(boot_sample) # apply function to bootstrap

# Apply bootstrap replicate function 'n' and return an array
def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""

    boot_rep = np.empty(size) # initialize array of replicates: bs_replicates
    for i in range(size):  # generate 'n' number of replicates
        boot_rep[i] = bootstrap_replicate_1d(data, func)
    return boot_rep
42/6:
# Define a variable for the materna['Age at Death'] Series
age_mortality = materna['Age at Death']
42/7:
# Create variables for sample statistical information
materna_age_std = age_mortality.std()
materna_sample_size = len(age_mortality)
materna_age_var = np.var(age_mortality)
mean_age = materna['Age at Death'].mean()

# Create an array of the sample mean that is equal to the boostrap array length
materna_mean_arr = np.full(10000, mean_age)

print('sample size: ', materna_sample_size)
print('sample mean age of death: ', mean_age)
print('sample standard deviation: ', materna_age_std)
print('sample variation: ', materna_age_var)
42/8:
# Create 10000 bootstrap replicates of the mean and take the mean of the returned array
boot_tenthousand =  draw_bs_reps(age_mortality, np.mean, size=10000)
print('bootstrap mean age of death: ' + str(np.mean(boot_tenthousand)))
42/9:
# Compute p-value
p_val = np.sum(boot_tenthousand >= materna_mean_arr) / len(boot_tenthousand)
print('p-value: {0:0.4f}'.format(p_val))

# Calculate the standard margin of error for a 95% confidence interval
conf_int_low = mean_age-(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
conf_int_high = mean_age+(1.98*(materna_age_std/math.sqrt(materna_sample_size)))
print('95% Confidence Interval: [{0:0.4f}  {1:0.4f}]'.format(conf_int_low, conf_int_high))
42/10:
# Verify both age_by_state and state_pop both contain all 32 Mexican States/Regions 
len(list(age_by_state.keys())), len(state_pop)
42/11:
# Modify state_pop to contain the extact same string value for State
state_pop['State'] = age_by_state.keys()
state_pop.head()
43/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pylab as pl


# Statistics
import statistics
import scipy.stats as stats
import random
import math
import re
%matplotlib inline
43/2:
# Load and test Metro by Region dataset and set the index if applicable
metro_by_region = pd.read_excel('data/metro_area_by_state.xlsx')
metro_by_region.info()
43/3:
# Load and test Mexico GDP by Region dataset and set the index if applicable
mexico_gdp = pd.read_csv('data/mexico_region_gdp_per_capita.csv')
mexico_gdp.info()
43/4: metro_by_region.head()
43/5:
#Drop NaN values 
metro_by_region = metro_by_region.dropna()
metro_by_region.head()
43/6:
# Organize dataset by State in alphabetical order
metro_by_region = metro_by_region.sort_values(by=['State(s)'],ascending=True)
metro_by_region.head()
43/7:
# Reset index 
metro_by_region = metro_by_region.reset_index()
metro_by_region.head()
43/8:
# Drop index column
metro_by_region = metro_by_region.drop(columns='index')
metro_by_region.head()
43/9: mexico_gdp.head(31)
43/10: mexico_gdp.tail(31)
43/11:
# Create a Dataframe to store data types in mexico_gdp
mex_gdp_dtypes = pd.DataFrame(mexico_gdp.dtypes)

#Rename column name to DataType
mex_gdp_dtypes = mex_gdp_dtypes.rename(columns={0:'DataType'})

#Analyze Missing Values
mex_gdp_dtypes['MissingVal'] = mexico_gdp.isnull().sum()

#Identify number of unique values
mex_gdp_dtypes['NumUnique'] = mexico_gdp.nunique()

#Identify the count for each variable
mex_gdp_dtypes['Count']= mexico_gdp.count()
mex_gdp_dtypes
43/12: list(np.unique(mexico_gdp['Metropolitan Areas']))
43/13:
# Convert number objects in Year_2010 and Year_2015 to numbers

# Year_2010
mexico_gdp['Year_2010'] = pd.to_numeric(mexico_gdp['Year_2010'], errors='coerce')

# Year_2015
mexico_gdp['Year_2015'] = pd.to_numeric(mexico_gdp['Year_2015'], errors='coerce')

# Verify object datatypes were switched to numbers
mexico_gdp.dtypes
43/14:
# Run descriptive statistics of number datatypes
mexico_gdp.describe(include=['number'])
43/15:
# Reindex by Metropolitan Area
mexico_gdp = mexico_gdp.sort_values(by=['Metropolitan Areas'],ascending=True)

# Reset the Index
mexico_gdp = mexico_gdp.reset_index()
mexico_gdp.head()
43/16:
# Remove New Index Column
mexico_gdp = mexico_gdp.drop(['index'], axis=1)
mexico_gdp.head()
43/17:
# Change the column names to be more clear
mexico_gdp.columns = ['Metropolitan Areas', 'Metro GDP 2010', 'Metro GDP 2015']
mexico_gdp.head()
43/18:
# Create an empty list to store new Metropolitan Areas string
metro_areas = []

# Iterate through Metropolitan Areas to Remove the MEX## from the string and add to metro_areas
for metro in mexico_gdp['Metropolitan Areas']:
    """Remove MEX##: from Metropolitan Area Strings."""
    
    # Remove all numbers from each Metro Area string
    metro_no_numbers = re.sub("\d+", " ", metro)
    
    # Remove all 'MEX : ' from each Metro Area string
    metro_new_string = metro_no_numbers.replace('MEX : ', '')
    metro_new_string = metro_new_string.replace('MEX: ', '')
    
    # Create new Metro Area Column in mexico_gdp with metro_str_modified
    metro_areas.append(metro_new_string)
43/19:
# Convert the list to a Series and add as new column to res_dataset
mexico_gdp['Metro Areas'] = pd.Series(metro_areas)
mexico_gdp['Metro Areas'].head()
43/20: mexico_gdp.head()
43/21:
# Drop 'Metropolitan Areas' column as it is unnecessary
mexico_gdp = mexico_gdp.drop(columns='Metropolitan Areas')
mexico_gdp.head()
43/22:
# Open res_dataset as a global variable that can be uploaded to other Jupyter Notebooks
%store -r res_dataset
43/23:
print('In res_dataset[Region], there are '+ str(len(np.unique(res_dataset['Region']))) + ' States in Mexico.')
list(np.unique(res_dataset['Region']))
43/24:
# Assess the number of Mexico States in dataset
print('The Number of Mexican States in this Dataset is: ',len(metro_by_region['State(s)'].unique()))
list(metro_by_region['State(s)'].unique())
43/25: metro_by_region.head()
43/26:
# Replace the duplicate 5 Mexican State names with name matching the dates dataset 

#Coahuila de Zaragoza
metro_by_region = metro_by_region.replace('Coahuila de Zaragoza / Durango', 'Coahuila de Zaragoza')

#Guanajuato
metro_by_region = metro_by_region.replace('Guanajuato / Michoacán de Ocampo','Guanajuato')

#Jalisco
metro_by_region = metro_by_region.replace('Jalisco / Nayarit', 'Jalisco')

#Puebla
metro_by_region = metro_by_region.replace('Puebla / Tlaxcala', 'Puebla')

#Tamaulipas
metro_by_region = metro_by_region.replace('Tamaulipas / Veracruz de Ignacio de la Llave', 'Tamaulipas')
43/27:
# Assess the resulting number of Mexico States in dataset
print('The New Number of Mexican States in this Dataset is: ',len(metro_by_region['State(s)'].unique()))
list(metro_by_region['State(s)'].unique())
43/28: res_dataset.head()
43/29:
# In res_dataset, rename 'Region' column to 'State' in order to match metro_by_region
res_dataset.columns = ['State',
                       'μ Age Maternal Mortality',
                       'μ Age Variance',
                       'Above(0)/Not(1) Mexico μ Age Maternal Mortality',
                       'μ Age Adolescent Maternal Mortality', 
                       'Above(0)/Not(1) Mexico μ *Ado* Age Maternal Mortality',
                       'Proportion Ado(n):Total(n)',
                       'Above(0)/Not(1) Proportion *Ado*(n):Total(n)',
                       'μ Region Education Level',
                       'μ Presence(0)/Not(1) Medical Assistance ATD']
res_dataset.head()
43/30:
print('There are '+ str(len(np.unique(mexico_gdp['Metro Areas']))) + ' Metropolitan Areas in Mexico.')
metro_areas = mexico_gdp['Metro Areas'].sort_values()
list(metro_areas.unique())
43/31:
# Assess the number of Mexico Metro Areas in dataset
print('The Number of Mexican Metropolitan Areas in this Dataset is: ',len(metro_by_region['Name'].unique()))
metro_areas_subdf = metro_by_region['Name'].sort_values()
list(metro_areas_subdf.unique())
43/32:
# Replace the metropolitan region names with the matching name in the GDP dataset

#Acapulco de Juarez
metro_by_region = metro_by_region.replace('Acapulco', 'Acapulco de Juarez')

#Chilpancingo de los Bravo
metro_by_region = metro_by_region.replace('Chilpancingo', 'Chilpancingo de los Bravo')

#Colima
metro_by_region = metro_by_region.replace('Colima - Villa de Álvarez', 'Colima')

#Culiacan
metro_by_region = metro_by_region.replace('Culiacán', 'Culiacan')

#Guadalupe
metro_by_region = metro_by_region.replace('Zacatecas - Guadalupe', 'Guadalupe')

#Juarez
metro_by_region = metro_by_region.replace('Juárez' , 'Juarez')

#Leon
metro_by_region = metro_by_region.replace('León' , 'Leon')

#Mazatlan
metro_by_region = metro_by_region.replace('Mazatlán' , 'Mazatlan')

#Mexico City
metro_by_region = metro_by_region.replace('Valle de México\xa0[Greater Mexico City]', 'Mexico City')

#Minatitlan
metro_by_region = metro_by_region.replace('Minatitlán' , 'Minatitlan')

#Monclova
metro_by_region = metro_by_region.replace('Monclova - Frontera', 'Monclova')

#Oaxaca de Juarez
metro_by_region = metro_by_region.replace('Oaxaca' , 'Oaxaca de Juarez')

#Pachuca de Soto
metro_by_region = metro_by_region.replace('Pachuca' , 'Pachuca de Soto')

#Poza Rica de Hidalgo
metro_by_region = metro_by_region.replace('Poza Rica' , 'Poza Rica de Hidalgo')

#Puebla
metro_by_region = metro_by_region.replace('Puebla - Tlaxcala', 'Puebla')

#Queretaro
metro_by_region = metro_by_region.replace('Querétaro' , 'Queretaro')

#San Luis Potosi
metro_by_region = metro_by_region.replace('San Luis Potosí' , 'San Luis Potosi')

#Tehuacan
metro_by_region = metro_by_region.replace('Tehuacán' , 'Tehuacan')

#Tlaxcala
metro_by_region = metro_by_region.replace('Tlaxcala - Apizaco', 'Tlaxcala')

#Torren
metro_by_region = metro_by_region.replace('La Laguna\xa0(Comarca Lagunera, Torreón - Gómez Palacio)', 'Torreon')
                                          
#Tulancingo de Bravo
metro_by_region = metro_by_region.replace('Tulancingo' , 'Tulancingo de Bravo')
                                          
#Tuxtla Gutierrez
metro_by_region = metro_by_region.replace('Tuxtla Gutiérrez' , 'Tuxtla Gutierrez')
43/33:
# Verify results
metro_areas_subdf_2 = metro_by_region['Name'].sort_values()
list(metro_areas_subdf_2.unique())
43/34: metro_by_region.head()
43/35:
# Combine the Population values for each Mexican State
state_pop = metro_by_region.groupby(['State(s)']).sum()
state_pop
43/36:
# Reset index so States is a column
state_pop = state_pop.reset_index()
state_pop.head()
43/37: len(state_pop)
43/38:
# Store state_pop dataset as a global variable that can be uploaded to other Jupyter Notebooks
%store state_pop
43/39:
condition = metro_by_region['Name'].isin(mexico_gdp['Metro Areas']) == True
#condition
43/40:
metro_by_region['Drop if False'] = condition
metro_by_region.head()
43/41:
# Remove all rows where metro_by_region['Drop if False'] == False
metro_by_region = metro_by_region[metro_by_region['Drop if False'] == True]
metro_by_region.head()
43/42:
# Remove metro_by_region['Drop if False'] column
metro_by_region = metro_by_region.drop(columns=['Drop if False', 'Status'])
metro_by_region.head()
43/43:
print('The Number of Mexican Matropolicatn Areas in this Dataset is: ', len(metro_by_region['Name'].unique()))
metro_areas_subdf = metro_by_region['Name'].sort_values()
#list(metro_areas_subdf.unique())
43/44:
# Sort metro_by_region Metro Areas in alphabetical order
metro_by_region = metro_by_region.sort_values(by='Name', ascending=True)
metro_by_region.head()
43/45:
# Create empty list variables for state population by year for metro_by_region

#2010
state_population_2010 = {}

#2015
state_population_2015 = {}
43/46:
# Test code to create function
row = state_pop[state_pop['State(s)'] == 'Campeche']
int(row['Population 2010'].values)
43/47:
# Add a total state population value for each metro area within a state
for state in state_pop['State(s)']:
    
    # Iterate over state_pop dataset
    for m_state in metro_by_region['State(s)']:
        
        if state == m_state: 
            
            #store pop values of state_pop in metro_by_region
            row = metro_by_region[metro_by_region['State(s)'] == state]
            
            #2010
            p2010 = int(row['Population 2010'].sum())
            state_population_2010[state] = p2010
            
            #2015
            p2015 = int(row['Population 2015'].sum())
            state_population_2015[state] = p2015
43/48:
# Test output
print(len(state_population_2010))
state_population_2010
43/49:
metro_state_pop2010 = []
metro_state_pop2015 = []

for state in metro_by_region['State(s)']:
    
    for state10 in state_population_2010:
        
        if state == state10:
            
            #2010
            pop2010 = state_population_2010[state]
            metro_state_pop2010.append(pop2010)
            
            #2015
            pop2015 = state_population_2015[state]
            metro_state_pop2015.append(pop2015)
43/50:
# Test output
print(len(metro_state_pop2010))
#metro_state_pop2010
43/51:
# Convert the state pop lists to a Series and add as new column to metro_by_region

#2010
metro_by_region['State Population 2010'] = metro_state_pop2010

#2015
metro_by_region['State Population 2015'] = metro_state_pop2015
metro_by_region.head()
43/52:
# Remove metro_by_region['Population 2010', 'Population 2015'] columns as they are no longer needed
metro_by_region = metro_by_region.drop(columns=['Population 2010', 'Population 2015'])
metro_by_region.head()
43/53:
# Rename 'Name' column to 'Metro Areas' in order to match mexico_gdp
metro_by_region.columns = ['Metro Areas', 'State', 'State Population 2010', 'State Population 2015']
metro_by_region.head()
43/54:
condition_2 = mexico_gdp['Metro Areas'].isin(metro_by_region['Metro Areas']) == True
#condition_2
43/55:
mexico_gdp['Drop if False'] = condition_2
mexico_gdp.head()
43/56:
# Remove all rows where metro_by_region['Drop if False'] == False
mexico_gdp = mexico_gdp[mexico_gdp['Drop if False'] == True]
mexico_gdp.head()
43/57:
# Remove metro_by_region['Drop if False'] column
mexico_gdp = mexico_gdp.drop(columns=['Drop if False'])
mexico_gdp.head()
43/58:
# Rename 'Year_2010' and 'Year_2015' columns to 'GDP 2010' and 'GDP 2015' for clarity
mexico_gdp.columns = ['Metro GDP 2010', 'Metro GDP 2015', 'Metro Areas']
mexico_gdp.head()
43/59:
print('The Number of Mexican Metropolitan Areas in this Dataset is: ',len(mexico_gdp['Metro Areas'].unique()))
metro_areas_subdf = mexico_gdp['Metro Areas'].sort_values()
#list(metro_areas_subdf.unique())
43/60: mexico_gdp.head()
43/61: metro_by_region.head()
43/62:
metro_gdp_merge = pd.merge(metro_by_region, mexico_gdp, on='Metro Areas')
metro_gdp_merge
43/63: len(res_dataset)
43/64: res_dataset.head()
43/65:
# Reindex dates by State
metro_gdp_merge = metro_gdp_merge.sort_values(by=['State'],ascending=True)

# Reset the Index
metro_gdp_merge = metro_gdp_merge.reset_index()
metro_gdp_merge.head()
43/66:
# Remove New Index Column
metro_gdp_merge = metro_gdp_merge.drop(['index'], axis=1)
metro_gdp_merge.head()
43/67:
condition_3 = metro_gdp_merge['State'].isin(res_dataset['State']) == True
#condition_3
43/68:
metro_gdp_merge['Drop if False'] = condition_3
metro_gdp_merge.head()
43/69:
# In res_dataset, rename 'Region' column to 'State' in order to match metro_by_region
res_dataset.columns = ['State',
                       'μ Age Maternal Mortality',
                       'Region (n)',
                       'μ Age Variance',
                       'Above(0) or Below(1) Country Average',
                       'μ Region Education Level',
                       'μ Presence(0)/Not(1) Medical Assistance ATD',
                       'μ Age Adolescent Maternal Death',
                       'Region Ado (n)']
res_dataset.head()
43/70:
print('There are '+ str(len(np.unique(mexico_gdp['Metro Areas']))) + ' Metropolitan Areas in Mexico.')
metro_areas = mexico_gdp['Metro Areas'].sort_values()
list(metro_areas.unique())
43/71:
# Assess the number of Mexico Metro Areas in dataset
print('The Number of Mexican Metropolitan Areas in this Dataset is: ',len(metro_by_region['Name'].unique()))
metro_areas_subdf = metro_by_region['Name'].sort_values()
list(metro_areas_subdf.unique())
43/72:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pylab as pl


# Statistics
import statistics
import scipy.stats as stats
import random
import math
import re
%matplotlib inline
43/73:
# Load and test Metro by Region dataset and set the index if applicable
metro_by_region = pd.read_excel('data/metro_area_by_state.xlsx')
metro_by_region.info()
43/74:
# Load and test Mexico GDP by Region dataset and set the index if applicable
mexico_gdp = pd.read_csv('data/mexico_region_gdp_per_capita.csv')
mexico_gdp.info()
43/75: metro_by_region.head()
43/76:
#Drop NaN values 
metro_by_region = metro_by_region.dropna()
metro_by_region.head()
43/77:
# Organize dataset by State in alphabetical order
metro_by_region = metro_by_region.sort_values(by=['State(s)'],ascending=True)
metro_by_region.head()
43/78:
# Reset index 
metro_by_region = metro_by_region.reset_index()
metro_by_region.head()
43/79:
# Drop index column
metro_by_region = metro_by_region.drop(columns='index')
metro_by_region.head()
43/80: mexico_gdp.head(31)
43/81: mexico_gdp.tail(31)
43/82:
# Create a Dataframe to store data types in mexico_gdp
mex_gdp_dtypes = pd.DataFrame(mexico_gdp.dtypes)

#Rename column name to DataType
mex_gdp_dtypes = mex_gdp_dtypes.rename(columns={0:'DataType'})

#Analyze Missing Values
mex_gdp_dtypes['MissingVal'] = mexico_gdp.isnull().sum()

#Identify number of unique values
mex_gdp_dtypes['NumUnique'] = mexico_gdp.nunique()

#Identify the count for each variable
mex_gdp_dtypes['Count']= mexico_gdp.count()
mex_gdp_dtypes
43/83: list(np.unique(mexico_gdp['Metropolitan Areas']))
43/84:
# Convert number objects in Year_2010 and Year_2015 to numbers

# Year_2010
mexico_gdp['Year_2010'] = pd.to_numeric(mexico_gdp['Year_2010'], errors='coerce')

# Year_2015
mexico_gdp['Year_2015'] = pd.to_numeric(mexico_gdp['Year_2015'], errors='coerce')

# Verify object datatypes were switched to numbers
mexico_gdp.dtypes
43/85:
# Run descriptive statistics of number datatypes
mexico_gdp.describe(include=['number'])
43/86:
# Reindex by Metropolitan Area
mexico_gdp = mexico_gdp.sort_values(by=['Metropolitan Areas'],ascending=True)

# Reset the Index
mexico_gdp = mexico_gdp.reset_index()
mexico_gdp.head()
43/87:
# Remove New Index Column
mexico_gdp = mexico_gdp.drop(['index'], axis=1)
mexico_gdp.head()
43/88:
# Change the column names to be more clear
mexico_gdp.columns = ['Metropolitan Areas', 'Metro GDP 2010', 'Metro GDP 2015']
mexico_gdp.head()
43/89:
# Create an empty list to store new Metropolitan Areas string
metro_areas = []

# Iterate through Metropolitan Areas to Remove the MEX## from the string and add to metro_areas
for metro in mexico_gdp['Metropolitan Areas']:
    """Remove MEX##: from Metropolitan Area Strings."""
    
    # Remove all numbers from each Metro Area string
    metro_no_numbers = re.sub("\d+", " ", metro)
    
    # Remove all 'MEX : ' from each Metro Area string
    metro_new_string = metro_no_numbers.replace('MEX : ', '')
    metro_new_string = metro_new_string.replace('MEX: ', '')
    
    # Create new Metro Area Column in mexico_gdp with metro_str_modified
    metro_areas.append(metro_new_string)
43/90:
# Convert the list to a Series and add as new column to res_dataset
mexico_gdp['Metro Areas'] = pd.Series(metro_areas)
mexico_gdp['Metro Areas'].head()
43/91: mexico_gdp.head()
43/92:
# Drop 'Metropolitan Areas' column as it is unnecessary
mexico_gdp = mexico_gdp.drop(columns='Metropolitan Areas')
mexico_gdp.head()
43/93:
# Open res_dataset as a global variable that can be uploaded to other Jupyter Notebooks
%store -r res_dataset
43/94:
print('In res_dataset[Region], there are '+ str(len(np.unique(res_dataset['Region']))) + ' States in Mexico.')
list(np.unique(res_dataset['Region']))
44/1:
# Store as a global variable 
%store res_dataset
44/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import statistics
44/3:
# Load the dataset
mortalitad_materna = pd.read_csv('data/mortalidad_materna.csv')
#materna.info()
44/4:
# 1. Combine patient birthdate information into one column
birth = DataFrame(mortalitad_materna, columns=['Año de nacimiento', 'Mes de nacimiento', 'Día de nacimiento'])
birth = mortalitad_materna['Año de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str) + '-' + mortalitad_materna['Mes de nacimiento'].map(str)
print(birth.sort_values(ascending=True).head(2))
len(birth)
44/5:
# Combine patient date of death information into one column
death = DataFrame(mortalitad_materna, columns=['Año de la defunción', 'Mes de la defunción', 'Día de la defunción'])
death = mortalitad_materna['Año de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str) + '-' + mortalitad_materna['Mes de la defunción'].map(str)
print(death.sort_values(ascending=True).head(2))
len(death)
44/6:
# Create variable to store:

#residence information 
residence_code = mortalitad_materna['Entidad de residencia']
residence_name = mortalitad_materna['Descripción de entidad de residencia']

#local community info
local_size = mortalitad_materna['Descripción del tamaño de localidad']

#educational level
edu_reached_code = mortalitad_materna['Escolaridad'] 
edu_reached = mortalitad_materna['Descripción de la escolaridad']

#age fulfilled by patient
last_age = mortalitad_materna['Edad cumplida']

#mortality reason
mortality_reason = mortalitad_materna['Razón de mortalidad materna']

#medical assistance
medical_received = mortalitad_materna['Descripción de la asistencia médica']
44/7:
# Create a sub-dataframe to hold all date- information 
materna = pd.concat([birth, 
                   death, 
                   residence_code,
                   residence_name,
                   local_size,
                   edu_reached_code,
                   edu_reached,
                   last_age,
                   mortality_reason,
                   medical_received], axis=1)
materna.columns = ['Date of Birth', 
                 'Date of Mortality', 
                 'Residence Code',
                 'Residence Name',
                 'Local Community Size',
                 'Education Code',
                 'Education Completed',
                 'Age at Death',
                 'Reason for Mortality',
                 'Medical Assistance Received']
    
materna.head(2)
44/8:
# Order dataframe to list in ascending order of approx. age at death
materna = materna.sort_values(by=['Age at Death'],ascending=True)
materna.head()
44/9:
# Reset Index 
materna = materna.reset_index(drop=True)
materna.head()
materna.tail()
44/10:
# Remove rows with NaN / '0-0-0' values in Date of Birth
materna = materna[materna['Date of Birth'] != '0-0-0']
materna.tail()
44/11: materna.head()
44/12:
# Create a variable for the description of Reason for Mortality Description
mortality_description = mortalitad_materna['Descripción de la razón de mortalidad materna']

# Create a sub-dataframe to show interaction of Reason for Mortality Code and Description
mortality = pd.concat([mortality_reason, mortality_description], axis=1)
mortality.columns = ['Reason Mortality Code', 'Reason Mortality Description']
mortality.head()
44/13: mortality.tail()
44/14:
print('0 Description:')
print('Spanish: Muertes Maternas excluidas para la razón de Mortalidad Materna')
print('English: Maternal deaths excluded for the reason of Maternal Mortality')
44/15:
print('1 Description:')
print('Spanish: Muertes Maternas para la razón de Mortalidad Materna')
print('English: Maternal deaths for the reason of Maternal Mortality')
44/16:
# Remove rows with 0 values in Reason for Mortality
materna = materna[materna['Reason for Mortality'] != 0 ]
materna.tail()
44/17:
# Create a sub-dataframe to show interaction of Education Code and Education Completed
education = materna[['Education Code', 'Education Completed']].sort_values(by='Education Code')
education = education.drop_duplicates()
print(len(education))
education
44/18:
# Overwriting column with replaced value of Education

# SE IGNORA / NINGUNA / NO ESPECIFICADO
materna["Education Completed"]= materna["Education Completed"].replace(['SE IGNORA', 'NINGUNA', 'NO ESPECIFICADO'], 0)

# PREESCOLAR
materna["Education Completed"]= materna["Education Completed"].replace('PREESCOLAR', 1)

# PRIMARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA INCOMPLETA', 2)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('PRIMARIA COMPLETA', 3)

# SECUNDARIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA INCOMPLETA', 4)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('SECUNDARIA COMPLETA', 5)

# BACHILLERATO O PREPARATORIA
#INCOMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA INCOMPLETA', 6)
#COMPLETA
materna["Education Completed"]= materna["Education Completed"].replace('BACHILLERATO O PREPARATORIA COMPLETA', 7)

# PROFESIONAL
materna["Education Completed"]= materna["Education Completed"].replace('PROFESIONAL', 8)

#POSGRADO
materna["Education Completed"]= materna["Education Completed"].replace('POSGRADO', 9)
44/19:
# Test output
list(materna['Education Completed'].sort_values().unique())
44/20:
# Create a list item to hold comparison response
binary_medassist = []

# Create an iteration function to compare region mean to popupation mean
for medassist in materna['Medical Assistance Received']:
    
    #test for assistance
    if medassist == 'CON ATENCION MEDICA':
        binary_medassist.append(0)
    else:
        binary_medassist.append(1)
44/21:
# Test output
#binary_medassist
44/22:
# Convert the list to a Series and add as new column
materna['Received(0)/Not(1) Medical Assistance'] = pd.Series(binary_medassist)
materna.head()
44/23:
# Drop 'Metropolitan Areas' column as it is unnecessary
materna = materna.drop(columns=['Date of Birth', 'Date of Mortality', 'Medical Assistance Received', 'Education Code', 'Reason for Mortality'])
materna.head()
44/24:
# Analyze shape of cleaned data
materna.describe()
44/25:
print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
44/26:
# Remove unnecessary rows from region_ages sub-dataset
materna = materna[materna['Residence Name'] != 'Estados Unidos de Norteamérica' ]
materna = materna[materna['Residence Name'] != 'Otros paises latinoamericanos' ]
materna = materna[materna['Residence Name'] != 'No especificado' ]
materna = materna[materna['Residence Name'] != 'Otros paises' ]

print('There are '+ str(len(np.unique(materna['Residence Name']))) + ' Provinces in Mexico.')
list(np.unique(materna['Residence Name']))
44/27:
# Store as a global variable that can be uploaded to other Jupyter Notebooks
%store materna
44/28:
# Test code to create function
aqua = materna[materna['Residence Name'] == 'Aguascalientes']
aqua = aqua['Age at Death']
aqua = np.array(aqua)

mex = materna[materna['Residence Name'] == 'México']
mex = mex['Age at Death']
mex = np.array(mex)

print('Aguascalientes Sample Length: '+ str(aqua))
print('México Sample Length: ' + str(mex))
44/29:
# Create a function to group all ages associated with materna death within a Province and store the ages in an array
def age_array(str):
    
    """Create arrays for all Ages of Maternal Death within a Region"""
    
    ages = materna[materna['Residence Name'] == str] # select the region 'str' from the 'Region' column
    ages = ages['Age at Death'] # select the ages within the region
    ages = np.array(ages) # store the ages in an array
    return ages # return the unique array
44/30:
# Test output
print('Aguascalientes', age_array('Aguascalientes'))
print('México', age_array('México'))
44/31:
# Create a variable for 'Region' names using np.unique()
list_regions = np.unique(materna['Residence Name'])

# Create an empty dictionary to hold the {Region : region_age_array} key pairs
age_by_state = {}
44/32:
# Use the age_array function with iteration over residence to create the {Region : region_age_array} key pairs
for region in list_regions:
    age_by_state[region] = age_array(region) # add arrays as values in dictionary with region-key
44/33:
# Test output
print('Aguascalientes', age_by_state['Aguascalientes'])
44/34:
# Var for residence name 
residence_uniq = np.unique(materna['Residence Name'])

# Var for residence code
residence_code = np.unique(materna['Residence Code'])

# Create the sub-dateframe for region and region code
res_dataset = pd.DataFrame(residence_uniq, index=residence_code)
res_dataset = res_dataset.rename(columns={0:'Region'})
res_dataset.head()
44/35:
# Test Code
mean_death_list_trial = []

aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_mean = aguas['Age at Death'].mean()
aguas_mean = '{0:0.2f}'.format(aguas_mean)
print(aguas_mean)
mean_death_list_trial.append(aguas_mean)
print(mean_death_list_trial)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_mean = baja['Age at Death'].mean()
baja_mean = '{0:0.2f}'.format(baja_mean)
print(baja_mean)
mean_death_list_trial.append(baja_mean)
print(mean_death_list_trial)
44/36:
# Create an empty list to store region sample size and mean age of maternal death
region_mean = []
region_n = []

# Calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Length of Age Array and Mean Age per Region"""

    sub_df = materna[materna['Residence Code'] == (i - 1)] # select one region
    n = len(sub_df['Age at Death']) # calculate sample length
    mean = sub_df['Age at Death'].mean() # calculate mean of region
    region_n.append(round(n, 2)) # append n to list
    region_mean.append(round(mean, 2)) # append mean to list
44/37:
# Test output
print(region_mean[1], region_n[1], type(region_mean[1]))
print(region_mean[2], region_n[2], type(region_mean[2]))
44/38:
# Convert the list to a Series and add as new column
res_dataset['μ Age Maternal Mortality'] = pd.Series(region_mean)
res_dataset['Region (n)'] = pd.Series(region_n)
res_dataset.tail()
44/39:
# Calculate the mean Age of Death for region 'Zacatecas'
zaca = materna[materna['Residence Code'] == 32 ]
zaca = zaca['Age at Death']

# Calculate sample size
zaca_n = len(zaca)

# Calculate mean
zaca_mean = zaca.mean()
zaca_mean = round(mean, 2)
print(zaca_mean)

# Change contents of res_dataset NaN to calculated mean
res_dataset['μ Age Maternal Mortality'] = res_dataset['μ Age Maternal Mortality'].replace(np.nan, zaca_mean)
res_dataset['Region (n)'] = res_dataset['Region (n)'].replace(np.nan, zaca_n)
res_dataset.tail()
44/40:
# Test code
aguas = materna[materna['Residence Code'] == 1 ]
aguas = aguas[['Residence Code', 'Age at Death']]
aguas_var = statistics.pvariance(aguas['Age at Death'])
print('Aguas', aguas_var)

baja = materna[materna['Residence Code'] == 2 ]
baja = baja[['Residence Code', 'Age at Death']]
baja_var = statistics.pvariance(baja['Age at Death'])
print('Baja Cal', baja_var)
44/41:
# Create an empty list to store age of maternal death variance per region
region_var = []

for i in materna['Residence Code'].sort_values().unique():
    """Calculate Age Standard Deviation and Age Variance per Region"""
    
    sub_df = materna[materna['Residence Code'] == i]
    age = list(sub_df['Age at Death'])
    var = statistics.pvariance(age) # calculate age variance of region pop
    
    for region in sub_df['Residence Name'].unique(): # prevent repeat entries in lists
        region_var.append(round(var, 2)) # append var to region list
44/42:
# Test output - Make sure it matches Test Results
print('Test Results - Aguas', round(aguas_var,2))
print('Function Results - Aguas', region_var[0])
44/43:
# Convert the list to a Series and add as new column
res_dataset['μ Age Variance'] = pd.Series(region_var, index=np.arange(1,33))
res_dataset.head()
44/44:
res_mean_age = res_dataset['μ Age Maternal Mortality'].mean()
print('The population mean is: ', round(res_mean_age, 2))
44/45:
# Create a dictionary item to hold comparison response
binary_mean = []

# Compare region mean to population mean
for mean in res_dataset['μ Age Maternal Mortality']:
    if mean >= res_mean_age:
        binary_mean.append(0)
    else:
        binary_mean.append(1)
44/46:
# Test output
#binary_mean
44/47:
# Convert the list to a Series and add as new column
res_dataset['Above(0) or Below(1) Average'] = pd.Series(binary_mean, index=np.arange(1,33))
res_dataset.head()
44/48:
# Create an empty list to store region sample size and mean age of maternal death
region_education = []
edu_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""

    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    education = sub_df['Education Completed'].mean()
    mean_edu = round(education, 2)
    region_education.append(mean_edu)
    edu_dict[region] = mean_edu
44/49:
# Test output
print(len(region_education))
#region_education
print(edu_dict)

# Store as a global variable
%store edu_dict
44/50:
# Convert the list to a Series and add as new column
res_dataset['μ Region Education Level'] = pd.Series(region_education, index=np.arange(1,33))
res_dataset.tail()
44/51:
# Create an empty list to store region sample size and mean age of maternal death
region_medical = []
medical_dict = {}

# Create an iteration function to calculate the mean age of maternal death per region
for i in materna['Residence Code'].sort_values().unique():
    """Calculate Mean Education per Region"""
    
    sub_df = materna[materna['Residence Code'] == i ]
    region = str(sub_df['Residence Name'].unique())
    med_assist = round(sub_df['Received(0)/Not(1) Medical Assistance'].mean(), 2)
    region_medical.append(med_assist) 
    medical_dict[region] = med_assist
44/52:
# Test output
print(len(region_medical))
#region_medical

# Store as a global variable
%store medical_dict
44/53:
# Convert the list to a Series and add as new column
res_dataset['μ Presence(0)/Not(1) of Medical Assistance ATD'] = pd.Series(region_medical, index=np.arange(1,33))
res_dataset.head()
44/54:
# Store as a global variable 
%store res_dataset
43/95:
# Open res_dataset as a global variable that can be uploaded to other Jupyter Notebooks
%store -r res_dataset
43/96:
print('In res_dataset[Region], there are '+ str(len(np.unique(res_dataset['Region']))) + ' States in Mexico.')
list(np.unique(res_dataset['Region']))
43/97:
# Assess the number of Mexico States in dataset
print('The Number of Mexican States in this Dataset is: ',len(metro_by_region['State(s)'].unique()))
list(metro_by_region['State(s)'].unique())
43/98: metro_by_region.head()
43/99:
# Replace the duplicate 5 Mexican State names with name matching the dates dataset 

#Coahuila de Zaragoza
metro_by_region = metro_by_region.replace('Coahuila de Zaragoza / Durango', 'Coahuila de Zaragoza')

#Guanajuato
metro_by_region = metro_by_region.replace('Guanajuato / Michoacán de Ocampo','Guanajuato')

#Jalisco
metro_by_region = metro_by_region.replace('Jalisco / Nayarit', 'Jalisco')

#Puebla
metro_by_region = metro_by_region.replace('Puebla / Tlaxcala', 'Puebla')

#Tamaulipas
metro_by_region = metro_by_region.replace('Tamaulipas / Veracruz de Ignacio de la Llave', 'Tamaulipas')
43/100:
# Assess the resulting number of Mexico States in dataset
print('The New Number of Mexican States in this Dataset is: ',len(metro_by_region['State(s)'].unique()))
list(metro_by_region['State(s)'].unique())
43/101: res_dataset.head()
43/102:
# In res_dataset, rename 'Region' column to 'State' in order to match metro_by_region
res_dataset.columns = ['State',
                       'μ Age Maternal Mortality',
                       'Region (n)',
                       'μ Age Variance',
                       'Above(0) or Below(1) Country Average',
                       'μ Region Education Level',
                       'μ Presence(0)/Not(1) Medical Assistance ATD',
                       'μ Age Adolescent Maternal Death',
                       'Region Ado (n)']
res_dataset.head()
43/103:
# In res_dataset, rename 'Region' column to 'State' in order to match metro_by_region
res_dataset.columns = ['State',
                       'μ Age Maternal Mortality',
                       'Region (n)',
                       'μ Age Variance',
                       'Above(0) or Below(1) Country Average',
                       'μ Region Education Level',
                       'μ Presence(0)/Not(1) Medical Assistance ATD']
res_dataset.head()
43/104:
print('There are '+ str(len(np.unique(mexico_gdp['Metro Areas']))) + ' Metropolitan Areas in Mexico.')
metro_areas = mexico_gdp['Metro Areas'].sort_values()
list(metro_areas.unique())
43/105:
# Assess the number of Mexico Metro Areas in dataset
print('The Number of Mexican Metropolitan Areas in this Dataset is: ',len(metro_by_region['Name'].unique()))
metro_areas_subdf = metro_by_region['Name'].sort_values()
list(metro_areas_subdf.unique())
43/106:
# Replace the metropolitan region names with the matching name in the GDP dataset

#Acapulco de Juarez
metro_by_region = metro_by_region.replace('Acapulco', 'Acapulco de Juarez')

#Chilpancingo de los Bravo
metro_by_region = metro_by_region.replace('Chilpancingo', 'Chilpancingo de los Bravo')

#Colima
metro_by_region = metro_by_region.replace('Colima - Villa de Álvarez', 'Colima')

#Culiacan
metro_by_region = metro_by_region.replace('Culiacán', 'Culiacan')

#Guadalupe
metro_by_region = metro_by_region.replace('Zacatecas - Guadalupe', 'Guadalupe')

#Juarez
metro_by_region = metro_by_region.replace('Juárez' , 'Juarez')

#Leon
metro_by_region = metro_by_region.replace('León' , 'Leon')

#Mazatlan
metro_by_region = metro_by_region.replace('Mazatlán' , 'Mazatlan')

#Mexico City
metro_by_region = metro_by_region.replace('Valle de México\xa0[Greater Mexico City]', 'Mexico City')

#Minatitlan
metro_by_region = metro_by_region.replace('Minatitlán' , 'Minatitlan')

#Monclova
metro_by_region = metro_by_region.replace('Monclova - Frontera', 'Monclova')

#Oaxaca de Juarez
metro_by_region = metro_by_region.replace('Oaxaca' , 'Oaxaca de Juarez')

#Pachuca de Soto
metro_by_region = metro_by_region.replace('Pachuca' , 'Pachuca de Soto')

#Poza Rica de Hidalgo
metro_by_region = metro_by_region.replace('Poza Rica' , 'Poza Rica de Hidalgo')

#Puebla
metro_by_region = metro_by_region.replace('Puebla - Tlaxcala', 'Puebla')

#Queretaro
metro_by_region = metro_by_region.replace('Querétaro' , 'Queretaro')

#San Luis Potosi
metro_by_region = metro_by_region.replace('San Luis Potosí' , 'San Luis Potosi')

#Tehuacan
metro_by_region = metro_by_region.replace('Tehuacán' , 'Tehuacan')

#Tlaxcala
metro_by_region = metro_by_region.replace('Tlaxcala - Apizaco', 'Tlaxcala')

#Torren
metro_by_region = metro_by_region.replace('La Laguna\xa0(Comarca Lagunera, Torreón - Gómez Palacio)', 'Torreon')
                                          
#Tulancingo de Bravo
metro_by_region = metro_by_region.replace('Tulancingo' , 'Tulancingo de Bravo')
                                          
#Tuxtla Gutierrez
metro_by_region = metro_by_region.replace('Tuxtla Gutiérrez' , 'Tuxtla Gutierrez')
43/107:
# Verify results
metro_areas_subdf_2 = metro_by_region['Name'].sort_values()
list(metro_areas_subdf_2.unique())
43/108: metro_by_region.head()
43/109:
# Combine the Population values for each Mexican State
state_pop = metro_by_region.groupby(['State(s)']).sum()
state_pop
43/110:
# Reset index so States is a column
state_pop = state_pop.reset_index()
state_pop.head()
43/111: len(state_pop)
43/112:
# Store state_pop dataset as a global variable that can be uploaded to other Jupyter Notebooks
%store state_pop
43/113:
condition = metro_by_region['Name'].isin(mexico_gdp['Metro Areas']) == True
#condition
42/12:
# Import the relevant python libraries for the analysis
import math
import numpy as np
import pandas as pd
import pylab as pl
import random
import seaborn as sns
import scipy.stats as stats
import statistics
42/13:
# Load datasets
%store -r  materna
%store -r res_dataset
44/55:
# Store as a global variable 
%store age_by_state
42/14:
# Load datasets
%store -r  materna
%store -r res_dataset 
%store -r age_by_state
42/15:
# Verify both age_by_state and state_pop both contain all 32 Mexican States/Regions 
len(list(age_by_state.keys())), len(state_pop)
42/16:
# Load datasets
%store -r  materna
%store -r res_dataset 
%store -r age_by_state
%store -r state_pop
42/17:
# Verify both age_by_state and state_pop both contain all 32 Mexican States/Regions 
len(list(age_by_state.keys())), len(state_pop)
42/18:
# Modify state_pop to contain the extact same string value for State
state_pop['State'] = age_by_state.keys()
state_pop.head()
42/19:
### OVER ESTIMATION - If Women are only 30% of Population ###
for state in age_by_state:
    age_arr = age_by_state[state]  # select age arr
    age_length = len(age_arr) # calculate State age sample size
    state_row = state_pop[state_pop['State'] == state] # select State row in state_pop
    
    # Calculate 10% state_pop State populations in 2010 and 2015
    ten_percent_2010 = round(float(state_row['Population 2010'])**0.1, 2)
    ten_percent_2015 = round(float(state_row['Population 2015'])**0.1, 2)
    
    # Calculate 10% of 30% of state_pop State populations in 2010 and 2015
    state_10_30 = round(float(state_row['Population 2010'])**0.3, 2)
    ten_percent_10_30 = round(state_10_30**0.1,2)
    state_15_30 = round(float(state_row['Population 2015'])**0.3, 2)
    ten_percent_15_30 = round(state_10_30**0.1,2)
        
    # Set condition: Compare age_length to 10% and 30% State populations in 2010 and 2015
    if age_length > ten_percent_2010 and age_length > ten_percent_2015:
        print('Accept Independence: ', state)
    else: 
        print('REJECT: ', state)
    
    if age_length > ten_percent_10_30 and ten_percent_10_30:
         print('Accept Independence - Over-Estimation: ', state)
    else: 
        print('REJECT - Over-Estimation: ', state)
42/20:
# Create a variable to hold list of Regions with normally-distributed sample sizes
norm_distr_regions = []

# Create a variable to hold list of Regions without normally-distributed sample sizes
not_norm_distr_regions = []
42/21:
# Determine if each Province has a normally distributed sample population of ages
for region in age_by_state:
    """Determine if Region Age Distribution is Normal"""
    
    region_name = str(region)
    arr = age_by_state[region_name]
    
    if len(arr) > 8: # skewtest (k2): not valid with less than 8 samples 
        k2, p = stats.normaltest(arr)
        alpha = 0.05 # 95% confidence
        print("p = {:g}".format(p))
        print("n = " + str(len(arr)))
        
        if p < alpha: # if norm
            print(str(region)+ " IS normally distributed.")
            norm_distr_regions.append(region_name) # add region to norm list  
        else:
            print(str(region)+ " *IS NOT* normally distributed.")
            not_norm_distr_regions.append(region_name) # add region to norm list     
    else: 
        print(str(region)+ " *sample size is too small*")
        not_norm_distr_regions.append(region_name) # add region to non-norm list of regions
42/22: print('Not Normally Distributed: ', list(np.unique(not_norm_distr_regions)))
43/114:
condition_3 = metro_gdp_merge['State'].isin(res_dataset['State']) == True
#condition_3
43/115:
metro_gdp_merge['Drop if False'] = condition_3
metro_gdp_merge.head()
47/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
47/2:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
47/3:
# Install requirements.txt
!pip install -r requirements.txt
47/4:
# %prun -> displays report of how time was run for that function- spot inefficient pieces of code
# export python notebook to just python files: bit.ly/py-html-config
# http://www.waffle.io
47/5:
from IPython.display import Image
Image(filename='data/RecommendationSystems.jpg')
47/6:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
47/7:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
47/8:
# List column names
list(beats.columns)
47/9: beats.info()
47/10: beats.head(1)
47/11:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
47/12:
# Test output
beats.info()
47/13:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
47/14:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
47/15:
# Import necessary modules
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
47/16:
# Limit to categorical data using df.select_dtypes()
categorical_genres = beats.select_dtypes(include=[object])
categorical_genres.tail(3)
47/17:
# Check data shape
categorical_genres.shape
47/18:
# Encode labels with value between 0 and n_classes-1
beats = pd.get_dummies(beats)
beats.head()
47/19:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
47/20:
# Install requirements.txt
!pip install -r requirements.txt
47/21:
# %prun -> displays report of how time was run for that function- spot inefficient pieces of code
# export python notebook to just python files: bit.ly/py-html-config
# http://www.waffle.io
47/22:
from IPython.display import Image
Image(filename='data/RecommendationSystems.jpg')
47/23:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
47/24:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
47/25:
# List column names
list(beats.columns)
47/26: beats.info()
47/27: beats.head(1)
47/28:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
47/29:
# Test output
beats.info()
47/30:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
47/31:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
47/32:
# Encode labels with value between 0 and n_classes-1
beats = pd.get_dummies(beats)
beats.head()
47/33:
# Encode labels with value between 0 and n_classes-1
beats = pd.get_dummies(beats)
beats.head()
47/34: beats.tail()
47/35:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
47/36: beats.columns
47/37:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genre']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
47/38:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
47/39:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
47/40:
# List column names
list(beats.columns)
47/41: beats.info()
47/42: beats.head(1)
47/43:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
47/44:
# Test output
beats.info()
47/45:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
47/46:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
47/47: ax = sns.countplot(x="Genre", data=beats)
47/48: ax = sns.countplot(x="Genre", data=beats[0:7000])
47/49:
ax = sns.countplot(x="Genre", data=beats[0:7000])
ax.set_xticklabels(rotation=30)
47/50:
ax = sns.factorplot(x="Genre", data=beats[0:7000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=30)
47/51:
ax = sns.catplot(x="Genre", data=beats[0:7000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=30)
47/52:
ax = sns.catplot(x="Genre", data=beats[0:7000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
47/53:
ax = sns.catplot(x="Genre", data=beats[7000:131552],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
47/54:
ax = sns.catplot(x="Genre", data=beats[0:14000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
47/55:
ax = sns.catplot(x="Genre", data=beats[0:10000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
47/56:
ax = sns.catplot(x="Genre", data=beats[10000:20000],aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
47/57: import plotly.graph_objs as go
47/58:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in data.dtypes])):
    if column == "object":
        data[data.columns[i]] = data[data.columns[i]].fillna(data[data.columns[i]].mode())
        data[data.columns[i]] = data[data.columns[i]].astype("category").cat.codes
    else:
        data[data.columns[i]] = data[data.columns[i]].fillna(data[data.columns[i]].median())
beats.head()
47/59:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
47/60: beats.tail()
47/61:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
47/62: beats.columns
47/63:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genre']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
47/64:
# Test output
standard_x
47/65:
# Import necessary modules
from sklearn.decomposition import PCA
47/66:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
47/67: len(principal_df)
47/68:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat(principal_df, target_y)
genre_principals.head()
47/69:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat(principal_df, pd.Series(target_y))
genre_principals.head()
47/70:
# Concatenate DataFrames before plotting the data
genre_principals = pd.concat(principal_df, beats['Genre'])
genre_principals.head()
47/71:
# Concatenate DataFrames before plotting the data
principal_df['Genre'] = beats['Genre']
principal_df.tail()
47/72: principal_df.info()
47/73:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
47/74:
# Determine the numbers of Genre
len(list(genre_principals.Genre.unique()))
47/75:
# Determine the numbers of Genre
len(list(principal_df.Genre.unique()))
47/76:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = genre_principals.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = genre_principals['Genre'] == target
    ax.scatter(genre_principals.loc[indices_to_keep, 'principal_component_1'], 
               genre_principals.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 30)
ax.legend(targets)
ax.grid()
47/77:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

targets = principal_df.Genre.unique()
colors = ['r', 'g', 'b']

for target, color in zip(targets, colors):
    indices_to_keep = principal_df['Genre'] == target
    ax.scatter(principal_df.loc[indices_to_keep, 'principal_component_1'], 
               principal_df.loc[indices_to_keep, 'principal_component_2'], 
               c = color, 
               s = 30)
ax.legend(targets)
ax.grid()
47/78:
import plotly.plotly as py
import plotly.graph_objs as go
47/79:
import plotly.plotly as py
import plotly.graph_objs as go
plotly.tools.set_credentials_file(username='emilyschoof', api_key='KYWeJXUM9JqgJCxde4BP')
47/80:
plotly.tools.set_credentials_file(username='emilyschoof', api_key='KYWeJXUM9JqgJCxde4BP')
import plotly.plotly as py
import plotly.graph_objs as go
47/81:
import plotly
plotly.tools.set_credentials_file(username='emilyschoof', api_key='KYWeJXUM9JqgJCxde4BP')
import plotly.plotly as py
import plotly.graph_objs as go
47/82:
conda install plotly
conda install -c conda-forge cufflinks-py
47/83:
pip install cufflinks
pip install plotly
47/84:
conda import plotly
conda import -c conda-forge cufflinks-py
47/85:
import plotly
import plotly.plotly as py
import plotly.graph_objs as go
47/86: go.Scattergl
47/87:
N = 100000
trace = go.Scattergl(
    x = np.random.randn(N),
    y = np.random.randn(N),
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1,
            color = '#404040')
    )
)
data = [trace]
py.iplot(data, filename='WebGL100000')
47/88:
N = 100000
trace = go.Scattergl(
    x = principal_df.principal_component_1,
    y = principal_df.principal_component_2,
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1,
            color = '#404040')
    )
)
data = [trace]
py.iplot(data, filename='WebGL100000')
47/89:
# Import necessary modules
import plotly
import plotly.plotly as py
import plotly.graph_objs as go
from IPython.display import IFrame
47/90:
N = 100000
trace = go.Scattergl(
    x = principal_df.principal_component_1,
    y = principal_df.principal_component_2,
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1,
            color = '#404040')
    )
)
data = [trace]
IFrame(data, filename='WebGL100000')
47/91:
N = 100000
trace = go.Scattergl(
    x = principal_df.principal_component_1,
    y = principal_df.principal_component_2,
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1,
            color = '#404040')
    )
)
data = [trace]
IFrame(data, filename='WebGL100000', width=700, height=350)
47/92:
N = 100000
trace = go.Scattergl(
    x = principal_df.principal_component_1,
    y = principal_df.principal_component_2,
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1,
            color = '#404040')
    )
)
data = [trace]
py.iplot(data, filename='WebGL100000')
47/93:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

trace = go.Scattergl(
    x = principal_df.principal_component_1,
    y = principal_df.principal_component_2,
    mode = 'markers',
    marker = dict(
        line = dict(
            width = 1)
    )
)
data = [trace]
py.iplot(data, filename='WebGL100000')
47/94:
for i in range(principal_df.Genre):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/95:
for i in range(principal_df.Genre.values):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/96:
# Reshape Genres column
genre_values = principal_df.Genre.values.reshape((1, -1))
47/97:
for i in range(genre_values):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/98:
for i in len(genre_values):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/99:
for i in range(1:625):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/100:
for i in range(625):
    go.Scattergl(
        x = principal_df.principal_component_1,
        y = principal_df.principal_component_2,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/101:
# Determine the numbers of Genre
len(principal_df.Genre.unique())
47/102:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2
47/103:
for i in range(625):
    colors = cm.rainbow(np.linspace(0, 1, len(pc_y))), # Set a colourmap and make a colour array
    for c in (colors):
        go.Scattergl(
            x = pc_x,
            y = pc_y,
            color = c,
            mode = 'markers',
            marker = dict(line = dict(width = 1))
        )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/104:
# Import necessary modules
import plotly
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
47/105:
for i in range(625):
    colors = cm.rainbow(np.linspace(0, 1, len(pc_y))), # Set a colourmap and make a colour array
    for c in (colors):
        go.Scattergl(
            x = pc_x,
            y = pc_y,
            color = c,
            mode = 'markers',
            marker = dict(line = dict(width = 1))
        )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/106:
for i in range(625):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.iplot(fig, filename='WebGL_line')
47/107:
layout = {
  "grid": {"ygap": 0.1}, 
  "title": "2 Component PCA", 
  "xaxis": {
    "title": "Principle Component 1", 
    "showline": True, 
    "autorange": True
  }, 
  "yaxis": { 
    "title": "Principle Component 2", 
    "autorange": True
  }, 
  "legend": {
    "y": 0.5, 
    "font": {
      "size": 20, 
      "color": "grey", 
      "family": "Arial, sans-serif"
    }, 
    "yref": "paper"
  }, 
  "autosize": True, 
  "hovermode": "closest"
}
47/108:
for i in range(625):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
47/109:
# View graph
from IPython.core.display import display, HTML
display('https://plot.ly/~emilyschoof/10.embed')
47/110:
# Import necessary modules
import plotly
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.core.display import IFrame
47/111:
# Import necessary modules
import plotly
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
47/112:
# View graph
IFrame(src='https://plot.ly/~emilyschoof/10.embed', width=700, height=600)
47/113:
# View graph
IFrame(src='https://plot.ly/~emilyschoof/10/', width=500, height=400)
47/114:
# View graph
IFrame(src='//plot.ly/~emilyschoof/10.embed', width=500, height=400)
47/115:
# View graph
IFrame(src='//plot.ly/~emilyschoof/10.embed', width=500, height=400)
47/116:
# View graph
IFrame(src='//plot.ly/~emilyschoof/10.embed', width=800, height=600)
47/117: pca.explained_variance_ratio_
47/118:
seq = ['Danceability','Energy', 'Loudness','Acousticness','Valence']

sns.pairplot(data=feature_corr[tuple(seq)], kind="reg")
47/119:
seq = ['Danceability','Energy','Loudness','Acousticness','Valence']
tuple(seq)
#sns.pairplot(data=feature_corr[tuple(seq)], kind="reg")
47/120:
seq = tuple(['Danceability','Energy','Loudness','Acousticness','Valence'])
sns.pairplot(data=feature_corr[seq], kind="reg")
47/121: explained_pca = pca.explained_variance_ratio_
47/122:
explained_pca = pca.explained_variance_ratio_
np.histogram(explained_pca)
47/123:
explained_pca = pca.explained_variance_ratio_
plt.hist(explained_pca, histtype='bar')
47/124:
explained_pca = pca.explained_variance_ratio_
plt.hist(explained_pca, bins=2, histtype='bar')
47/125:
explained_pca = pca.explained_variance_ratio_
plt.hist(explained_pca, bins=3, histtype='bar')
47/126:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_

# View bar plot
fig, ax = plt.subplots()
ax.yaxis.set_major_formatter(formatter)
plt.bar(explained_pca)
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/127:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_

# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(explained_pca)
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/128:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
explained_pca
47/129:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, y=range(0,0.5))
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/130: range(0, 0.5)
47/131: range(0, 1)
47/132:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, y=range(0, 1))
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/133:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, height=0.5)
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/134:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, height=0.5)
plt.xticks(explained_pca, ('PCA1', 'PCA2'))
plt.show()
47/135:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, height=0.5)
plt.xticks(('PCA1', 'PCA2'))
plt.show()
47/136:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, height=0.5)
plt.xticks((0.21918498, 0.12409448), ('PCA1', 'PCA2'))
plt.show()
47/137:
# View bar plot
fig, ax = plt.subplots()
#ax.yaxis.set_major_formatter(formatter)
plt.bar(x=explained_pca, height=0.5)
plt.xticks(2, ('PCA1', 'PCA2'))
plt.show()
47/138:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
plt.bar(x=explained_pca, height=0.5)
plt.xticks(x, ('PCA1', 'PCA2'))
plt.show()
47/139: explained_pca[0]
47/140:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=explained_pca[0], height=0.5)
pl2 = plt.bar(x=explained_pca[1], height=0.5)

# Define labels
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/141:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=1, height=explained_pca[0])
pl2 = plt.bar(x=2, height=explained_pca[1])

# Define labels
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/142:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=explained_pca[0])
pl2 = plt.bar(x=1, height=explained_pca[1])

# Define labels
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/143:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=explained_pca[0])
pl2 = plt.bar(x=1, height=explained_pca[1])

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/144:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[2]
47/145:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
47/146:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, tick_label=pca1, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/147:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, label=pca1, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/148:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, label=pca1, height=pca1)
pl1.annotate(pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/149:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
47/150:
train_data, test_data = train_test_split(genre_principals, test_size=0.20, random_state=0)
train_data.head(5)
47/151:
# Import necessary modules 
from sklearn.model_selection import train_test_split
47/152:
train_data, test_data = train_test_split(genre_principals, test_size=0.20, random_state=0)
train_data.head(5)
47/153:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
47/154:
# Save order and list of beats Genre list
genre_strings = beats['Genre']
47/155:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['Genre'])

# The response variable
y = train_data.copy().pop('Genre')
47/156:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
47/157:
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=Y[ y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/158:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
47/159:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
47/160:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/161:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
47/162:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/163:
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=y[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/164:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/165: np.unique(y)
47/166:
for idx, cl in enumerate(np.unique(y)):
    print(cl)
47/167: np.hstack((y_train, y_test))
47/168:
for idx, cl in enumerate(np.unique(np.hstack((y_train, y_test)))):
    print(cl)
47/169:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=y[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/170:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/171:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        print(y[y == cl, 1])
        plt.scatter(x=X[y == c1, 0],
                    y=y[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/172:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/173:
y = np.hstack((y_train, y_test))
for idx, cl in enumerate(np.unique(y)):
    print(y[y == cl, 1])
47/174:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        plt.scatter(x=X[y == c1, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/175:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/176:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        x=X[y == c1, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        #plt.scatter(,
                    #,
                    #alpha=0.6,
                    #c=colors[idx],
                    #marker=markers[idx],
                    #label=c1,
                    #edgecolor='black')
47/177:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/178:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        x=X[y == c1, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
47/179:
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree)
47/180: plot_decision_regions(X, y, classifier=knn)
47/181:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
47/182:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
47/183:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
47/184: plot_decision_regions(X, y, classifier=knn)
47/185:
# Plot decision regions
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))

# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.xlabel('PCA Audio Components')
plt.ylabel('Music Genre')
47/186:
# Plot decision regions
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))

# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
47/187:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
47/188: plot_decision_regions(X_combined, y_combined, classifier=knn)
47/189:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Fit data to KNN
knn.fit(X_train, y_train)
47/190: plot_decision_regions(X_combined, y_combined, classifier=knn)
47/191:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
47/192:
# Import necessary modules
from numpy import array
from sklearn.model_selection import KFold
47/193:
# prepare cross validation
kfold = KFold(n_splits=5, shuffle=True, random_state=9)

# create a dataframe to hold training and testing data from splits
kfold_splits = pd.DataFrame(columns=['Split Number','X_train', 'X_test', 'y_train', 'y_test', 'knn', 'kparams','kneighbors', 'kpredict', 'kscore'])
kfold_splits
47/194:
i = 0

# enumerate splits
for train_index, test_index in kfold.split(X, y):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    i += 1
    # add row to kfolds dataframe
    kfold_splits = kfold_splits.append({'Split Number':i , 
                         'X_train':X_train, 
                         'X_test':X_test, 
                         'y_train':y_train, 
                         'y_test':y_test, 
                         'knn':np.nan,
                         'kparams':np.nan,
                         'kneighbors':np.nan,
                         'kpredict':np.nan,
                         'kscore':np.nan}, ignore_index=True)
47/195:
i = 0

# enumerate splits
for train_index, test_index in kfold.split(X_train, y_train):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    i += 1
    
    # add row to kfolds dataframe
    kfold_splits = kfold_splits.append({'Split Number':i , 
                         'X_train':X_train, 
                         'X_test':X_test, 
                         'y_train':y_train, 
                         'y_test':y_test, 
                         'knn':np.nan,
                         'kparams':np.nan,
                         'kneighbors':np.nan,
                         'kpredict':np.nan,
                         'kscore':np.nan}, ignore_index=True)
47/196:
# Plot decision regions
plot_decision_regions(X, y, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
47/197:
# Plot decision regions
plot_decision_regions(X, y, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
47/198:
# Plot decision regions
plot_decision_regions(X_train, y_train, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
47/199:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
47/200:
i = 0

# enumerate splits
for train_index, test_index in kfold.split(X, y):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    i += 1
    
    # add row to kfolds dataframe
    kfold_splits = kfold_splits.append({'Split Number':i , 
                         'X_train':X_train, 
                         'X_test':X_test, 
                         'y_train':y_train, 
                         'y_test':y_test, 
                         'knn':np.nan,
                         'kparams':np.nan,
                         'kneighbors':np.nan,
                         'kpredict':np.nan,
                         'kscore':np.nan}, ignore_index=True)
47/201:
i = 0

# enumerate splits
for train_index, test_index in kfold.split(X, y):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    i += 1
    
    # add row to kfolds dataframe
    kfold_splits = kfold_splits.append({'Split Number':i , 
                         'X_train':X_train, 
                         'X_test':X_test, 
                         'y_train':y_train, 
                         'y_test':y_test, 
                         'knn':np.nan,
                         'kparams':np.nan,
                         'kneighbors':np.nan,
                         'kpredict':np.nan,
                         'kscore':np.nan}, ignore_index=True)
48/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
48/2:
# Install requirements.txt
!pip install -r requirements.txt
48/3:
# %prun -> displays report of how time was run for that function- spot inefficient pieces of code
# export python notebook to just python files: bit.ly/py-html-config
# http://www.waffle.io
48/4:
from IPython.display import Image
Image(filename='data/RecommendationSystems.jpg')
48/5:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
49/1:
from IPython.display import Image
Image(filename='data/RecommendationSystems.jpg')
48/6: beats.genre.unique()
48/7: beats.Genre.unique()
48/8:
# Insertions with SQL 
import sqlite3
48/9:
# Insertions with SQL 
import sqlite3 
connection = sqlite3.connect(beats)
48/10:
# Insertions with SQL 
import pandasql as ps
48/11:
# Insertions with SQL 
import pandasql as ps
48/12:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre,
  count(*) as population
from Table1
group by ccountry"""

print(ps.sqldf(q1))
48/13:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre,
  count(*) as population
from Table1
group by ccountry"""

print(ps.sqldf(music_query))
48/14:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats
group by Genre"""

print(ps.sqldf(music_query))
48/15:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats"""

print(ps.sqldf(music_query))
48/16:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats"""

beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/17:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats"""
48/18:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/19:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
48/20:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
48/21:
# List column names
list(beats.columns)
48/22: beats.info()
48/23: beats.head(1)
48/24:
# Insertions with SQL 
import pandasql as ps
48/25:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
48/26:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
48/27:
# Insertions with SQL 
import pandasql as ps
48/28:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats"""
48/29:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/30:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type'], axis=1)
48/31:
# Test output
beats.info()
48/32:
ax = sns.catplot(x="Genre", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
48/33:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
48/34:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
48/35:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
48/36:
# List column names
list(beats.columns)
48/37: beats.info()
48/38: beats.head(1)
48/39:
# Insertions with SQL 
import pandasql as ps
48/40:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/41:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
48/42:
# Test output
beats.info()
48/43:
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
48/44:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
48/45:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
48/46:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
48/47:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
48/48: beats.tail()
48/49:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
48/50: beats.columns
48/51:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
48/52:
# Test output
standard_x
48/53:
# Import necessary modules
from sklearn.decomposition import PCA
48/54:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
48/55: len(principal_df)
48/56:
# Concatenate DataFrames before plotting the data
principal_df['Genres'] = beats['Genres']
principal_df.tail()
48/57: principal_df.info()
48/58:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
48/59:
# Import necessary modules
import plotly # has packages for large datasets
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
48/60:
# Determine the numbers of Genre
len(principal_df.Genre.unique())
48/61:
# Determine the numbers of Genre
len(principal_df.Genres.unique())
48/62:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2
48/63:
# Plot the graph
for i in range(15):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
48/64:
# Plot the graph
for i in range(15):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
48/65:
# Plot the graph
trace = for i in range(15):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
data = [trace]
fig=dict(data=data, layout=layout)
py.plot(fig)
48/66:
# Plot the graph
trace = (for i in range(15):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    ))

layout = dict(showlegend=False)
data = [trace]
fig=dict(data=data, layout=layout)
py.plot(fig)
48/67:
# Plot the graph
for i in range(15):
    go.Scattergl(
        x = pc_x,
        y = pc_y,
        mode = 'markers',
        marker = dict(line = dict(width = 1))
    )

layout = dict(showlegend=False)
data = [trace]
fig=dict(data=data, layout=layout)
py.plot(fig)
48/68:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2
print(pc_x)
48/69:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2
48/70:
# Plot the graph
data = []

for i in range(15):
    data.append(go.Scattergl(
                x = pc_x,
                y = pc_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
               )
    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
48/71:
# Plot the graph
for i in range(15):
    go.Scattergl(x = pc_x,
                y = pc_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
    
layout = dict(showlegend=False)
fig=dict(data=principal_df, layout=layout)
py.plot(fig)
48/72:
# Plot the graph
trace = go.Scattergl(x = pc_x,
                y = pc_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
    
layout = dict(showlegend=False)
data = [trace]
fig=dict(data=, layout=layout)
py.plot(fig)
48/73:
# Plot the graph
trace = go.Scattergl(x = pc_x,
                y = pc_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
48/74:
# View graph
IFrame(src='//plot.ly/~emilyschoof/14.embed', width=800, height=600)
48/75:
# View graph
IFrame(src='//plot.ly/~emilyschoof/14.embed', width=800, height=600)
48/76:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
48/77:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
48/78:
# Import necessary modules
from sklearn.manifold import TSNE
48/79:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2).fit_transform(X)
X_embedded.shape
48/80:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2).fit_transform(standard_x)
X_embedded.shape
48/81:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2)
#X_embedded.shape
48/82:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2)
X_embedded = X_embedded.fit_transform(standard_x)
#X_embedded.shape
48/83:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2)
#X_embedded.shape
48/84: X_embedded = X_embedded.fit(standard_x)
48/85:
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.set_title('Number of Instances of Each Genre within Beats')
plt.show()
48/86:
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.title('Number of Instances of Each Genre within Beats')
plt.show()
48/87:
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
sns.plt.title('Number of Instances of Each Genre within Beats')
plt.show()
48/88:
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
48/89:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
48/90:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
48/91:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
48/92:
# List column names
list(beats.columns)
48/93: beats.info()
48/94: beats.head(1)
48/95:
# Insertions with SQL 
import pandasql as ps
48/96:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
  end as Genre
from beats"""
48/97:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/98:
# Save Genre and Genres Series for the Genre key
genres_key = beats[['Genre', 'Genres']].unique()
genres_key
48/99:
# Save Genre and Genres Series for the Genre key
genres_key = np.unique(beats[['Genre', 'Genres']])
genres_key
48/100:
# Save Genre and Genres Series for the Genre key
genres_key = beats[['Genre', 'Genres']]
genres_key = genres_key.drop_duplicates()
genres_key
48/101:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%salsa%' then 'Salsa'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    else 'Misc.'
  end as Genre
from beats"""
48/102:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/103:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
48/104:
# Test output
beats.info()
48/105:
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
48/106: beats['Genres']
48/107:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
48/108:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
48/109:
# List column names
list(beats.columns)
48/110: beats.info()
48/111: beats.head(1)
48/112: betas['Genres']
48/113: beats['Genres']
48/114: beats['Genre']
48/115: list(beats['Genre'])
48/116: list(beats['Genre'].unique())
48/117:
# Insertions with SQL 
import pandasql as ps
48/118:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%trance%' then 'Trance'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    when Genre like '%dub%' then 'Dub'
  end as Genre
from beats"""
48/119:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
48/120:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
48/121:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
50/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
50/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
50/3:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
50/4:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
50/5:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
50/6:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
50/7:
# List column names
list(beats.columns)
50/8: beats.info()
50/9: beats.head(1)
50/10: list(beats['Genre'].unique())
50/11: #list(beats['Genre'].unique())
50/12:
# Insertions with SQL 
import pandasql as ps
50/13:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%trance%' then 'Trance'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    when Genre like '%dub%' then 'Dub'
  end as Genre
from beats"""
50/14:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
50/15:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
50/16:
# Test output
beats.info()
50/17:
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/18:
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/19:
fig = plt.subplot(1)
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/20:
fig = plt.subplot(221)
fig.subplots_adjust(top=0.9)
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/21:
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.fig.subplots_adjust(top=0.9)
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/22:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
50/23:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
50/24:
# Import necessary modules
from scipy.stats import norm
from statsmodels.distributions.empirical_distribution import ECDF
50/25:
# View counts
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.fig.subplots_adjust(top=0.9)
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
50/26:
# Import necessary modules
import plotly # has packages for large datasets
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
50/27:
# Define x and y variables
energy_x = beats['Energy']
loudness_y = beats['Loudness']
50/28:
# Plot the graph
trace = go.Scattergl(x = energy_x,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/29:
# Visualize data on two-dimensional scatter plot

# select genres
y = beats['Genres'].values

# extract top features
x = beats[['Energy', 'Loudness']]

print(y)
50/30:
# Visualize data on two-dimensional scatter plot

# select genres
y = np.unique(beats['Genres']).values

# extract top features
x = beats[['Energy', 'Loudness']]

print(y)
50/31:
# Visualize data on two-dimensional scatter plot

# select genres
y = beats['Genres'].drop_duplicates().values

# extract top features
x = beats[['Energy', 'Loudness']]

print(y)
50/32:
# Visualize data on two-dimensional scatter plot

# select genres
y = beats['Genres'].drop_duplicates().values

# extract top features
x = beats[['Energy', 'Loudness']]

print(x)
50/33:
# Plot the graph
trace = go.Scattergl(x = beats['Genres'],
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/34:
# View graph
IFrame(src='//plot.ly/~emilyschoof/18.embed', width=800, height=600)
50/35:
# Define x and y variables
energy_x = abs(beats['Energy'])
loudness_y = abs(beats['Loudness'])
50/36:
# Plot the graph
trace = go.Scattergl(x = beats['Genres'],
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/37:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=800, height=600)
50/38:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=800, height=600)
50/39:
# Define x and y variables
energy_y = abs(beats['Energy'])
loudness_y = abs(beats['Loudness'])
50/40:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=400, height=300)
50/41:
# Plot the graph
trace = go.Scattergl(x = beats['Genres'],
                y = energy_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/42:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=600, height=500)
50/43: IFrame(src='//plot.ly/~emilyschoof/22.embed', width=800, height=600)
50/44: IFrame(src='//plot.ly/~emilyschoof/22.embed', width=600, height=500)
50/45: IFrame(src='//plot.ly/~emilyschoof/22.embed', width=600, height=500)
50/46:
# Plot the graph
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/47:
# Plot the graph
trace = go.Scattergl(data=beats,
                x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/48:
# Plot the graph
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
50/49:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
50/50:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
50/51:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
50/52:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
50/53: beats.tail()
50/54:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
50/55: beats.columns
50/56:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
50/57:
# Test output
standard_x
50/58:
# Import necessary modules
from sklearn.decomposition import PCA
50/59:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
50/60: len(principal_df)
50/61:
# Concatenate DataFrames before plotting the data
principal_df['Genres'] = beats['Genres']
principal_df.tail()
50/62: principal_df.info()
50/63:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
50/64:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2
50/65:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.unique().values
50/66:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
target_genres
50/67:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
target_genres, len(target_genres)
50/68:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
print(target_genres, len(target_genres))

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
50/69:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
print(target_genres, len(target_genres))

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors)
50/70:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
print(target_genres)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(target_genres)
50/71:
# Concatenate DataFrames before plotting the data
principal_df['target'] = beats['Genres']
principal_df.tail()
50/72:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)

# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
50/73: len(principal_df)
50/74:
# Concatenate DataFrames before plotting the data
principal_df['target'] = beats['Genres']
principal_df.tail()
50/75: principal_df.info()
50/76:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
50/77:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
target_genres = genre_strings.drop_duplicates().values
print(target_genres)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(target_genres)
50/78:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(target_genres,colors):
    indicesToKeep = principal_df['target'] == target
    ax.scatter(principal_df.loc[indicesToKeep, 'principal component 1']
               , principal_df.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
50/79:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
50/80:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    indicesToKeep = principal_df['target'] == target
    ax.scatter(x=pc_x,
               y=pc_y,
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/81:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    indicesToKeep = principal_df['target'] == target
    print(indicesToKeep)
    #ax.scatter(principal_df.loc[indicesToKeep, 'principal component 1'],
               #principal_df.loc[indicesToKeep, 'principal component 2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/82:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    indicesToKeep = principal_df['target'] == target
    #print(indicesToKeep)
    print(principal_df.loc[indicesToKeep, 'principal_component_1'])
    #ax.scatter(principal_df.loc[indicesToKeep, 'principal component 1'],
               #principal_df.loc[indicesToKeep, 'principal component 2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/83:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    print(target)
    #target_indices = principal_df['target'] == target
    #print(indicesToKeep)
    #ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               #principal_df.loc[target_indices, 'principal_component_2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/84:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_indices = principal_df['target'] == target
    print(target_indices)
    #ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               #principal_df.loc[target_indices, 'principal_component_2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/85:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_indices = principal_df['target'] == target
    print(principal_df.loc[target_indices, 'principal_component_1'])
    #ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               #principal_df.loc[target_indices, 'principal_component_2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/86:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_indices = principal_df['target'] == target
    ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               principal_df.loc[target_indices, 'principal_component_2'],
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/87:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    print(target_df)
    #ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               #principal_df.loc[target_indices, 'principal_component_2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/88:
# Concatenate DataFrames before plotting the data
principal_df['target'] = genre_strings
principal_df.tail()
50/89: principal_df.info()
50/90:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
50/91:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
50/92:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_indices = principal_df['target'] == target
    ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               principal_df.loc[target_indices, 'principal_component_2'],
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/93:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    print(target_df)
    #ax.scatter(principal_df.loc[target_indices, 'principal_component_1'],
               #principal_df.loc[target_indices, 'principal_component_2'],
               #c = colors,
               #s = 50)
ax.legend(targets)
ax.grid()
50/94:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    ax.scatter(target_df.loc['principal_component_1'],
               target_df.loc['principal_component_2'],
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/95:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    ax.scatter(target_df.['principal_component_1'].values,
               target_df.['principal_component_2'].values,
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/96:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    ax.scatter(target_df['principal_component_1'].values,
               target_df['principal_component_2'].values,
               c = colors,
               s = 50)
ax.legend(targets)
ax.grid()
50/97:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    ax.scatter(target_df['principal_component_1'].values,
               target_df['principal_component_2'].values,
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/98:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/99:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
50/100:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
50/101:
# Import necessary modules
from sklearn.manifold import TSNE
50/102: X_embedded.fit(standard_x)
50/103:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=2)
50/104: X_embedded.fit(standard_x)
50/105: X_embedded.fit(X)
50/106: X_embedded.fit(standard_x)
50/107:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
50/108:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
50/109:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['Genre'])

# The response variable
y = train_data.copy().pop('Genre')
50/110:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
50/111:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
50/112:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
50/113:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        x=X[y == c1, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/114:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
50/115:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
50/116:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/117:
# Concatenate DataFrames before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
50/118: principal_df.info()
50/119:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
50/120:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
50/121:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(target_string,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/122:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/123:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
50/124:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
50/125:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
50/126:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['target', 'target_string'])

# The response variable
y = train_data.copy().pop('target')
50/127:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
50/128:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
50/129:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        x=X[y == c1, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/130:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
50/131:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
50/132:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/133:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, c1 in enumerate(np.unique(y)):
        x=X[y == c1, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/134:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
50/135:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
50/136:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/137:
for idx, c1 in enumerate(np.unique(y_combined)):
        x=X[y_combined == c1, 0]
        y=X[y_combined == cl, 1]
        return X_combined.size, y_combined.size
        plt.scatter(X_combined,y_combined,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/138:
for idx, c1 in enumerate(np.unique(y_combined)):
        x=X[y_combined == c1, 0]
        y=X[y_combined == cl, 1]
        print(X_combined.size, y_combined.size)
        plt.scatter(X_combined,y_combined,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/139:
for idx, c1 in enumerate(np.unique(y_combined)):
        x=X_combined[y_combined == c1, 0]
        y=X_combined[y_combined == cl, 1]
        print(X_combined.size, y_combined.size)
        plt.scatter(X_combined,y_combined,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/140:
# Entire dataset (even with response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
50/141:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
50/142:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
50/143:
for idx, c1 in enumerate(np.unique(y_combined)):
        x=X_combined[y_combined == c1, 0]
        y=X_combined[y_combined == cl, 1]
        print(X_combined.size, y_combined.size)
        plt.scatter(X_combined,y_combined,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/144:
for idx, cl in enumerate(np.unique(y_combined)):
        x=X_combined[y_combined == cl, 0]
        y=X_combined[y_combined == cl, 1]
        print(X_combined.size, y_combined.size)
        plt.scatter(X_combined,y_combined,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/145:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
50/146:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/147:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.legend(labels)
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/148:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.legend(targets)
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/149:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.legend(targets, loc='left')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/150:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.legend(targets, loc='right')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/151:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
50/152:
# Create a Principle Component instance with 2 principle components
pca = PCA()
pca.n_components_
50/153:
# Create a Principle Component instance with 2 principle components
pca = PCA(.95)
pca.n_components_
50/154:
# Create a Principle Component instance with 2 principle components
pca = PCA(.95)
50/155:
# Create a Principle Component instance with 2 principle components
pca = PCA(.95)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
50/156:
# Create a Principle Component instance with 2 principle components
pca = PCA(.95)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
principal_components.n_components_
50/157:
# Create a Principle Component instance with 2 principle components
pca = PCA(.95)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
50/158:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
50/159:
# Create a Principle Component instance with 2 principle components
pca = PCA(.75)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
50/160:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
50/161:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
50/162:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 'principal_component_3', 'principal_component_4'])
principal_df.head()
50/163:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
50/164: len(principal_df)
50/165:
# Concatenate DataFrames before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
50/166: principal_df.info()
50/167:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
50/168:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
50/169:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/170:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 2', fontsize = 15)
ax.set_ylabel('Principal Component 3', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/171:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 3', fontsize = 15)
ax.set_ylabel('Principal Component 4', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/172:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 4', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
50/173:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
50/174:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 3, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
50/175:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
50/176:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(n_components=4)
50/177:
# Create TSNE instance with 2 components and fit to model
X_embedded = TSNE(learning_rate=50)
50/178:
# Create TSNE instance
tsne = TSNE(learning_rate=50)
50/179:
# Define features
tsne_features = tsne.fit_transform(standard_x)
51/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
51/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
51/3:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
51/4:
# List column names
list(beats.columns)
51/5: beats.info()
51/6: beats.head(1)
51/7: #list(beats['Genre'].unique())
51/8:
# Insertions with SQL 
import pandasql as ps
51/9:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%trance%' then 'Trance'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    when Genre like '%dub%' then 'Dub'
  end as Genre
from beats"""
51/10:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
51/11:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
51/12:
# Test output
beats.info()
51/13:
# View counts
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.fig.subplots_adjust(top=0.9)
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
51/14:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
51/15:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
51/16:
# Import necessary modules
import plotly # has packages for large datasets
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
51/17:
# Define x and y variables
energy_y = abs(beats['Energy'])
loudness_y = abs(beats['Loudness'])
51/18:
# Plot the graph
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
51/19:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
51/20:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=600, height=500)
51/21: IFrame(src='//plot.ly/~emilyschoof/22.embed', width=600, height=500)
51/22:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
51/23:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
51/24: beats.tail()
51/25:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
51/26: beats.columns
51/27:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
51/28:
# Test output
standard_x
51/29:
# Import necessary modules
from sklearn.decomposition import PCA
51/30:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
51/31:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
51/32: len(principal_df)
51/33:
# Concatenate DataFrames before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
51/34: principal_df.info()
51/35:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
51/36:
# Set x and y variables
pc_x = principal_df.principal_component_1
pc_y = principal_df.principal_component_2

# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
51/37:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
51/38:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 2', fontsize = 15)
ax.set_ylabel('Principal Component 3', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
51/39:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 3', fontsize = 15)
ax.set_ylabel('Principal Component 4', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
51/40:
# Plot graph 
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 4', fontsize = 15)
ax.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
51/41:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
51/42:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
51/43:
# Import necessary modules
from sklearn.manifold import TSNE
51/44:
# Create TSNE instance
tsne = TSNE(learning_rate=50)
51/45:
# Define features
tsne_features = tsne.fit_transform(standard_x)
52/1:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
52/2:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
52/3:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl

# Recommending System
%run Recommenders.ipynb
import networkx as nx
52/4:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
52/5:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
52/6:
# List column names
list(beats.columns)
52/7: beats.info()
52/8: beats.head(1)
52/9: #list(beats['Genre'].unique())
52/10:
# Insertions with SQL 
import pandasql as ps
52/11:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%trance%' then 'Trance'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    when Genre like '%dub%' then 'Dub'
  end as Genre
from beats"""
52/12:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
52/13:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
52/14:
# Test output
beats.info()
52/15:
# Store as global variable
%store beats
53/1:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
53/2:
# Import data
%store -r beats
53/3:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
53/4:
# View counts
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.fig.subplots_adjust(top=0.9)
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
53/5:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
53/6:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
53/7:
# Import necessary modules
import plotly # has packages for large datasets
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
53/8:
# Define x and y variables
energy_y = abs(beats['Energy'])
loudness_y = abs(beats['Loudness'])
53/9:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
53/10:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
53/11:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
53/12:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
53/13:
# View graph
IFrame(src='//plot.ly/~emilyschoof/20.embed', width=600, height=500)
53/14: IFrame(src='//plot.ly/~emilyschoof/22.embed', width=600, height=500)
53/15:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
52/16:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
54/1:
# Import data
%store -r beats2
54/2:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats.loc[:, features].values

# Separate out the target
target_y = beats.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
54/3:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
54/4:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
54/5:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)
54/6:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)

# Test output
standard_x
54/7:
# Import necessary modules
from sklearn.decomposition import PCA
54/8:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
54/9:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
54/10:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
54/11:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
54/12:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
52/17:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
52/18:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
52/19:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
54/13:
# Import data
%store -r beats2
%store -r genre_strings
54/14:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
54/15:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
54/16: principal_df.info()
54/17:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
54/18:
# Set target 
targets = genre_strings.drop_duplicates().values
print(targets)

# Set target colors
colors = ['black','chocolate','red','orange','gold', 
          'yellowgreen', 'olivedrab', 'aquamarine','c','deepskyblue',
          'steelblue', 'slategrey', 'midnightblue', 'blue', 'mediumpurple',
          'darkviolet', 'violet', 'deeppink', 'pink', 'lightgreen']
len(colors), len(targets)
54/19:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
54/20:
# Plot graph 
fig, (ax1, ax2, ax3,ax4) = plt.subplots(nrows=2, ncols=2) 
ax = fig.add_subplot(1,1,1) 
ax1.set_xlabel('Principal Component 1', fontsize = 15)
ax1.set_ylabel('Principal Component 2', fontsize = 15)
ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/21:
fig, axes = plt.subplots(2, 2, subplot_kw=dict(polar=True))
axes[0, 0].plot(x, y)
axes[1, 1].scatter(x, y)
54/22:
# Plot graph 
fig, axes = plt.subplots(nrows=2, ncols=2) 
ax = fig.add_subplot(1,1,1) 
ax[0,0].set_xlabel('Principal Component 1', fontsize = 15)
ax[0,0].set_ylabel('Principal Component 2', fontsize = 15)
ax[0,0].set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax[0,0].scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax[1,0].scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax[0,1].scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax[1,1].scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/23:
# Plot graph 
fig,  = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False) 
ax = fig.add_subplot(1,1,1) 
ax1.set_xlabel('Principal Component 1', fontsize = 15)
ax1.set_ylabel('Principal Component 2', fontsize = 15)
ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/24:
# Plot graph 
fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False) 
ax = fig.add_subplot(1,1,1) 
ax1.set_xlabel('Principal Component 1', fontsize = 15)
ax1.set_ylabel('Principal Component 2', fontsize = 15)
ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/25:
# Plot graph 
fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=4, sharex=False, sharey=False) 
ax = fig.add_subplot(1,1,1) 
ax1.set_xlabel('Principal Component 1', fontsize = 15)
ax1.set_ylabel('Principal Component 2', fontsize = 15)
ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/26:
# Plot graph 
fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=4, sharex=False, sharey=False) 
#ax = fig.add_subplot(1,1,1) 
#ax1.set_xlabel('Principal Component 1', fontsize = 15)
#ax1.set_ylabel('Principal Component 2', fontsize = 15)
#ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/27:
# Plot graph 
fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False) 
#ax = fig.add_subplot(1,1,1) 
#ax1.set_xlabel('Principal Component 1', fontsize = 15)
#ax1.set_ylabel('Principal Component 2', fontsize = 15)
#ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/28:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/29:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
import numpy as np
54/30:
# Plot graph 
fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False) 
#ax = fig.add_subplot(1,1,1) 
#ax1.set_xlabel('Principal Component 1', fontsize = 15)
#ax1.set_ylabel('Principal Component 2', fontsize = 15)
#ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/31:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/32:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, (ax1, ax2) = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/33:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, (ax1, ax2, ax3) = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/34:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, (ax1, ax2, ax3, ax4) = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/35:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, ax1, ax2, ax3, ax4 = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/36:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, ax1, ax2, ax3 = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/37:
# First create some toy data:
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Creates two subplots and unpacks the output array immediately
f, ax1, ax2 = plt.subplots(2, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
54/38:
# Plot graph 
fig, [[ax1, ax2],[ax3, ax4]] = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(10,4)) 
#ax = fig.add_subplot(1,1,1) 
#ax1.set_xlabel('Principal Component 1', fontsize = 15)
#ax1.set_ylabel('Principal Component 2', fontsize = 15)
#ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/39:
# Plot graph 
fig, [[ax1, ax2],[ax3, ax4]] = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(10,4)) 
#ax = fig.add_subplot(1,1,1) 
ax1.set_xlabel('Principal Component 1', fontsize = 15)
ax1.set_ylabel('Principal Component 2', fontsize = 15)
ax1.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/40:
# Plot graph 
fig, [[ax1, ax2],[ax3, ax4]] = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(10,4)) 

ax1.set_xlabel('Principal Component 1', fontsize = 10)
ax1.set_ylabel('Principal Component 2', fontsize = 10)
fig.set_title('2 of 4 Component PCA of Music Genres by Audio Features', fontsize = 20)

ax2.set_xlabel('Principal Component 2', fontsize = 10)
ax2.set_ylabel('Principal Component 3', fontsize = 10)

ax3.set_xlabel('Principal Component 3', fontsize = 10)
ax3.set_ylabel('Principal Component 4', fontsize = 10)

ax4.set_xlabel('Principal Component 1', fontsize = 10)
ax4.set_ylabel('Principal Component 4', fontsize = 10)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/41:
# Plot graph 
fig, [[ax1, ax2],[ax3, ax4]] = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(10,4)) 

ax1.set_xlabel('Principal Component 1', fontsize = 10)
ax1.set_ylabel('Principal Component 2', fontsize = 10)

ax2.set_xlabel('Principal Component 2', fontsize = 10)
ax2.set_ylabel('Principal Component 3', fontsize = 10)

ax3.set_xlabel('Principal Component 3', fontsize = 10)
ax3.set_ylabel('Principal Component 4', fontsize = 10)

ax4.set_xlabel('Principal Component 1', fontsize = 10)
ax4.set_ylabel('Principal Component 4', fontsize = 10)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_3'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_1'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
ax.legend(targets)
ax.grid()
54/42:
# Plot graph 
fig, [[ax1, ax2],[ax3, ax4],[ax5, ax6]] = plt.subplots(nrows=3, ncols=3, sharex=False, sharey=False, figsize=(10,4)) 

ax1.set_xlabel('Principal Component 1', fontsize = 10)
ax1.set_ylabel('Principal Component 2', fontsize = 10)

ax2.set_xlabel('Principal Component 1', fontsize = 10)
ax2.set_ylabel('Principal Component 3', fontsize = 10)

ax3.set_xlabel('Principal Component 2', fontsize = 10)
ax3.set_ylabel('Principal Component 3', fontsize = 10)

ax4.set_xlabel('Principal Component 2', fontsize = 10)
ax4.set_ylabel('Principal Component 4', fontsize = 10)

ax5.set_xlabel('Principal Component 4', fontsize = 10)
ax5.set_ylabel('Principal Component 1', fontsize = 10)

ax6.set_xlabel('Principal Component 4', fontsize = 10)
ax6.set_ylabel('Principal Component 3', fontsize = 10)

for target, color in zip(targets,colors):
    target_df = principal_df[principal_df['target_string'] == target]
    ax1.scatter(target_df['principal_component_1'],
               target_df['principal_component_2'],
               c = color,
               s = 50)
    ax2.scatter(target_df['principal_component_1'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax3.scatter(target_df['principal_component_2'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
    ax4.scatter(target_df['principal_component_2'],
               target_df['principal_component_4'],
               c = color,
               s = 50)
    ax5.scatter(target_df['principal_component_4'],
               target_df['principal_component_1'],
               c = color,
               s = 50)
    ax6.scatter(target_df['principal_component_4'],
               target_df['principal_component_3'],
               c = color,
               s = 50)
fig.legend(targets)
fig.grid()
54/43:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
import numpy as np
import seaborn as sns
54/44:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
import numpy as np
import seaborn as sns; sns.set(style="ticks", color_codes=True)
54/45:
# Plot graph
sns.pairplot(principal_df)
54/46:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
54/47:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
54/48:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
54/49:
# Import necessary modules
from sklearn.manifold import TSNE
54/50:
# Create TSNE instance
tsne = TSNE(learning_rate=50)
54/51:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
54/52:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
52/20:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
52/21:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
52/22: beats.tail()
52/23:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
54/53:
# Import data
%store -r beats2
%store -r genre_strings
54/54:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats['Genres']
principal_df.tail()
54/55:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
54/56: principal_df.info()
54/57:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
54/58:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
54/59:
# Plot graph
sns.pairplot(principal_df)
54/60:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
54/61:
# Drop target_string column
principal_df = principal_df.drop(columns='target_string')
54/62:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
54/63:
# Entire dataset (without response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
54/64:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
54/65:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
54/66:
# Source Code: 'Training Simple Machine Learning Algorithms for Classigication' pg.32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
54/67:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
54/68:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
54/69:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
54/70:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
54/71:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Fit data to KNN
knn.fit(X_train, y_train)
54/72:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
54/73:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
54/74:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
54/75:
# Drop target_string column, PCA3, and PCA4
principal_df = principal_df.drop(columns=['principal_component_3','principal_component_4','target_string'])
54/76:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
54/77:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
54/78: principal_df.info()
54/79:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
54/80:
# Drop target_string column, PCA3, and PCA4
principal_df = principal_df.drop(columns=['principal_component_3','principal_component_4','target_string'])
54/81:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
54/82:
# Entire dataset (without response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
54/83:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
54/84:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
54/85:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
54/86:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
54/87:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
54/88:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
54/89:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
54/90:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Fit data to KNN
knn.fit(X_train, y_train)
54/91:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
56/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
56/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
56/3:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
56/4:
# List column names
list(beats.columns)
56/5: beats.info()
56/6: beats.head(1)
56/7: #list(beats['Genre'].unique())
56/8:
# Insertions with SQL 
import pandasql as ps
56/9:
music_query = """
select
  case 
    when Genre like '%metal%' then 'Metal'
    when Genre like '%trap%' then 'Trap'
    when Genre like '%rock%' then 'Rock'
    when Genre like '%hiphop%' then 'HipHop'
    when Genre like '%blues%' then 'Blues'
    when Genre like '%pop%' then 'Pop'
    when Genre like '%folk%' then 'Folk'
    when Genre like '%punk%' then 'Punk'
    when Genre like '%indie%' then 'Indie'
    when Genre like '%trance%' then 'Trance'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'House'
    when Genre like '%country%' then 'Country'
    when Genre like '%jazz%' then 'Jazz'
    when Genre like '%choir%' then 'Choir'
    when Genre like '%beat%' then 'Beat'
    when Genre like '%rap%' then 'Rap'
    when Genre like '%reggae%' then 'Reggae'
    when Genre like '%chill%' then 'Chill'
    when Genre like '%dub%' then 'Dub'
  end as Genre
from beats"""
56/10:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
56/11:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
56/12:
# Test output
beats.info()
56/13:
# Store as global variable
%store beats
56/14:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
56/15:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
56/16: beats.tail()
56/17:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
56/18:
# Install requirements.txt
!pip install -r requirements.txt
56/19:
# install watermark extension
!pip install --upgrade pip
!pip install watermark
56/20:
# Use a future note
%load_ext watermark
56/21: %watermark -a "Emily Schoof" -d -t -v -p numpy,pandas,seaborn,matplotlib,sklearn -g
58/1:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
58/2:
# default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver = 'lbfgs')
58/3:
# Fit model to data
logisticRegr.fit(X_train, y_train)
58/4:
# Import data
%store -r beats2
%store -r genre_strings
58/5:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
58/6:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)

# Test output
standard_x
58/7:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
58/8:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
58/9:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
58/10:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
58/11: principal_df.info()
58/12:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
58/13:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
58/14:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
58/15:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
58/16:
# Import necessary modules
import matplotlib as plt
58/17:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
58/18:
# Import necessary modules
import matplotlib.pyplot as plt
58/19:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
58/20:
# Import necessary modules
import matplotlib.pyplot as plt
import numpy as np
58/21:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
58/22:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
58/23:
# Drop target_string column, PCA3, and PCA4
principal_df = principal_df.drop(columns=['principal_component_3','principal_component_4','target_string'])
58/24:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
58/25:
# Entire dataset (without response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
58/26:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
58/27:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
58/28:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
58/29:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
58/30:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
58/31:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
58/32:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
58/33:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Fit data to KNN
knn.fit(X_train, y_train)
58/34:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
58/35:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
58/36:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver = 'lbfgs')
58/37:
# Fit model to data
logisticRegr.fit(X_train, y_train)
58/38:
# Fit model to data
logisticRegr.fit(X_train, y_train, multi_class='warn')
58/39:
# Fit model to data
logisticRegr.fit(X_train, y_train)
58/40:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='warn')
58/41:
# Fit model to data
logisticRegr.fit(X_train, y_train)
58/42:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='auto')
58/43:
# Fit model to data
logisticRegr.fit(X_train, y_train)
58/44:
# Predict for One Observation 
logisticRegr.predict(X_test[0].reshape(1,-1))
58/45:
# Predict for One Observation
logisticRegr.predict(X_test[0:10])
58/46:
#While accuracy is not always the best metric for machine learning algorithms, it is used here for simplicity.
logisticRegr.score(X_test, y_test)
59/1:
# Import data
%store -r beats2
%store -r genre_strings
59/2:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
59/3:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)

# Test output
standard_x
59/4:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
59/5:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x)
59/6:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
59/7:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
59/8: principal_df.info()
59/9:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
59/10:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
59/11:
# Plot graph
sns.pairplot(principal_df)
59/12:
# Import necessary modules
import matplotlib.pyplot as plt
import numpy as np
59/13:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
59/14:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
59/15:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/16:
# Create LDA instance
clf = LinearDiscriminantAnalysis()
59/17:
# Create LDA instance
lda = LinearDiscriminantAnalysis()
59/18:
# Fit model to data
lda.fit(X, y)
59/19: beats2.head(1)
59/20:
# Fit and transform model to standardized data
lda.fit_transform(standard_x)
59/21:
# Fit and transform model to standardized data
lda.fit_transform(standard_x, target_y)
59/22:
# Fit and transform model to standardized data
lda.fit(standard_x)
59/23:
# Fit and transform model to standardized data
lda.fit(standard_x, target_y)
59/24:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2.loc[:,['Genres']].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)

# Test output
standard_x, target_y
59/25:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x, target_y)
59/26:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
59/27:
# Add columns before plotting the data
principal_df['target_string'] = genre_strings
principal_df['target'] = beats2['Genres']
principal_df.tail()
59/28: principal_df.info()
59/29:
# Drop NaN
principal_df = principal_df.dropna()
principal_df.info()
59/30:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
59/31:
# Plot graph
sns.pairplot(principal_df)
59/32:
# Import necessary modules
import matplotlib.pyplot as plt
import numpy as np
59/33:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
59/34:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
59/35:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/36:
# Create LDA instance
lda = LinearDiscriminantAnalysis()
59/37:
# Fit and transform model to standardized data
lda.fit(standard_x, target_y)
59/38:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
59/39:
# Drop target_string column, PCA3, and PCA4
principal_df = principal_df.drop(columns=['principal_component_3','principal_component_4','target_string'])
59/40:
train_data, test_data = train_test_split(principal_df, test_size=0.20, random_state=0)
train_data.head(5)
59/41:
# Entire dataset (without response variable)
X = train_data.copy().drop(columns=['target'])

# The response variable
y = train_data.copy().pop('target')
59/42:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/43:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
59/44:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/45:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
59/46:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
59/47:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
59/48:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
59/49:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Fit data to KNN
knn.fit(X_train, y_train)
59/50:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
59/51:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
59/52:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='auto')
59/53:
# Fit model to data
logisticRegr.fit(X_train, y_train)
59/54:
# Predict for One Observation 
logisticRegr.predict(X_test[0].reshape(1,-1))
59/55:
# Predict for One Observation
logisticRegr.predict(X_test[0:10])
59/56:
#While accuracy is not always the best metric for machine learning algorithms, it is used here for simplicity.
logisticRegr.score(X_test, y_test)
59/57:
# Define the features of beats
features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms', 'time_signature', ]

# Separate out the features
feature_x = beats2.loc[:, features].values

# Separate out the target
target_y = beats2['Genres'].values

# Standardizing the features
standard_x = StandardScaler().fit_transform(feature_x)

# Test output
standard_x, target_y
59/58:
# Fit and transform model to standardized data
lda.fit(standard_x, target_y)
59/59:
# Fit and transform model to standardized data
lda_components = lda.fit(standard_x, target_y)
59/60:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_1'])
linear_discrim_df.head()
59/61:
# Fit and transform model to standardized data
lda_components = lda.fit(standard_x, target_y)  
lda_components
59/62:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=2)
59/63:
# Fit and transform model to standardized data
lda_components = lda.fit(standard_x, target_y)  
lda_components
59/64:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_1'])
linear_discrim_df.head()
59/65:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2'])
linear_discrim_df.head()
59/66:
# Fit and transform model to standardized data
lda_components = lda.fit_transform(standard_x, target_y)  
lda_components
59/67:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2'])
linear_discrim_df.head()
59/68:
# Add columns before plotting the data
linear_discrim_df['target_string'] = genre_strings
linear_discrim_df['target'] = beats2['Genres']
linear_discrim_df.tail()
59/69: linear_discrim_df.info()
59/70:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/71:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=2)
59/72:
# Fit and transform model to standardized data
lda_components = lda.fit_transform(standard_x, target_y)  
lda_components
59/73:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2'])
linear_discrim_df.head()
59/74:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
59/75:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=4)

# Fit to standardized data
principal_components = pca.fit_transform(standard_x, target_y)
59/76:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2', 
                                     'principal_component_3', 'principal_component_4'])
principal_df.head()
59/77: principal_df.info()
59/78:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
59/79:
# Plot graph
sns.pairplot(principal_df)
59/80:
# Import necessary modules
import matplotlib.pyplot as plt
import numpy as np
59/81:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
59/82:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(4)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
59/83:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/84:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=2)
59/85:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=4)
59/86:
# Fit and transform model to standardized data
lda_components = lda.fit_transform(standard_x, target_y)  
lda_components
59/87:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2',
                                    'linear_discriminant_3', 'linear_discriminant_4'])
linear_discrim_df.head()
59/88:
# Plot graph
sns.pairplot(principal_df)
59/89:
# Plot graph
sns.pairplot(linear_discrim_df)
59/90:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
59/91:
train_data, test_data = train_test_split(linear_discrim_df, test_size=0.20, random_state=0)
train_data.head(5)
59/92:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
59/93:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
59/94:
# Separate out the features
feature_X = beats2.copy().drop(columns='Genres')

# Separate out the target
target_y = beats2['Genres'].values

# Standardizing the features
standard_X = StandardScaler().fit_transform(feature_X)
59/95:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Standardizing JUST the features
X = StandardScaler().fit_transform(X)

# Separate out the target
y = beats2['Genres'].values
59/96:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(X, y)
59/97:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
59/98:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
59/99:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(X, y)
59/100:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
59/101: principal_df.info()
59/102:
# Import necessary modules
import seaborn as sns; sns.set(style="ticks", color_codes=True)
59/103:
# Plot graph
sns.pairplot(principal_df)
59/104:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
59/105:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
59/106:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
59/107:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/108:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=2)
59/109:
# Fit and transform model to standardized data
lda_components = lda.fit_transform(standard_x, target_y)  
lda_components
59/110:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2'])
linear_discrim_df.head()
59/111:
# Plot graph
sns.pairplot(linear_discrim_df)
59/112:
# Import necessary modules 
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
59/113:
pca_df = pd.concat([principal_df, beats2[['Genre']]], axis = 1)
pca_df.head()
59/114:
pca_df = pd.concat([principal_df, beats2[['Genres']]], axis = 1)
pca_df.head()
59/115: pca_df.info()
59/116:
# Plot graph
sns.pairplot(pca_df)
59/117:
# Plot graph
sns.pairplot(pca_df[['principal_component_1', 'principal_component_2']])
59/118:
# Import necessary modules
import matplotlib.pyplot as plt
import numpy as np
59/119:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
59/120:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
59/121:
# Import necessary modules
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
59/122:
# Create LDA instance
lda = LinearDiscriminantAnalysis(n_components=2)
59/123:
# Fit and transform model to standardized data
lda_components = lda.fit_transform(standard_x, target_y)  
lda_components
59/124:
# Create a dataframe for the lda components
linear_discrim_df = pd.DataFrame(data=lda_components, 
                            columns=['linear_discriminant_1', 'linear_discriminant_2'])
linear_discrim_df.head()
59/125:
lda_df = pd.concat([linear_discrim_df, beats2[['Genres']]], axis = 1)
lda_df.head()
59/126:
# Plot graph
sns.pairplot(lda_df)
59/127:
# Plot graph
sns.pairplot(lda_df[['linear_discriminant_1', 'linear_discriminant_2']])
59/128: np.unique(beats2['Genres'])
59/129: genre_strings
59/130: np.unique(genre_strings)
59/131:
target_array = np.unique(genre_strings)
target_array
59/132:
target_num_array = np.unique(beats2['Genres'])
target_num_array
59/133:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Linear Discriminant 1', fontsize = 15)
ax.set_ylabel('Linear Discriminant 2', fontsize = 15)
ax.set_title('2 Linear Discriminant Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'o', 'y', 'g', 'b', 
          'p', 'b', 'c', 'm', 'l',
          'a', 'd', 'e', 'f', 'h', 
          'i', 'j', 'k', 'n', 'q']

for target, color in zip(targets,colors):
    indices = lda_df[''] == target
    ax.scatter(lda_df.loc[indices, 'linear_discriminant_1'], 
               lda_df.loc[indices, 'linear_discriminant_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/134:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Linear Discriminant 1', fontsize = 15)
ax.set_ylabel('Linear Discriminant 2', fontsize = 15)
ax.set_title('2 Linear Discriminant Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'o', 'y', 'g', 'b', 
          'p', 'b', 'c', 'm', 'l',
          'a', 'd', 'e', 'f', 'h', 
          'i', 'j', 'k', 'n', 'q']

for target, color in zip(targets,colors):
    indices = lda_df['Genres'] == target
    ax.scatter(lda_df.loc[indices, 'linear_discriminant_1'], 
               lda_df.loc[indices, 'linear_discriminant_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/135:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Linear Discriminant 1', fontsize = 15)
ax.set_ylabel('Linear Discriminant 2', fontsize = 15)
ax.set_title('2 Linear Discriminant Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'k', 'y', 'g', 'b', 
          'p', 'b', 'c', 'm', 'l',
          'a', 'd', 'e', 'f', 'h', 
          'i', 'j', 'k', 'n', 'q']

for target, color in zip(targets,colors):
    indices = lda_df['Genres'] == target
    ax.scatter(lda_df.loc[indices, 'linear_discriminant_1'], 
               lda_df.loc[indices, 'linear_discriminant_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/136:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Linear Discriminant 1', fontsize = 15)
ax.set_ylabel('Linear Discriminant 2', fontsize = 15)
ax.set_title('2 Linear Discriminant Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'k', 'y', 'g', 'b', 
          'navy', 'b', 'c', 'm', 'l',
          'steelblue', 'slategrey', 'indigo', 'lightseagreen', 'darkred', 
          'orange', 'gold', 'darkgreen', 'teal', 'deeppink']

for target, color in zip(targets,colors):
    indices = lda_df['Genres'] == target
    ax.scatter(lda_df.loc[indices, 'linear_discriminant_1'], 
               lda_df.loc[indices, 'linear_discriminant_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/137:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Linear Discriminant 1', fontsize = 15)
ax.set_ylabel('Linear Discriminant 2', fontsize = 15)
ax.set_title('2 Linear Discriminant Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'k', 'y', 'g', 'b', 
          'navy', 'b', 'c', 'm', 'grey',
          'steelblue', 'slategrey', 'indigo', 'lightseagreen', 'darkred', 
          'orange', 'gold', 'darkgreen', 'teal', 'deeppink']

for target, color in zip(targets,colors):
    indices = lda_df['Genres'] == target
    ax.scatter(lda_df.loc[indices, 'linear_discriminant_1'], 
               lda_df.loc[indices, 'linear_discriminant_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/138:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principle Component 1', fontsize = 15)
ax.set_ylabel('Principle Component 2', fontsize = 15)
ax.set_title('2 Principle Component Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'k', 'y', 'g', 'b', 
          'navy', 'b', 'c', 'm', 'grey',
          'steelblue', 'slategrey', 'indigo', 'lightseagreen', 'darkred', 
          'orange', 'gold', 'darkgreen', 'teal', 'deeppink']

for target, color in zip(targets,colors):
    indices = pca_df['Genres'] == target
    ax.scatter(pca_df.loc[indices, 'principal_component_1'], 
               pca_df.loc[indices, 'principal_component_1'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/139:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principle Component 1', fontsize = 15)
ax.set_ylabel('Principle Component 2', fontsize = 15)
ax.set_title('2 Principle Component Analysis', fontsize = 20)
targets = target_num_array
colors = ['r', 'k', 'y', 'g', 'b', 
          'navy', 'b', 'c', 'm', 'grey',
          'steelblue', 'slategrey', 'indigo', 'lightseagreen', 'darkred', 
          'orange', 'gold', 'darkgreen', 'teal', 'deeppink']

for target, color in zip(targets,colors):
    indices = pca_df['Genres'] == target
    ax.scatter(pca_df.loc[indices, 'principal_component_1'], 
               pca_df.loc[indices, 'principal_component_2'],
               c = color,
               s = 50)
ax.legend(target_array)
ax.grid()
59/140:
# Define target and feature data
target = beats2['Genres']
features = beats2.drop(columns='Genres')
59/141:
train_data, test_data = train_test_split(target, features, test_size=0.20, random_state=0)
train_data.head(5)
59/142:
# Define target (X) and feature (y) data
X = beats2['Genres']
y = beats2.drop(columns='Genres')
59/143:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/144:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
59/145:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/146:
# Import necessary modules
from sklearn.tree import DecisionTreeClassifier
59/147:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
59/148:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train).reshape(-1, 1)
59/149:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train.reshape(-1, 1), y_train)
59/150: X_train, X_test, y_train, y_test
59/151: X_train, X_test
59/152: X_train.reshape(-1,1), X_test
59/153:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
59/154:
# Define target (X) and feature (y) data
X = beats2['Genres'].reshape(-1,1)
y = beats2.drop(columns='Genres')
59/155:
# Define target (X) and feature (y) data
X = beats2['Genres'].values
y = beats2.drop(columns='Genres')
59/156:
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/157:
X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/158:
# Define target (X) and feature (y) data
X = beats2['Genres'].values
y = beats2.drop(columns='Genres').values
59/159:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/160:
# Reshape X_train 
X_train.reshape(-1,1), X_test
59/161:
# Reshape X_train 
X_train = X_train.reshape(-1,1) 
X_test = X_test.reshape(-1,1) 
X_train, X_test
59/162:
# Create combined variables
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
59/163:
# Define target (X) and feature (y) data
X = beats2['Genres'].values
y = beats2.drop(columns='Genres').values
59/164: X
59/165: y
59/166:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
59/167:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Standardizing JUST the features
X = StandardScaler().fit_transform(X)

# Separate out the target
y = beats2['Genres'].values
59/168:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/169:
# Create combined variables for plotting
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
59/170:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train, y_train)
59/171:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
59/172:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/173:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
59/174:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
59/175:
# Create a scaler instance
scaler = StandardScaler()
59/176:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
59/177:
# Make an instance of the Model
ml_pca = PCA(.95)
59/178:
# Make an instance of the Model
ml_pca = PCA(.95)

# Fit model to *training* data
pca.fit(X_train)
59/179:
# Make an instance of the Model
ml_pca = PCA(.95)

# Fit model to *training* data
ml_pca.fit(X_train)
59/180:
# Determine number of components in model
ml_pca.n_components_
59/181:
# Map (transform) to both the training set and the test set
X_train = ml_pca.transform(X_train)
X_test = pca.transform(X_test)
59/182:
# Map (transform) to both the training set and the test set
X_train = ml_pca.transform(X_train)
X_test = ml_pca.transform(X_test)
59/183:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
59/184:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/185:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
59/186:
# Map (transform) to both the training set and the test set
X_train = ml_pca.transform(X_train)
X_test = ml_pca.transform(X_test)
59/187:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
59/188:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='auto')
59/189:
# Fit model to data
logisticRegr.fit(X_train, y_train)
59/190:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(multi_class='auto')
59/191:
# Fit model to data
logisticRegr.fit(X_train, y_train)
59/192:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='auto')
59/193:
# Fit model to data
logisticRegr.fit(X_train, y_train)
59/194:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticRegr = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)
59/195:
# Fit model to data
logisticRegr.fit(X_train, y_train)
59/196:
# Predict for One Observation
logisticRegr.predict(X_test[0].reshape(1,-1))
59/197:
# Predict for One Observation
print(logisticRegr.predict(X_test[0].reshape(1,-1)))

# Predict for One Observation
logisticRegr.predict(X_test[0:10])
59/198:
# Get rough estimate of model accuracy score
logisticRegr.score(X_test, y_test)
59/199:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)
59/200:
# Fit model to data
log_reg.fit(X_train, y_train)
59/201:
# Plot decision regions
plot_decision_regions(X_train, y_train, classifier=lr)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/202:
# Plot decision regions
plot_decision_regions(X_train, y_train, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/203:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/204:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
59/205:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
59/206:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/207:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
59/208:
# Make an instance of the Model
ml_pca = PCA(.95)

# Fit model to *training* data
ml_pca.fit(X_train_scaled)
59/209:
# Determine number of components in model
ml_pca.n_components_
59/210:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/211:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
59/212:
# Plot decision regions
plot_decision_regions(X_train_scal_pca, y_train, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/213:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/214:
# Import necessary modules
from matplotlib.colors import ListedColormap
59/215:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/216:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Standardizing JUST the features
X = StandardScaler().fit_transform(X)

# Separate out the target
y = beats2['Genres'].values
59/217:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
59/218:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
59/219:
# Import necessary modules 
from sklearn.model_selection import train_test_split
59/220:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
59/221:
# Create a scaler instance
scaler = StandardScaler()
59/222:
# Fit on training target set *only*
scaler.fit(X_train)

# Apply transform to both the training target set and the test target set
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
59/223:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
59/224:
# Make an instance of the Model
ml_pca = PCA(.95)

# Fit model to *training* data
ml_pca.fit(X_train_scaled)
59/225:
# Make an instance of the Model
ml_pca = PCA(.95)
59/226:
# Determine number of components in model
ml_pca.n_components_
59/227:
# Make an instance of the Model
ml_pca = PCA(components=2)
59/228:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
59/229:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/230:
# Determine number of components in model
ml_pca.n_components_
59/231:
# Make an instance of the Model
ml_pca = PCA(.95)
59/232:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/233:
# Determine number of components in model
ml_pca.n_components_
59/234:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
59/235:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)
59/236:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
59/237:
# Import necessary modules
from matplotlib.colors import ListedColormap
59/238:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/239:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/240:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
59/241:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/242:
# Determine number of components in model
ml_pca.n_components_
59/243:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
59/244:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)
59/245:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
59/246:
# Import necessary modules
from matplotlib.colors import ListedColormap
59/247:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/248:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/249:
# Predict for One Observation
print(logisticRegr.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
logisticRegr.predict(X_test_scal_pca[0:10])
59/250:
# Get rough estimate of model accuracy score
logisticRegr.score(X_test, y_test)
59/251:
# Get rough estimate of model accuracy score
logisticRegr.score(X_test_scal_pca, y_test)
59/252:
# Get rough estimate of model accuracy score
logisticRegr.score(X_test_scaled, y_test)
59/253:
# Predict for One Observation
print(log_reg.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
log_reg.predict(X_test_scal_pca[0:10])
59/254:
# Get rough estimate of model accuracy score
log_reg.score(X_test_scal_pca, y_test)
59/255:
# Make an instance of the Model
ml_pca = PCA(0.95)
59/256:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/257:
# Determine number of components in model
ml_pca.n_components_
59/258:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
59/259:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/260:
# Predict for One Observation
print(log_reg.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
log_reg.predict(X_test_scal_pca[0:10])
59/261:
# Get rough estimate of model accuracy score
log_reg.score(X_test_scal_pca, y_test)
59/262:
# Plot decision regions
plot_decision_regions(X_test_scal_pca[:,2], y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/263:
# Plot decision regions
plot_decision_regions(X_test_scal_pca[2], y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/264:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x3_min, x3_max = X[:, 2].min() - 1, X[:, 2].max() + 1
    x4_min, x4_max = X[:, 3].min() - 1, X[:, 3].max() + 1
    x5_min, x5_max = X[:, 4].min() - 1, X[:, 4].max() + 1
    x6_min, x6_max = X[:, 5].min() - 1, X[:, 5].max() + 1
    x7_min, x7_max = X[:, 6].min() - 1, X[:, 6].max() + 1
    x8_min, x8_max = X[:, 7].min() - 1, X[:, 7].max() + 1
    x9_min, x9_max = X[:, 8].min() - 1, X[:, 8].max() + 1
    x10_min, x10_max = X[:, 9].min() - 1, X[:, 9].max() + 1
    x11_min, x11_max = X[:, 10].min() - 1, X[:, 10].max() + 1
    
    xx1, xx2, xx3, xx4,xx5, xx6, xx7, xx8, xx9, xx10, xx11 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution),
                            np.arange(x3_min, x3_max, resolution),
                            np.arange(x4_min, x4_max, resolution),
                            np.arange(x5_min, x5_max, resolution),
                            np.arange(x6_min, x6_max, resolution),
                            np.arange(x7_min, x7_max, resolution),
                            np.arange(x8_min, x8_max, resolution),
                            np.arange(x9_min, x9_max, resolution),
                            np.arange(x10_min, x10_max, resolution),
                            np.arange(x11_min, x11_max, resolution))
    
    for NUM in range(1,10):
        Z = classifier.predict(np.array([xxNUM.ravel(), xx(NUM+1).ravel()]).T)
        Z = Z.reshape(xxNUM.shape)
        plt.contourf(xxNUM, xx(NUM+1), Z, alpha=0.3, cmap=cmap)
        plt.xlim(xxNUM.min(), xxNUM.max())
        plt.ylim(xx(NUM+1).min(), xx(NUM+1).max())

        # plot class samples
        for idx, cl in enumerate(np.unique(y)):
            x=X[y == cl, 0]
            y=X[y == cl, 1]
            return x.size, y.size
            plt.scatter(x,y,
                        alpha=0.6,
                        c=colors[idx],
                        marker=markers[idx],
                        label=c1,
                        edgecolor='black')
59/265:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/266:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/267:
# Creates four axes, and accesses them through the returned array
fig, [[ax1, ax2, ax3, ax4, ax5, ax6], [ax7, ax8, ax9, ax10, ax11]] = plt.subplots(6, 5)
59/268:
# Creates four axes, and accesses them through the returned array
fig, [[ax1, ax2, ax3], [ax4, ax5, ax6, ax7]] = plt.subplots(3, 4)
59/269:
# Creates four axes, and accesses them through the returned array
fig, [[ax1, ax2, ax3, ax4], [ax5, ax6, ax7]] = plt.subplots(4, 3)
59/270:
for component in X_test_scal_pca:
    print(component)
59/271:
for component in range(0, len(X_test_scal_pca)):
    print(component)
59/272:
for component in range(0, len(X_test_scal_pca.columns)):
    print(component)
59/273: X_test_scal_pca
59/274:
for each in X_test_scal_pca:
    print(1)
59/275: for each in X_test_scal_pca:
59/276:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
59/277:
# Plot decision regions

plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)

plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/278:  X_test_scal_pca.shape[1]
59/279:
for component in range(0, X_test_scal_pca.shape[1]):
    print(component)
59/280:
for component in X_test_scal_pca.shape[1]:
    print(component)
59/281:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
59/282:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
59/283:
# Determine number of components in model
ml_pca.n_components_
59/284:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
59/285:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)
59/286:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
59/287:
# Import necessary modules
from matplotlib.colors import ListedColormap
59/288:
# Plot decision regions
plot_decision_regions(X_test_scal_pca, y_test, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/289:
# Predict for One Observation
print(log_reg.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
log_reg.predict(X_test_scal_pca[0:10])
59/290:
# Get rough estimate of model accuracy score
log_reg.score(X_test_scal_pca, y_test)
59/291:
# Create combined variables for plotting
X_combined = np.vstack((X_train_scal_pca, X_test_scal_pca))
y_combined = np.hstack((y_train, y_test))
59/292:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=log_reg)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Logistic Regression of Music Genres based on PCA of Audio Features')
59/293:
# Predict for One Observation
print(log_reg.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
log_reg.predict(X_test_scal_pca[0:10])
59/294:
# Get rough estimate of model accuracy score
log_reg.score(X_test_scal_pca, y_test)
59/295:
# Create a tree instance
tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)

# Fit data to tree
tree.fit(X_train_scal_pca, y_train)
59/296:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=tree)
plt.ylabel('PCA Audio Components')
plt.xlabel('Principle Components of Music Audio Feature Data')
plt.title('Decision Tree Classifier into Genres based on PCA Music Audio Features')
59/297:
# Predict for One Observation
print(tree.predict(X_test_scal_pca[0].reshape(1,-1)))

# Predict for One Observation
tree.predict(X_test_scal_pca[0:10])
59/298:
# Get rough estimate of model accuracy score
tree.score(X_test_scal_pca, y_test)
59/299:
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import AgglomerativeClustering
59/300:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=2)

# Fit data to KNN
knn.fit(X_train, y_train)
59/301:
# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=2)

# Fit data to KNN
knn.fit(X_train_scal_pca, y_train)
59/302:
# Plot decision regions
plot_decision_regions(X_combined, y_combined, classifier=knn)
plt.ylabel('PCA Audio Components')
plt.xlabel('Music Genre')
plt.title('KNN Classifier into Genres based on PCA Music Audio Features')
59/303:
# Predict for One Observation 
logisticRegr.predict(X_test_scal_pca[0].reshape(1,-1))
59/304:
# Predict for One Observation 
log_reg.predict(X_test_scal_pca[0].reshape(1,-1))
59/305:
# Import necessary modules
from sklearn.metrics import roc_curve
59/306:
# Get rough estimate of model accuracy score
score = tree.score(X_test_scal_pca, y_test)
score
59/307:
# Predict
tree.predict(X_test_scal_pca)
59/308:
# Predict
prediction = tree.predict(X_test_scal_pca)
prediction
59/309:
# Get rough estimate of model accuracy score
score = tree.score(X_test_scal_pca)
score
59/310:
# Get rough estimate of model accuracy score
score = tree.score(X_test_scal_pca, y_test)
score
59/311:
# Get rough estimate of model accuracy score
score = tree.score(X_train_scal_pca, y_train)
score
59/312:
# Get rough estimate of model accuracy score
train_score = tree.score(X_train_scal_pca, y_train)
train_score
59/313:
# Get rough estimate of model accuracy score
train_score = tree.score(X_train_scal_pca, y_train)
test_score = tree.score(X_test_scal_pca, y_test)
train_score, test_score
64/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
64/2:
# Load json dataset
login_data = pd.read_json('logins.json')

# Define Data Type
type(login_data)
64/3:
# Load json dataset
login_data = pd.read_json('logins.json')
login_data.head()
64/4:
# Assess the dataframe
login_data.info()
64/5: login_data.describe()
64/6:
login_data_df.set_index('timestamp', inplace=True)
login_data_df.head()
64/7:
login_data.set_index('timestamp', inplace=True)
login_data.head()
64/8:
login_data.set_index('login_time', inplace=True)
login_data.head()
64/9:
login_data['counts'] = 1
login_data.head()
64/10:
login_data = login_data.resample(rule='15T').sum()
login_data.head()
64/11:
login_data = login_data.reset_index()
login_data.head()
64/12:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
64/13:
# Basic Visualization of Data
ax = login_data.plot(x='timestamp', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("Recorded Timestamps")
ax.set_ylabel("Frequency of User Logins")
plt.show()
64/14:
# Basic Visualization of Data
ax = login_data.plot(x='login_time', y='counts', color='g', kind='hist',
                       title="Number of 15-min Interval Timestamps Between Jan and April of 1970 within one Geographic Location")
ax.set_xlabel("Recorded Timestamps")
ax.set_ylabel("Frequency of User Logins")
plt.show()
64/15:
# Make subdfs for each month
jan = login_data[(login_data.login_time > '1970-01') & (login_data.login_time < '1970-02')]
feb = login_data[(login_data.login_time > '1970-02') & (login_data.login_time < '1970-03')]
mar = login_data[(login_data.login_time > '1970-03') & (login_data.login_time < '1970-04')]
apr = login_data[(login_data.login_time > '1970-04') & (login_data.login_time < '1970-05')]
64/16: login_data.date(month='01')
64/17: login_data.login_time.date(month='01')
64/18:
for date in login_data.login_time:
    print(date)
64/19:
for date in login_data.login_time:
    
    month = date.date(month='01')
    print(month)
64/20:
# Make subdfs for each month - select month by range 
jan = login_data[(login_data.login_time > '1970-01') & (login_data.login_time < '1970-02')]
feb = login_data[(login_data.login_time > '1970-02') & (login_data.login_time < '1970-03')]
mar = login_data[(login_data.login_time > '1970-03') & (login_data.login_time < '1970-04')]
apr = login_data[(login_data.login_time > '1970-04') & (login_data.login_time < '1970-05')]
64/21:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Trend of User Login in Geographic Region Between Jan and April of 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
login_data_df.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
login_data_df.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
login_data_df.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/22:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('User Login Trends by Month, Jan-April, in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x='month', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
feb.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
mar.plot(x='hour', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)
apr.plot(x='hour', y='counts', color='r', kind='scatter', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/23:
# Test output
jan
64/24:
# Test output
jan.head(1)
64/25:
# Test output
jan.head(1), feb.head(1), mar.head(1), apr.head(1)
64/26:
# Test output
jan.login_time.head(1), feb.login_time.head(1), mar.login_time.head(1), apr.login_time.head(1)
64/27:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.login_time.plot(x='day', y='counts', color='g', kind='scatter', ax=ax1, alpha=.2)
feb.login_time.plot(x='day', y='counts', color='b', kind='scatter', ax=ax2, alpha=.2)
mar.login_time.plot(x='day', y='counts', color='b', kind='scatter', ax=ax3, alpha=.2)
apr.login_time.plot(x='day', y='counts', color='r', kind='scatter', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/28:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x=jan['login_time'], y=jan['counts'], color='g', kind='scatter', ax=ax1, alpha=.2)
feb.plot(x=feb['login_time'], y=feb['counts'], color='b', kind='scatter', ax=ax2, alpha=.2)
mar.plot(x=mar['login_time'], y=mar['counts'], color='b', kind='scatter', ax=ax3, alpha=.2)
apr.plot(x=apr['login_time'], y=apr['counts'], color='r', kind='scatter', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/29:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x=jan['login_time'], y=jan['counts'], color='g', ax=ax1, alpha=.2)
feb.plot(x=feb['login_time'], y=feb['counts'], color='b', ax=ax2, alpha=.2)
mar.plot(x=mar['login_time'], y=mar['counts'], color='b', ax=ax3, alpha=.2)
apr.plot(x=apr['login_time'], y=apr['counts'], color='r', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/30:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x=jan['login_time'], y=jan['counts'], color='g', ax=ax1, alpha=.2)
feb.plot(x=feb['login_time'], y=feb['counts'], color='b', ax=ax2, alpha=.2)
mar.plot(x=mar['login_time'], y=mar['counts'], color='b', ax=ax3, alpha=.2)
apr.plot(x=apr['login_time'], y=apr['counts'], color='r', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/31:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2)
feb.plot(x=feb['login_time'], y=feb['counts'], color='b', ax=ax2, alpha=.2)
mar.plot(x=mar['login_time'], y=mar['counts'], color='b', ax=ax3, alpha=.2)
apr.plot(x=apr['login_time'], y=apr['counts'], color='r', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/32:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/33:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(200, 100))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
ax.set_xlabel('common xlabel')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/34:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(100, 100))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
fig.xticks(rotation=45)

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax2.set_ylabel('Number User Logins')
ax3.set_ylabel('Number User Logins')
ax4.set_ylabel('Number User Logins')

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/35:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(100, 100))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')
plt.setp(axa.xaxis.get_majorticklabels(), rotation=45)

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/36:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(100, 100))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 6), dpi=200)

plt.show()
64/37:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/38:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/39:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
#figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/40:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.2, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
#plt.subplots_adjust(wspace=0.8)
#figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/41:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.2, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.2, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.2, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
#plt.subplots_adjust(wspace=0.8)
#figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/42:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
#plt.subplots_adjust(wspace=0.8)
#figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/43:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

# Adjust layout of subplots
plt.subplots_adjust(wspace=0.8)
figure(num=None, figsize=(2, 2), dpi=200)

plt.show()
64/44:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970')
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970')
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970')
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970')

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

set('FontSize',12)

plt.show()
64/45:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(50, 50), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

plt.show()
64/46:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

plt.show()
64/47:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
64/48:
# Load json dataset
login_data = pd.read_json('logins.json')
login_data.head()
64/49:
# Assess the dataframe
login_data.info()
64/50: login_data.describe()
64/51:
login_data.set_index('login_time', inplace=True)
login_data.head()
64/52:
login_data['counts'] = 1
login_data.head()
64/53:
login_data_15T = login_data.resample(rule='15T').sum()
login_data_15T.head()
64/54:
login_data_15T = login_data_15T.reset_index()
login_data_15T.head()
64/55:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
64/56:
# Test output
jan.head(1), feb.head(1), mar.head(1), apr.head(1)
64/57:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set axis labels
ax1.set_ylabel('Number User Logins')
ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
ax4.tick_params(labelrotation=45)

plt.show()
64/58:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set axis labels
ax1.set_ylabel('Number User Logins')
#ax1.tick_params(labelrotation=45)
ax2.set_ylabel('Number User Logins')
#ax2.tick_params(labelrotation=45)
ax3.set_ylabel('Number User Logins')
#ax3.tick_params(labelrotation=45)
ax4.set_ylabel('Number User Logins')
#ax4.tick_params(labelrotation=45)

plt.show()
64/59:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)


# Set common labels
fig.text(0.5, 0.04, 'common xlabel', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/60:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)


# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/61:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set X lables
ax1.set_xticklabels([2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], fontsize=12)
ax2.set_xticklabels([2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], fontsize=12)
ax3.set_xticklabels([2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], fontsize=12)
ax4.set_xticklabels([2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/62:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, label='Jan 1970', fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, label='Feb 1970', fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, label='Mar 1970', fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, label='Apr 1970', fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/63:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=200)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/64:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/65:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10))

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/66:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

plt.show()
64/67:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
plt.xticks(np.arange(0, 32, step=2))

plt.show()
64/68:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
fig.xticks(np.arange(0, 32, step=2))

plt.show()
64/69:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
fig.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/70:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/71:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
jan.plot(x='login_time', y='counts', color='g', ax=ax1, alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/72:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(x=jan['login_time'], y=['counts'], color='g', alpha=.8, fontsize=12)
feb.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/73:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/74:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
#ax2.plot(x='login_time', y='counts', color='b', ax=ax2, alpha=.8, fontsize=12)
mar.plot(x='login_time', y='counts', color='b', ax=ax3, alpha=.8, fontsize=12)
apr.plot(x='login_time', y='counts', color='r', ax=ax4, alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/75:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
apr.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/76:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/77:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Month, Day, and Hour subplots
ax1.plot(data=jan, x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/78:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
#fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
#fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/79:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/80:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(x=['login_time'], y=jan['counts'], ax=ax1, color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/81:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(x='login_time', y='counts', ax=ax1, color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/82:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(x=jan['login_time'], y=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/83:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(jan))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
64/84:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(jan))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
print('Length of March Data is ', len(mar))
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
print('Length of April Data is ', len(apr))
64/85:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(np.unique(jan.login_time)))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
print('Length of March Data is ', len(mar))
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
print('Length of April Data is ', len(apr))
64/86:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(np.unique(jan.login_time.days)))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
print('Length of March Data is ', len(mar))
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
print('Length of April Data is ', len(apr))
64/87:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(jan.login))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
print('Length of March Data is ', len(mar))
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
print('Length of April Data is ', len(apr))
64/88:
# Make subdfs for each month - select month by range 
jan = login_data_15T[(login_data_15T.login_time > '1970-01') & (login_data_15T.login_time < '1970-02')]
print('Length of January Data is ', len(jan.login_time))
feb = login_data_15T[(login_data_15T.login_time > '1970-02') & (login_data_15T.login_time < '1970-03')]
print('Length of February Data is ', len(feb))
mar = login_data_15T[(login_data_15T.login_time > '1970-03') & (login_data_15T.login_time < '1970-04')]
print('Length of March Data is ', len(mar))
apr = login_data_15T[(login_data_15T.login_time > '1970-04') & (login_data_15T.login_time < '1970-05')]
print('Length of April Data is ', len(apr))
64/89:
# Test April output
apr.login_time.max
64/90:
# Test April output
apr.login_time.tail(1)
64/91:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(xdata=jan['login_time'], ydata=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(x=feb['login_time'], y=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(x=mar['login_time'], y=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(x=apr['login_time'], y=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/92:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(xdata=jan['login_time'], ydata=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(xdata=feb['login_time'], ydata=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(xdata=mar['login_time'], ydata=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(xdata=apr['login_time'], ydata=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/93:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(xdata=jan['login_time'], ydata=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(xdata=feb['login_time'], ydata=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(xdata=mar['login_time'], ydata=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(xdata=apr['login_time'], ydata=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.4, 'User Login Time', ha='center', va='center')
fig.text(0.6, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/94:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(xdata=jan['login_time'], ydata=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(xdata=feb['login_time'], ydata=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(xdata=mar['login_time'], ydata=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(xdata=apr['login_time'], ydata=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
ax1.set_xticks(np.arange(0, 32, step=2))
ax2.set_xticks(np.arange(0, 32, step=2))
ax3.set_xticks(np.arange(0, 32, step=2))
ax4.set_xticks(np.arange(0, 32, step=2))

plt.show()
64/95:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
ax1.plot(xdata=jan['login_time'], ydata=jan['counts'], color='g', alpha=.8, fontsize=12)
ax2.plot(xdata=feb['login_time'], ydata=feb['counts'], color='b', alpha=.8, fontsize=12)
ax3.plot(xdata=mar['login_time'], ydata=mar['counts'], color='b', alpha=.8, fontsize=12)
ax4.plot(xdata=apr['login_time'], ydata=apr['counts'], color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/96:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, xdata='login_time', ydata='counts', color='g', alpha=.8, fontsize=12)
feb.plot(ax=ax2, xdata='login_time', ydata='counts', color='b', alpha=.8, fontsize=12)
mar.plot(ax=ax3, xdata='login_time', ydata='counts', color='b', alpha=.8, fontsize=12)
apr.plot(ax=ax4, xdata='login_time', ydata='counts', color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/97:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, x='login_time', y='counts', color='g', alpha=.8, fontsize=12)
feb.plot(ax=ax2, x='login_time', y='counts', color='b', alpha=.8, fontsize=12)
mar.plot(ax=ax3, x='login_time', y='counts', color='b', alpha=.8, fontsize=12)
apr.plot(ax=ax4, x='login_time', y='counts', color='r', alpha=.8, fontsize=12)

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/98:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, x='login_time', y='counts', color='g', alpha=.8, fontsize=12, label='Jan 1970')
feb.plot(ax=ax2, x='login_time', y='counts', color='b', alpha=.8, fontsize=12, label='Feb 1970')
mar.plot(ax=ax3, x='login_time', y='counts', color='b', alpha=.8, fontsize=12, label='Mar 1970')
apr.plot(ax=ax4, x='login_time', y='counts', color='r', alpha=.8, fontsize=12, label='Apr 1970')

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/99:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=100)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, x='login_time', y='counts', color='g', alpha=.8, fontsize=12, label='Jan 1970')
feb.plot(ax=ax2, x='login_time', y='counts', color='b', alpha=.8, fontsize=12, label='Feb 1970')
mar.plot(ax=ax3, x='login_time', y='counts', color='b', alpha=.8, fontsize=12, label='Mar 1970')
apr.plot(ax=ax4, x='login_time', y='counts', color='r', alpha=.8, fontsize=12, label='Apr 1970')

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
#[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/100:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=50)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, x='login_time', y='counts', color='g', alpha=.8, fontsize=8, label='Jan 1970')
feb.plot(ax=ax2, x='login_time', y='counts', color='b', alpha=.8, fontsize=8, label='Feb 1970')
mar.plot(ax=ax3, x='login_time', y='counts', color='b', alpha=.8, fontsize=8, label='Mar 1970')
apr.plot(ax=ax4, x='login_time', y='counts', color='r', alpha=.8, fontsize=8, label='Apr 1970')

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
#[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/101:
# Visualization of Monthly Data
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10, 10), dpi=75)

# Set common labels
fig.suptitle('Jan-April Monthly User Login Trends by Day in 1970')

# Define Monthly subplots
jan.plot(ax=ax1, x='login_time', y='counts', color='g', alpha=.8, fontsize=8, label='Jan 1970')
feb.plot(ax=ax2, x='login_time', y='counts', color='b', alpha=.8, fontsize=8, label='Feb 1970')
mar.plot(ax=ax3, x='login_time', y='counts', color='b', alpha=.8, fontsize=8, label='Mar 1970')
apr.plot(ax=ax4, x='login_time', y='counts', color='r', alpha=.8, fontsize=8, label='Apr 1970')

# Set common labels
fig.text(0.5, 0.04, 'User Login Time', ha='center', va='center')
fig.text(0.06, 0.5, 'Number User Logins', ha='center', va='center', rotation='vertical')

# Set x tick frequency
#[ax.set_xticks(np.arange(0, 32, step=2)) for ax in [ax1, ax2, ax3, ax4]]

plt.show()
64/102:
# Counting based on weekdays, 0 = Monday, 6 = Sunday
login_data_15T['weekday'] = login_data_15T['login_time'].dt.dayofweek
wkday = login_data_15T.sort_values(by='counts', ascending=False)
wkday_counts = wkday[['counts', 'weekday']]
wkday_count = wkday_counts.groupby('weekday').sum().reset_index()
wkday_count
64/103:
# Counting based on weekdays, 0 = Monday, 6 = Sunday
login_data_15T['weekday'] = login_data_15T['login_time'].dt.dayofweek

# Create weekday subdf
weekday = login_data_15T.sort_values(by='counts', ascending=False)
weekday = weekday[['counts', 'weekday']]
weekday = weekday.groupby('weekday').sum().reset_index()

# Rename columns
weekday['weekday'] = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']
weekday
64/104:
# Counting based on weekdays, 0 = Monday, 6 = Sunday
login_data_15T['weekday'] = login_data_15T['login_time'].dt.dayofweek

# Create weekday subdf
weekday = login_data_15T.sort_values(by='counts', ascending=False)
weekday = weekday[['counts', 'weekday']]
weekday = weekday.groupby('weekday').sum().reset_index()

# Rename columns
weekday['day'] = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']
weekday
64/105:
# Counting based on weekdays, 0 = Monday, 6 = Sunday
login_data_15T['weekday'] = login_data_15T['login_time'].dt.dayofweek

# Create weekday subdf
weekday = login_data_15T.sort_values(by='counts', ascending=False)
weekday = weekday[['counts', 'weekday']]
weekday = weekday.groupby('weekday').sum().reset_index()

# Rename columns
weekday['day'] = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']
weekday
64/106:
# Plot weekday counts 
sns.countplot(x='day', data=weekday)
64/107:
# Plot weekday counts 
sns.countplot(x='day', data=weekday).set_title('User Login Trends by Day in 1970')
64/108:
# Plot weekday counts 
plt.plot(x=weekday['day'], y=weekday['counts'])
plt.title('User Login Trends by Day in 1970')
64/109:
# Counting based on weekdays, 0 = Monday, 6 = Sunday
login_data_15T['weekday'] = login_data_15T['login_time'].dt.dayofweek

# Create weekday subdf
weekday = login_data_15T.sort_values(by='counts', ascending=False)
weekday = weekday[['counts', 'weekday']]
weekday = weekday.groupby('weekday').sum().reset_index()

# Rename columns
weekday['day'] = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']
weekday
64/110:
# Plot weekday counts 
plt.plot(x=weekday['day'], y=weekday['counts'])
plt.title('User Login Trends by Day in 1970')
plt.show()
64/111:
# Plot weekday counts 
sns.countplot(x='day', data=weekday).set_title('User Login Trends by Day in 1970')
64/112:
# Plot weekday counts 
plt.plot(x=weekday['day'], y=weekday['counts'])
plt.title('User Login Trends by Day in 1970')
plt.show()
64/113:
# Plot weekday counts 
plt.plot(x=weekday['day'], y=weekday['counts'])
plt.title('User Login Trends by Day in 1970')
plt.show()
64/114:
# Plot weekday counts 
sns.barplot(x='day', y='count' data=weekday).set_title('User Login Trends by Day in 1970')
64/115:
# Plot weekday counts 
sns.barplot(x='day', y='count', data=weekday).set_title('User Login Trends by Day in 1970')
64/116:
# Plot weekday counts 
sns.barplot(x='day', y='count', data=weekday)
64/117:
# Plot weekday counts 
sns.barplot(x='day', y='counts', data=weekday)
64/118:
# Plot weekday counts 
sns.barplot(x='day', y='counts', data=weekday)
plt.ylabel('Average Count of User Logins')
plt.xlabel('Weekday')
plt.title('Count of Logins by Weekday from Jan to mid-Apr 1970')
64/119:
# Splitting into hours
login_data_15T['hour'] = login_data_15T['login_time'].dt.hour
hours = login_data_15T
hours = hours[['count', 'hour']]
hours = hours.groupby('hour').sum().reset_index()
hours.head()
64/120:
# Splitting into hours
login_data_15T['hour'] = login_data_15T['login_time'].dt.hour
hours = login_data_15T
hours = hours[['counts', 'hour']]
hours = hours.groupby('hour').sum().reset_index()
hours.head()
64/121:
# Splitting into hours
login_data_15T['hour'] = login_data_15T['login_time'].dt.hour
hours = login_data_15T
hours = hours[['counts', 'hour']]
hours = hours.groupby('hour').sum().reset_index()
hours
64/122:
# Splitting into hours
login_data_15T['hour'] = login_data_15T['login_time'].dt.hour
hours = login_data_15T
hours = hours[['counts', 'hour']]
hours = hours.groupby('hour').sum().reset_index()
#hours
64/123:
# Plot hour counts 
sns.barplot(x='hour', y='counts', data=hours)
plt.ylabel('Average Count of User Logins')
plt.xlabel('Hour')
plt.title('Count of Logins by Hour in Jan to mid-Apr 1970')
65/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
65/2:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
65/3:
# Load challenge json dataset
challenge_json_data = open('ultimate_data_challenge.json')
challenge_data = json.load(challenge_json_data)
# Define Data Type
type(challenge_data)
65/4:
# Assess Contents of Data
challenge_data[0]
65/5:
# Create a general function that:

#appends value to list
def add_to_list(li, val):
    return li.append(val)

#converts a list to series
def list_to_series(li):
    return pd.Series(li)
65/6:
# Grab key names for column
column_names = []

for each in challenge_data[0]:
    add_to_list(column_names, each)

# Test output
column_names
65/7:
# Convert the list of lists of key:value pairs to a dataframe
ultimate_df = pd.DataFrame(challenge_data, columns=column_names)
ultimate_df.head()
65/8:
# Assess Dataframe
ultimate_df.info()
65/9:
# Drop NaN
ultimate_df = ultimate_df.dropna()
ultimate_df.info()
65/10:
# Import necessary modules
from datetime import datetime
65/11:
# Convert date strings to datetime objects

# signup
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]

# last_trip
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
65/12:
# Assess Dataframe
ultimate_df.describe()
65/13:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
65/14:
# Total number of unique values within each column
print(ultimate_df.nunique())
65/15:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
challenge_data = json.load(file)
file.close()

# Define Data Type
type(challenge_data)
65/16:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
challenge_data = pd.DataFrame(json.load(file))
file.close()
challenge_data.head()
65/17:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
ultimate_df = pd.DataFrame(json.load(file))
file.close()
ultimate_df.head()
65/18:
# Assess Dataframe
ultimate_df.info()
65/19:
# Get description of each feature
ultimate_df.describe()
65/20:
# Impute in missing values
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
65/21:
# Impute in missing values
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
65/22:
# Test output of phone column
len(ultimate_df[ultimate_df['phone'] == 'iPhone']), len(ultimate_df[ultimate_df['phone'] == 'Android'])
65/23:
# Test output of phone column
iphone = len(ultimate_df[ultimate_df['phone'] == 'iPhone'])
andriod = len(ultimate_df[ultimate_df['phone'] == 'Android'])
total = iphone + andriod
percent_iphone = (iphone / total) * 100
percent_andriod = (android / total) * 100
percent_iphone, percent_andriod
65/24:
# Test output of phone column
iphone = len(ultimate_df[ultimate_df['phone'] == 'iPhone'])
android = len(ultimate_df[ultimate_df['phone'] == 'Android'])
total = iphone + android
percent_iphone = (iphone / total) * 100
percent_android = (android / total) * 100
percent_iphone, percent_android
65/25:
# Calculate percent of iPhone and Andriods in dataset
iphone = len(ultimate_df[ultimate_df['phone'] == 'iPhone'])
android = len(ultimate_df[ultimate_df['phone'] == 'Android'])
total = iphone + android
percent_iphone = (iphone / total) * 100
percent_android = (android / total) * 100
percent_iphone, percent_android
65/26:
# Fill NaN in phone column based on the percent of occurance
ultimate_df['phone'].mean()
65/27:
# Fill NaN in phone column based on the percent of occurance
ultimate_df['phone'].mode()
65/28:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)
65/29:
# Assess Dataframe
ultimate_df.info()
65/30:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)
65/31:
# Assess Dataframe
ultimate_df.info()
65/32: ultimate_df['phone'].mode()
65/33:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)
65/34:
# Assess Dataframe
ultimate_df.info()
65/35:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode())
65/36:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode())

# Assess Dataframe
ultimate_df.info()
65/37:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(value='iPhone')

# Assess Dataframe
ultimate_df.info()
65/38:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(value='iPhone', inplace=True)

# Assess Dataframe
ultimate_df.info()
65/39:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)

# Assess Dataframe
ultimate_df.info()
65/40:
# Assess Dataframe
ultimate_df.info()
65/41:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
65/42:
# Assess Dataframe
ultimate_df.describe()
65/43:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
65/44:
# Total number of unique values within each column
print(ultimate_df.nunique())
65/45:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
65/46:
# Create dataframe for city and trips
city_trips = (df[['trips_in_first_30_days', 'city', 'phone']].groupby('city').sum().reset_index())
city_trips
65/47:
# Create dataframe for city and trips
city_trips = (ultimate_df[['trips_in_first_30_days', 'city', 'phone']].groupby('city').sum().reset_index())
city_trips
65/48:
# Create dataframe for trips in first 30 days grouped by city
trips_by_city = (ultimate_df[['trips_in_first_30_days', 'city', 'phone']].groupby('city').sum().reset_index())
trips_by_city
65/49:
# Plot counts of trips in first 30 days by city 
sns.barplot(x='city', y='trips_in_first_30_days', data=trips_by_city)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('City')
plt.title('Trips in First 30 Days by City')
65/50:
# Create dataframe for trips in first 30 days grouped by city
trips_by_city = (ultimate_df[['trips_in_first_30_days', 'city']].groupby('city').sum().reset_index())
trips_by_city
65/51:
# Create dataframe for trips in first 30 days grouped by city
trips_by_phone = (ultimate_df[['trips_in_first_30_days', 'phone']].groupby('phone').sum().reset_index())
trips_by_phone
65/52:
# Plot counts of trips in first 30 days by city 
sns.barplot(x='phone', y='trips_in_first_30_days', data=trips_by_phone)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Phone')
plt.title('Trips in First 30 Days by City')
65/53:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
trips_by_usertype
65/54:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='phone', y='trips_in_first_30_days', data=trips_by_phone)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Phone')
plt.title('Trips in First 30 Days by Phone Type')
65/55:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel(['Regular', 'Ultimate Black'])
plt.title('Trips in First 30 Days by User Type')
65/56:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Regular', 'Ultimate Black')
plt.title('Trips in First 30 Days by User Type')
65/57:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel(['Regular', 'Ultimate Black'])
plt.title('Trips in First 30 Days by User Type')
65/58:
# User type
users = ['Regular', 'Ultimate Black']
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel(users)
plt.title('Trips in First 30 Days by User Type')
65/59:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
65/60:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
#trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
65/61:
# Relabel True/False 
trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
65/62:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Type of User')
plt.title('Trips in First 30 Days by User Type')
65/63:
# Create a datetime range for retained customers
retained_range = pd.date_range('2014-01-01', periods=6, freq='m')
retained_range
65/64:
# Import necessary modules
from pylab import rcParams
65/65:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if '2014-01-01' <= last_trip < '2014-07':
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
65/66:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if datetime.strptime('2014-01', '%Y-%m')  <= last_trip < datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
65/67:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if datetime.strptime('2014-01', '%Y-%m')  <= last_trip < datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
customer_retained, customer_lost, total_customers
65/68:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained
    if datetime.strptime('2014-01', '%Y-%m')  <= last_trip < datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
65/69:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
65/70:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
65/71:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
65/72:
# Create a datetime range for retained customers
retained_range = pd.date_range('2014-01-01', periods=30, freq='d')
retained_range
65/73:
# Determine last day in range
ultimate_df['last_trip_date'].max()
65/74:
# Create variables for customer_retained and customer_lost
total_customers = len(ultimate_df)
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
    else:
        add_to_list(customer_lost, 0)

# Test output
#customer_retained, customer_lost, total_customers
65/75:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
65/76:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
65/77:
labels = ['Retained', 'Lost']
colors = ['grey','purple'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
65/78:
labels = ['Retained', 'Lost']
colors = ['grey','light blue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
65/79:
labels = ['Retained', 'Lost']
colors = ['grey','babyblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
65/80:
labels = ['Retained', 'Lost']
colors = ['grey','lightblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.show()
65/81:
labels = ['Retained', 'Lost']
colors = ['grey','lightblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.savefig('customersretained.png', dpi=300)
plt.show()
65/82:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
        add_to_list(customers, 1)
    else:
        add_to_list(customer_lost, 0)
        add_to_list(customers, 0)

# Test output
#customer_retained, customer_lost, customers
65/83:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
        add_to_list(customers, True)
    else:
        add_to_list(customer_lost, 0)
        add_to_list(customers, False)

# Test output
#customer_retained, customer_lost, customers
65/84:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
        add_to_list(customers, True)
    else:
        add_to_list(customer_lost, 0)
        add_to_list(customers, False)

# Test output
customer_retained, customer_lost, customers
65/85:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
        add_to_list(customers, True)
    else:
        add_to_list(customer_lost, 0)
        add_to_list(customers, False)

# Test output
#customer_retained, customer_lost, customers
65/86:
# Add customers to ultimate_df 
ultimate_df['Retained'] = pd.Series(customers)
ultimate_df.tail()
65/87: ultimate_df.head()
65/88:
# Remove datetime objects
ultimate_df = ultimate_df.drop(columns=['signup_date','last_trip_date'])
ultimate_df.info()
65/89:
# Remove datetime objects
ultimate_df = ultimate_df.drop(columns=['signup_date','last_trip_date'])
65/90:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
ultimate_df = pd.DataFrame(json.load(file))
file.close()
ultimate_df.head()
65/91:
# Get description of each feature
ultimate_df.describe()
65/92:
# Assess Dataframe
ultimate_df.info()
65/93:
# Determine mode of string column
ultimate_df['phone'].mode()
65/94:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)

# Assess Dataframe
ultimate_df.info()
65/95:
# Import necessary modules
from datetime import datetime
65/96:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
65/97:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
65/98:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
66/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
66/2:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
ultimate_df = pd.DataFrame(json.load(file))
file.close()
ultimate_df.head()
66/3:
# Get description of each feature
ultimate_df.describe()
66/4:
# Assess Dataframe
ultimate_df.info()
66/5:
# Determine mode of string column
ultimate_df['phone'].mode()
66/6:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(ultimate_df['phone'].mode(), inplace=True)

# Assess Dataframe
ultimate_df.info()
66/7:
# Import necessary modules
from datetime import datetime
66/8:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
66/9:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
66/10:
# Total number of unique values within each column
print(ultimate_df.nunique())
66/11:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
66/12:
# Create dataframe for trips in first 30 days grouped by city
trips_by_city = (ultimate_df[['trips_in_first_30_days', 'city']].groupby('city').sum().reset_index())
trips_by_city
66/13:
# Plot counts of trips in first 30 days by city 
sns.barplot(x='city', y='trips_in_first_30_days', data=trips_by_city)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('City')
plt.title('Trips in First 30 Days by City')
66/14:
# Create dataframe for trips in first 30 days grouped by phone
trips_by_phone = (ultimate_df[['trips_in_first_30_days', 'phone']].groupby('phone').sum().reset_index())
trips_by_phone
66/15:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='phone', y='trips_in_first_30_days', data=trips_by_phone)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Phone')
plt.title('Trips in First 30 Days by Phone Type')
66/16:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
trips_by_usertype
66/17:
# Relabel True/False 
trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
66/18:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Type of User')
plt.title('Trips in First 30 Days by User Type')
66/19:
# Import necessary modules
from pylab import rcParams
66/20:
# Determine last day in range
ultimate_df['last_trip_date'].max()
66/21:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        add_to_list(customer_retained, 1)
        add_to_list(customers, True)
    else:
        add_to_list(customer_lost, 0)
        add_to_list(customers, False)

# Test output
#customer_retained, customer_lost, customers
66/22:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        customer_retained.append(1)
        customers.append(1)
    else:
        customer_lost.append(0)
        customers.append(0)

# Test output
#customer_retained, customer_lost, customers
66/23:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        customer_retained.append(1)
        customers.append(True)
    else:
        customer_lost.append(0)
        customers.append(False)

# Test output
#customer_retained, customer_lost, customers
66/24:
# Add customers to ultimate_df 
ultimate_df['Retained'] = pd.Series(customers)
ultimate_df.tail()
66/25: ultimate_df = ultimate_df.drop('Retained')
66/26: ultimate_df = ultimate_df.drop(columns='Retained')
66/27:
# Add customers to ultimate_df 
ultimate_df['retained_customer'] = pd.Series(customers)
ultimate_df.tail()
66/28:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
66/29:
# Calculate total retained and total lost
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
66/30:
# Calculate total retained and total lost
total_customers = len(ultimate_df['retained_customer'])
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
66/31:
labels = ['Retained', 'Lost']
colors = ['grey','lightblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.savefig('customersretained.png', dpi=300)
plt.show()
66/32: ultimate_df.head()
66/33:
# Remove datetime objects
ultimate_df = ultimate_df.drop(columns=['signup_date','last_trip_date'])
66/34: int(ultimate_df['ultimate_black_user'])
66/35:
# Verify output
ultimate_df.info()
66/36:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
66/37:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
ultimate_df = pd.DataFrame(json.load(file))
file.close()
ultimate_df.head()
66/38:
# Get description of each feature
ultimate_df.describe()
66/39:
# Assess Dataframe
ultimate_df.info()
66/40:
# Determine mode of string column
ultimate_df['phone'].mode()
66/41:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(value='iPhone', inplace=True)

# Assess Dataframe
ultimate_df.info()
66/42:
# Import necessary modules
from datetime import datetime
66/43:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
66/44:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
66/45:
# Total number of unique values within each column
print(ultimate_df.nunique())
66/46:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
66/47:
# Create dataframe for trips in first 30 days grouped by city
trips_by_city = (ultimate_df[['trips_in_first_30_days', 'city']].groupby('city').sum().reset_index())
trips_by_city
66/48:
# Plot counts of trips in first 30 days by city 
sns.barplot(x='city', y='trips_in_first_30_days', data=trips_by_city)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('City')
plt.title('Trips in First 30 Days by City')
66/49:
# Create dataframe for trips in first 30 days grouped by phone
trips_by_phone = (ultimate_df[['trips_in_first_30_days', 'phone']].groupby('phone').sum().reset_index())
trips_by_phone
66/50:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='phone', y='trips_in_first_30_days', data=trips_by_phone)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Phone')
plt.title('Trips in First 30 Days by Phone Type')
66/51:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
trips_by_usertype
66/52:
# Relabel True/False 
trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
66/53:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Type of User')
plt.title('Trips in First 30 Days by User Type')
66/54:
# Import necessary modules
from pylab import rcParams
66/55:
# Determine last day in range
ultimate_df['last_trip_date'].max()
66/56:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        customer_retained.append(1)
        customers.append(True)
    else:
        customer_lost.append(0)
        customers.append(False)

# Test output
#customer_retained, customer_lost, customers
66/57:
# Add customers to ultimate_df 
ultimate_df['retained_customer'] = pd.Series(customers)
ultimate_df.tail()
66/58:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
66/59:
# Calculate total retained and total lost
total_customers = len(ultimate_df['retained_customer'])
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
66/60:
labels = ['Retained', 'Lost']
colors = ['grey','lightblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.savefig('customersretained.png', dpi=300)
plt.show()
66/61: ultimate_df.head()
66/62:
# Remove datetime objects
ultimate_df = ultimate_df.drop(columns=['signup_date','last_trip_date'])
66/63:
# Verify output
ultimate_df.info()
66/64:
# Define model variables
y_target = ultimate_df['retained_customer']
X_features = ultimate_df.drop(columns='retained_customer')
66/65:
# One-hot-encode all categorical features
X_features_encoded = pd.get_dummies(X_features, columns = ['city', 'phone', 'ultimate_black_user'])
X_features_encoded.head()
66/66:
# One-hot-encode target variable boolean
y_target_encoded = pd.get_dummies(y_target, columns=['retained_customer'])
y_target_encoded.head()
66/67:
# Define model variables
target = ultimate_df['retained_customer']
features = ultimate_df.drop(columns='retained_customer')
66/68:
# One-hot-encode all categorical features
features_encoded = pd.get_dummies(X_features, columns=['city', 'phone', 'ultimate_black_user'])
features_encoded.head()
66/69:
# One-hot-encode target variable boolean
target_encoded = pd.get_dummies(y_target, columns=['retained_customer'])
target_encoded.head()
66/70:
# Define model variables
X_features_encoded = features_encoded.values
y_target_encoded = target_encoded.values
66/71:
# Split dataset into Training Testing
X_train, X_test, y_train, y_test = train_test_split(X_features_encoded, y_target_encoded, 
                                                    test_size=0.25, random_state=42)
66/72:
# Import necessary modules
from sklearn.model_selection import train_test_split
66/73:
# Split dataset into Training Testing
X_train, X_test, y_train, y_test = train_test_split(X_features_encoded, y_target_encoded, 
                                                    test_size=0.25, random_state=42)
66/74:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
66/75:
# Initiate a scaler instance
sc = StandardScaler()

# Fit scaler to model featurs
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
66/76:
# Import necessary modules
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
66/77:
# Import necessary modules
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
66/78:
# Create a class for 2-step Neural Network
class NeuralNetwork:
    def __init__(self, x, y):
        self.input = x
        self.weights1 = np.random.rand(self.input.shape[1],4) 
        self.weights2 = np.random.rand(4,1)                 
        self.y = y
        self.output = np.zeros(y.shape)
67/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
import json
67/2:
# Load challenge json dataset
file = open('ultimate_data_challenge.json')
ultimate_df = pd.DataFrame(json.load(file))
file.close()
ultimate_df.head()
67/3:
# Get description of each feature
ultimate_df.describe()
67/4:
# Assess Dataframe
ultimate_df.info()
67/5:
# Determine mode of string column
ultimate_df['phone'].mode()
67/6:
# Impute in missing values with mean / mode
ultimate_df['avg_rating_by_driver'].fillna(ultimate_df['avg_rating_by_driver'].mean(), inplace=True)
ultimate_df['avg_rating_of_driver'].fillna(ultimate_df['avg_rating_of_driver'].mean(), inplace=True)
ultimate_df['phone'].fillna(value='iPhone', inplace=True)

# Assess Dataframe
ultimate_df.info()
67/7:
# Import necessary modules
from datetime import datetime
67/8:
# Convert date strings to datetime objects
ultimate_df['signup_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['signup_date']]
ultimate_df['last_trip_date'] = [datetime.strptime(x, '%Y-%m-%d') for x in ultimate_df['last_trip_date']]

# Test output
ultimate_df[['signup_date','last_trip_date']].info()
67/9:
# Number of entries in the cleaned dataset
print(ultimate_df.shape)
67/10:
# Total number of unique values within each column
print(ultimate_df.nunique())
67/11:
# Import necessary modules
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns; sns.set(style="darkgrid")
67/12:
# Create dataframe for trips in first 30 days grouped by city
trips_by_city = (ultimate_df[['trips_in_first_30_days', 'city']].groupby('city').sum().reset_index())
trips_by_city
67/13:
# Plot counts of trips in first 30 days by city 
sns.barplot(x='city', y='trips_in_first_30_days', data=trips_by_city)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('City')
plt.title('Trips in First 30 Days by City')
67/14:
# Create dataframe for trips in first 30 days grouped by phone
trips_by_phone = (ultimate_df[['trips_in_first_30_days', 'phone']].groupby('phone').sum().reset_index())
trips_by_phone
67/15:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='phone', y='trips_in_first_30_days', data=trips_by_phone)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Phone')
plt.title('Trips in First 30 Days by Phone Type')
67/16:
# Create dataframe for trips in first 30 days grouped by user type
trips_by_usertype = (ultimate_df[['trips_in_first_30_days', 'ultimate_black_user']].groupby('ultimate_black_user').sum().reset_index())
trips_by_usertype
67/17:
# Relabel True/False 
trips_by_usertype['ultimate_black_user'] = ['Regular', 'Ultimate Black']
trips_by_usertype
67/18:
# Plot counts of trips in first 30 days by phone 
sns.barplot(x='ultimate_black_user', y='trips_in_first_30_days', data=trips_by_usertype)
plt.ylabel('Total Trips in First 30 Days')
plt.xlabel('Type of User')
plt.title('Trips in First 30 Days by User Type')
67/19:
# Import necessary modules
from pylab import rcParams
67/20:
# Determine last day in range
ultimate_df['last_trip_date'].max()
67/21:
# Create variables for customer_retained and customer_lost
customers = []
customer_retained = []
customer_lost = []

for last_trip in ultimate_df['last_trip_date']:
    
    # Select customers retained 30 days prior to last day
    if datetime.strptime('2014-06', '%Y-%m')  <= last_trip <= datetime.strptime('2014-07', '%Y-%m'):
        customer_retained.append(1)
        customers.append(True)
    else:
        customer_lost.append(0)
        customers.append(False)

# Test output
#customer_retained, customer_lost, customers
67/22:
# Add customers to ultimate_df 
ultimate_df['retained_customer'] = pd.Series(customers)
ultimate_df.tail()
67/23:
# Create a general function for percent
def calculate_percentage(val, total):
    return round((val / total)*100, 3)

# Test output
print(calculate_percentage(30, 100))
67/24:
# Calculate total retained and total lost
total_customers = len(ultimate_df['retained_customer'])
len_retained = len(customer_retained)
len_lost = len(customer_lost)
customers_list = [len_retained, len_lost]

# Calculate percentages
percent_retained = calculate_percentage(len_retained, total_customers)
percent_lost = calculate_percentage(len_lost, total_customers)

# Test output
len_retained, len_lost, percent_retained, percent_lost
67/25:
labels = ['Retained', 'Lost']
colors = ['grey','lightblue'] 
rcParams['figure.figsize'] = 8,8
explode = (0.1, 0) # only "explode" 'Retained'

# Plot
plt.pie(customers_list, explode=explode, colors=colors, labels=labels,
        autopct='%1.1f%%', shadow=True, startangle=270,)
plt.title('Percentage Retained Customers in January 2014')
plt.savefig('customersretained.png', dpi=300)
plt.show()
67/26: ultimate_df.head()
67/27:
# Remove datetime objects
ultimate_df = ultimate_df.drop(columns=['signup_date','last_trip_date'])
67/28:
# Verify output
ultimate_df.info()
67/29:
# Define target and feature variables
target = ultimate_df['retained_customer']
features = ultimate_df.drop(columns='retained_customer')
67/30:
# One-hot-encode all categorical features
features_encoded = pd.get_dummies(X_features, columns=['city', 'phone', 'ultimate_black_user'])
features_encoded.head()
67/31:
# One-hot-encode all categorical features
features_encoded = pd.get_dummies(features, columns=['city', 'phone', 'ultimate_black_user'])
features_encoded.head()
67/32:
# One-hot-encode target variable boolean
target_encoded = pd.get_dummies(target, columns=['retained_customer'])
target_encoded.head()
67/33:
# Define model variables
X_features_encoded = features_encoded.values
y_target_encoded = target_encoded.values
67/34:
# Import necessary modules
from sklearn.model_selection import train_test_split
67/35:
# Split dataset into Training Testing
X_train, X_test, y_train, y_test = train_test_split(X_features_encoded, y_target_encoded, 
                                                    test_size=0.25, random_state=42)
67/36:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
67/37:
# Initiate a scaler instance
sc = StandardScaler()

# Fit scaler to model featurs
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
67/38:
# Import necessary modules
import mlp
68/1:
# -*- coding: utf-8 -*-
"""
Author: Raymundo Cassani
April 2017
This file contains the Multi-Layer Perceptron (MLP) class which creates a
fully-connected-feedforward-artifitial-neural-network object with methods
for its usage
Methods:
    __init__()
    train(X, y, iterations, reset)
    predict(X)
    initialize_theta_weights()
    backpropagation(X, Y)
    feedforward(X)
    unroll_weights(rolled_data)
    roll_weights(unrolled_data)
    sigmoid(z)
    relu(z)
    sigmoid_derivative(z)
    relu_derivative(z)
"""
import numpy as np

class Mlp():
    '''
    fully-connected Multi-Layer Perceptron (MLP)
    '''

    def __init__(self, size_layers, act_funct='sigmoid', reg_lambda=0, bias_flag=True):
        '''
        Constructor method. Defines the characteristics of the MLP
        Arguments:
            size_layers : List with the number of Units for:
                [Input, Hidden1, Hidden2, ... HiddenN, Output] Layers.
            act_funtc   : Activation function for all the Units in the MLP
                default = 'sigmoid'
            reg_lambda: Value of the regularization parameter Lambda
                default = 0, i.e. no regularization
            bias: Indicates is the bias element is added for each layer, but the output
        '''
        self.size_layers = size_layers
        self.n_layers    = len(size_layers)
        self.act_f       = act_funct
        self.lambda_r    = reg_lambda
        self.bias_flag   = bias_flag
 
        # Ramdomly initialize theta (MLP weights)
        self.initialize_theta_weights()

    def train(self, X, Y, iterations=400, reset=False):
        '''
        Given X (feature matrix) and y (class vector)
        Updates the Theta Weights by running Backpropagation N tines
        Arguments:
            X          : Feature matrix [n_examples, n_features]
            Y          : Sparse class matrix [n_examples, classes]
            iterations : Number of times Backpropagation is performed
                default = 400
            reset      : If set, initialize Theta Weights before training
                default = False
        '''
        n_examples = Y.shape[0]
#        self.labels = np.unique(y)
#        Y = np.zeros((n_examples, len(self.labels)))
#        for ix_label in range(len(self.labels)):
#            # Find examples with with a Label = lables(ix_label)
#           ix_tmp = np.where(y == self.labels[ix_label])[0]
#            Y[ix_tmp, ix_label] = 1

        if reset:
            self.initialize_theta_weights()
        for iteration in range(iterations):
            self.gradients = self.backpropagation(X, Y)
            self.gradients_vector = self.unroll_weights(self.gradients)
            self.theta_vector = self.unroll_weights(self.theta_weights)
            self.theta_vector = self.theta_vector - self.gradients_vector
            self.theta_weights = self.roll_weights(self.theta_vector)

    def predict(self, X):
        '''
        Given X (feature matrix), y_hay is computed
        Arguments:
            X      : Feature matrix [n_examples, n_features]
        Output:
            y_hat  : Computed Vector Class for X
        '''
        A , Z = self.feedforward(X)
        Y_hat = A[-1]
        return Y_hat

    def initialize_theta_weights(self):
        '''
        Initialize theta_weights, initialization method depends
        on the Activation Function and the Number of Units in the current layer
        and the next layer.
        The weights for each layer as of the size [next_layer, current_layer + 1]
        '''
        self.theta_weights = []
        size_next_layers = self.size_layers.copy()
        size_next_layers.pop(0)
        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):
            if self.act_f == 'sigmoid':
                # Method presented "Understanding the difficulty of training deep feedforward neurla networks"
                # Xavier Glorot and Youshua Bengio, 2010
                epsilon = 4.0 * np.sqrt(6) / np.sqrt(size_layer + size_next_layer)
                # Weigts from a uniform distribution [-epsilon, epsion]
                if self.bias_flag:  
                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer + 1) * 2.0 ) - 1)
                else:
                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer) * 2.0 ) - 1)            
            elif self.act_f == 'relu':
                # Method presented in "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication"
                # He et Al. 2015
                epsilon = np.sqrt(2.0 / (size_layer * size_next_layer) )
                # Weigts from Normal distribution mean = 0, std = epsion
                if self.bias_flag:
                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer + 1 ))
                else:
                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer))                    
            self.theta_weights.append(theta_tmp)
        return self.theta_weights

    def backpropagation(self, X, Y):
        '''
        Implementation of the Backpropagation algorithm with regularization
        '''
        if self.act_f == 'sigmoid':
            g_dz = lambda x: self.sigmoid_derivative(x)
        elif self.act_f == 'relu':
            g_dz = lambda x: self.relu_derivative(x)

        n_examples = X.shape[0]
        # Feedforward
        A, Z = self.feedforward(X)

        # Backpropagation
        deltas = [None] * self.n_layers
        deltas[-1] = A[-1] - Y
        # For the second last layer to the second one
        for ix_layer in np.arange(self.n_layers - 1 - 1 , 0 , -1):
            theta_tmp = self.theta_weights[ix_layer]
            if self.bias_flag:
                # Removing weights for bias
                theta_tmp = np.delete(theta_tmp, np.s_[0], 1)
            deltas[ix_layer] = (np.matmul(theta_tmp.transpose(), deltas[ix_layer + 1].transpose() ) ).transpose() * g_dz(Z[ix_layer])

        # Compute gradients
        gradients = [None] * (self.n_layers - 1)
        for ix_layer in range(self.n_layers - 1):
            grads_tmp = np.matmul(deltas[ix_layer + 1].transpose() , A[ix_layer])
            grads_tmp = grads_tmp / n_examples
            if self.bias_flag:
                # Regularize weights, except for bias weigths
                grads_tmp[:, 1:] = grads_tmp[:, 1:] + (self.lambda_r / n_examples) * self.theta_weights[ix_layer][:,1:]
            else:
                # Regularize ALL weights
                grads_tmp = grads_tmp + (self.lambda_r / n_examples) * self.theta_weights[ix_layer]       
            gradients[ix_layer] = grads_tmp;
        return gradients

    def feedforward(self, X):
        '''
        Implementation of the Feedforward
        '''
        if self.act_f == 'sigmoid':
            g = lambda x: self.sigmoid(x)
        elif self.act_f == 'relu':
            g = lambda x: self.relu(x)

        A = [None] * self.n_layers
        Z = [None] * self.n_layers
        input_layer = X

        for ix_layer in range(self.n_layers - 1):
            n_examples = input_layer.shape[0]
            if self.bias_flag:
                # Add bias element to every example in input_layer
                input_layer = np.concatenate((np.ones([n_examples ,1]) ,input_layer), axis=1)
            A[ix_layer] = input_layer
            # Multiplying input_layer by theta_weights for this layer
            Z[ix_layer + 1] = np.matmul(input_layer,  self.theta_weights[ix_layer].transpose() )
            # Activation Function
            output_layer = g(Z[ix_layer + 1])
            # Current output_layer will be next input_layer
            input_layer = output_layer

        A[self.n_layers - 1] = output_layer
        return A, Z


    def unroll_weights(self, rolled_data):
        '''
        Unroll a list of matrices to a single vector
        Each matrix represents the Weights (or Gradients) from one layer to the next
        '''
        unrolled_array = np.array([])
        for one_layer in rolled_data:
            unrolled_array = np.concatenate((unrolled_array, one_layer.flatten(1)) )
        return unrolled_array

    def roll_weights(self, unrolled_data):
        '''
        Unrolls a single vector to a list of matrices
        Each matrix represents the Weights (or Gradients) from one layer to the next
        '''
        size_next_layers = self.size_layers.copy()
        size_next_layers.pop(0)
        rolled_list = []
        if self.bias_flag:
            extra_item = 1
        else:
            extra_item = 0
        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):
            n_weights = (size_next_layer * (size_layer + extra_item))
            data_tmp = unrolled_data[0 : n_weights]
            data_tmp = data_tmp.reshape(size_next_layer, (size_layer + extra_item), order = 'F')
            rolled_list.append(data_tmp)
            unrolled_data = np.delete(unrolled_data, np.s_[0:n_weights])
        return rolled_list

    def sigmoid(self, z):
        '''
        Sigmoid function
        z can be an numpy array or scalar
        '''
        result = 1.0 / (1.0 + np.exp(-z))
        return result

    def relu(self, z):
        '''
        Rectified Linear function
        z can be an numpy array or scalar
        '''
        if np.isscalar(z):
            result = np.max((z, 0))
        else:
            zero_aux = np.zeros(z.shape)
            meta_z = np.stack((z , zero_aux), axis = -1)
            result = np.max(meta_z, axis = -1)
        return result

    def sigmoid_derivative(self, z):
        '''
        Derivative for Sigmoid function
        z can be an numpy array or scalar
        '''
        result = self.sigmoid(z) * (1 - self.sigmoid(z))
        return result

    def relu_derivative(self, z):
        '''
        Derivative for Rectified Linear function
        z can be an numpy array or scalar
        '''
        result = 1 * (z > 0)
        return result
67/39:
# Import necessary modules
import mlp
67/40:
# Import necessary modules
from multilayer_perceptron_mlp.ipynb import mlp
67/41:
# Import necessary modules
from multilayer_perceptron_mlp import mlp
67/42:
# Import mlp class from other notebook
%run multilayer_perceptron_mlp.ipynb
67/43:
# Creating the MLP object initialize the weights
mlp_classifier = mlp.Mlp(size_layers = [784, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/44:
# Import mlp class from other notebook
%run multilayer_perceptron_mlp.ipynb as mlp
67/45:
# Creating the MLP object initialize the weights
mlp_classifier = mlp.Mlp(size_layers = [784, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/46: mlp
67/47: Mlp
67/48:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [784, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/49:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(train_X, train_Y, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - train_y).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(train_X)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == train_y))
print('Training Accuracy: ' + str(acc*100))
67/50:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/51:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [15, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/52:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/53:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [16, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/54:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/55:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [17, 25, 10, 10], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/56:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/57:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [15, 10, 5, 5], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/58:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/59:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [15, 10, 5, 5], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/60:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/61:
print('Shape of training set: ' + str(X_train.shape))
print('Shape of test set: ' + str(X_test.shape))
67/62:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [14, 10, 5, 5], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/63:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
67/64:
# Creating the MLP object initialize the weights
mlp_classifier = Mlp(size_layers = [14, 7, 3, 3], 
                         act_funct   = 'relu',
                         reg_lambda  = 0,
                         bias_flag   = True)
print(mlp_classifier)
67/65:
# Training with Backpropagation and 400 iterations
iterations = 400
loss = np.zeros([iterations,1])

for ix in range(iterations):
    mlp_classifier.train(X_train, y_train, 1)
    Y_hat = mlp_classifier.predict(train_X)
    y_tmp = np.argmax(Y_hat, axis=1)
    y_hat = labels[y_tmp]
    
    loss[ix] = (0.5)*np.square(y_hat - y_train).mean()

# Ploting loss vs iterations
plt.figure()
ix = np.arange(iterations)
plt.plot(ix, loss)

# Training Accuracy
Y_hat = mlp_classifier.predict(X_train)
y_tmp = np.argmax(Y_hat, axis=1)
y_hat = labels[y_tmp]

acc = np.mean(1 * (y_hat == y_train))
print('Training Accuracy: ' + str(acc*100))
71/1: %load ../Desktop/pyAudioAnalysis/pyAudioAnalysis/audioFeatureExtraction.py
71/2: %load Desktop/pyAudioAnalysis/pyAudioAnalysis/audioFeatureExtraction.py
71/3: %load ../pyAudioAnalysis/pyAudioAnalysis/audioFeatureExtraction.py
71/4:
# %load ../pyAudioAnalysis/pyAudioAnalysis/audioFeatureExtraction.py
from __future__ import print_function
import time
import os
import glob
import numpy
import math
from scipy.fftpack import fft
from scipy.fftpack.realtransforms import dct
import matplotlib.pyplot as plt
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import utilities
from scipy.signal import lfilter

eps = 0.00000001

""" Time-domain audio features """


def stZCR(frame):
    """Computes zero crossing rate of frame"""
    count = len(frame)
    countZ = numpy.sum(numpy.abs(numpy.diff(numpy.sign(frame)))) / 2
    return (numpy.float64(countZ) / numpy.float64(count-1.0))


def stEnergy(frame):
    """Computes signal energy of frame"""
    return numpy.sum(frame ** 2) / numpy.float64(len(frame))


def stEnergyEntropy(frame, n_short_blocks=10):
    """Computes entropy of energy"""
    Eol = numpy.sum(frame ** 2)    # total frame energy
    L = len(frame)
    sub_win_len = int(numpy.floor(L / n_short_blocks))
    if L != sub_win_len * n_short_blocks:
            frame = frame[0:sub_win_len * n_short_blocks]
    # sub_wins is of size [n_short_blocks x L]
    sub_wins = frame.reshape(sub_win_len, n_short_blocks, order='F').copy()

    # Compute normalized sub-frame energies:
    s = numpy.sum(sub_wins ** 2, axis=0) / (Eol + eps)

    # Compute entropy of the normalized sub-frame energies:
    Entropy = -numpy.sum(s * numpy.log2(s + eps))
    return Entropy


""" Frequency-domain audio features """


def stSpectralCentroidAndSpread(X, fs):
    """Computes spectral centroid of frame (given abs(FFT))"""
    ind = (numpy.arange(1, len(X) + 1)) * (fs/(2.0 * len(X)))

    Xt = X.copy()
    Xt = Xt / Xt.max()
    NUM = numpy.sum(ind * Xt)
    DEN = numpy.sum(Xt) + eps

    # Centroid:
    C = (NUM / DEN)

    # Spread:
    S = numpy.sqrt(numpy.sum(((ind - C) ** 2) * Xt) / DEN)

    # Normalize:
    C = C / (fs / 2.0)
    S = S / (fs / 2.0)

    return (C, S)


def stSpectralEntropy(X, n_short_blocks=10):
    """Computes the spectral entropy"""
    L = len(X)                         # number of frame samples
    Eol = numpy.sum(X ** 2)            # total spectral energy

    sub_win_len = int(numpy.floor(L / n_short_blocks))   # length of sub-frame
    if L != sub_win_len * n_short_blocks:
        X = X[0:sub_win_len * n_short_blocks]

    sub_wins = X.reshape(sub_win_len, n_short_blocks, order='F').copy()  # define sub-frames (using matrix reshape)
    s = numpy.sum(sub_wins ** 2, axis=0) / (Eol + eps)                      # compute spectral sub-energies
    En = -numpy.sum(s*numpy.log2(s + eps))                                    # compute spectral entropy

    return En


def stSpectralFlux(X, X_prev):
    """
    Computes the spectral flux feature of the current frame
    ARGUMENTS:
        X:            the abs(fft) of the current frame
        X_prev:        the abs(fft) of the previous frame
    """
    # compute the spectral flux as the sum of square distances:
    sumX = numpy.sum(X + eps)
    sumPrevX = numpy.sum(X_prev + eps)
    F = numpy.sum((X / sumX - X_prev/sumPrevX) ** 2)

    return F


def stSpectralRollOff(X, c, fs):
    """Computes spectral roll-off"""
    totalEnergy = numpy.sum(X ** 2)
    fftLength = len(X)
    Thres = c*totalEnergy
    # Ffind the spectral rolloff as the frequency position 
    # where the respective spectral energy is equal to c*totalEnergy
    CumSum = numpy.cumsum(X ** 2) + eps
    [a, ] = numpy.nonzero(CumSum > Thres)
    if len(a) > 0:
        mC = numpy.float64(a[0]) / (float(fftLength))
    else:
        mC = 0.0
    return (mC)


def stHarmonic(frame, fs):
    """
    Computes harmonic ratio and pitch
    """
    M = numpy.round(0.016 * fs) - 1
    R = numpy.correlate(frame, frame, mode='full')

    g = R[len(frame)-1]
    R = R[len(frame):-1]

    # estimate m0 (as the first zero crossing of R)
    [a, ] = numpy.nonzero(numpy.diff(numpy.sign(R)))

    if len(a) == 0:
        m0 = len(R)-1
    else:
        m0 = a[0]
    if M > len(R):
        M = len(R) - 1

    Gamma = numpy.zeros((M), dtype=numpy.float64)
    CSum = numpy.cumsum(frame ** 2)
    Gamma[m0:M] = R[m0:M] / (numpy.sqrt((g * CSum[M:m0:-1])) + eps)

    ZCR = stZCR(Gamma)

    if ZCR > 0.15:
        HR = 0.0
        f0 = 0.0
    else:
        if len(Gamma) == 0:
            HR = 1.0
            blag = 0.0
            Gamma = numpy.zeros((M), dtype=numpy.float64)
        else:
            HR = numpy.max(Gamma)
            blag = numpy.argmax(Gamma)

        # Get fundamental frequency:
        f0 = fs / (blag + eps)
        if f0 > 5000:
            f0 = 0.0
        if HR < 0.1:
            f0 = 0.0

    return (HR, f0)


def mfccInitFilterBanks(fs, nfft):
    """
    Computes the triangular filterbank for MFCC computation 
    (used in the stFeatureExtraction function before the stMFCC function call)
    This function is taken from the scikits.talkbox library (MIT Licence):
    https://pypi.python.org/pypi/scikits.talkbox
    """

    # filter bank params:
    lowfreq = 133.33
    linsc = 200/3.
    logsc = 1.0711703
    numLinFiltTotal = 13
    numLogFilt = 27

    if fs < 8000:
        nlogfil = 5

    # Total number of filters
    nFiltTotal = numLinFiltTotal + numLogFilt

    # Compute frequency points of the triangle:
    freqs = numpy.zeros(nFiltTotal+2)
    freqs[:numLinFiltTotal] = lowfreq + numpy.arange(numLinFiltTotal) * linsc
    freqs[numLinFiltTotal:] = freqs[numLinFiltTotal-1] * logsc ** numpy.arange(1, numLogFilt + 3)
    heights = 2./(freqs[2:] - freqs[0:-2])

    # Compute filterbank coeff (in fft domain, in bins)
    fbank = numpy.zeros((nFiltTotal, nfft))
    nfreqs = numpy.arange(nfft) / (1. * nfft) * fs

    for i in range(nFiltTotal):
        lowTrFreq = freqs[i]
        cenTrFreq = freqs[i+1]
        highTrFreq = freqs[i+2]

        lid = numpy.arange(numpy.floor(lowTrFreq * nfft / fs) + 1, 
                           numpy.floor(cenTrFreq * nfft / fs) + 1,  
                                       dtype=numpy.int)
        lslope = heights[i] / (cenTrFreq - lowTrFreq)
        rid = numpy.arange(numpy.floor(cenTrFreq * nfft / fs) + 1, 
                                       numpy.floor(highTrFreq * nfft / fs) + 1, 
                                       dtype=numpy.int)
        rslope = heights[i] / (highTrFreq - cenTrFreq)
        fbank[i][lid] = lslope * (nfreqs[lid] - lowTrFreq)
        fbank[i][rid] = rslope * (highTrFreq - nfreqs[rid])

    return fbank, freqs


def stMFCC(X, fbank, n_mfcc_feats):
    """
    Computes the MFCCs of a frame, given the fft mag

    ARGUMENTS:
        X:        fft magnitude abs(FFT)
        fbank:    filter bank (see mfccInitFilterBanks)
    RETURN
        ceps:     MFCCs (13 element vector)

    Note:    MFCC calculation is, in general, taken from the 
             scikits.talkbox library (MIT Licence),
    #    with a small number of modifications to make it more 
         compact and suitable for the pyAudioAnalysis Lib
    """

    mspec = numpy.log10(numpy.dot(X, fbank.T)+eps)
    ceps = dct(mspec, type=2, norm='ortho', axis=-1)[:n_mfcc_feats]
    return ceps


def stChromaFeaturesInit(nfft, fs):
    """
    This function initializes the chroma matrices used in the calculation of the chroma features
    """
    freqs = numpy.array([((f + 1) * fs) / (2 * nfft) for f in range(nfft)])    
    Cp = 27.50    
    nChroma = numpy.round(12.0 * numpy.log2(freqs / Cp)).astype(int)

    nFreqsPerChroma = numpy.zeros((nChroma.shape[0], ))

    uChroma = numpy.unique(nChroma)
    for u in uChroma:
        idx = numpy.nonzero(nChroma == u)
        nFreqsPerChroma[idx] = idx[0].shape
    
    return nChroma, nFreqsPerChroma


def stChromaFeatures(X, fs, nChroma, nFreqsPerChroma):
    #TODO: 1 complexity
    #TODO: 2 bug with large windows

    chromaNames = ['A', 'A#', 'B', 'C', 'C#', 'D', 
                   'D#', 'E', 'F', 'F#', 'G', 'G#']
    spec = X**2    
    if nChroma.max()<nChroma.shape[0]:        
        C = numpy.zeros((nChroma.shape[0],))
        C[nChroma] = spec
        C /= nFreqsPerChroma[nChroma]
    else:        
        I = numpy.nonzero(nChroma>nChroma.shape[0])[0][0]        
        C = numpy.zeros((nChroma.shape[0],))
        C[nChroma[0:I-1]] = spec            
        C /= nFreqsPerChroma
    finalC = numpy.zeros((12, 1))
    newD = int(numpy.ceil(C.shape[0] / 12.0) * 12)
    C2 = numpy.zeros((newD, ))
    C2[0:C.shape[0]] = C
    C2 = C2.reshape(int(C2.shape[0]/12), 12)
    #for i in range(12):
    #    finalC[i] = numpy.sum(C[i:C.shape[0]:12])
    finalC = numpy.matrix(numpy.sum(C2, axis=0)).T
    finalC /= spec.sum()

#    ax = plt.gca()
#    plt.hold(False)
#    plt.plot(finalC)
#    ax.set_xticks(range(len(chromaNames)))
#    ax.set_xticklabels(chromaNames)
#    xaxis = numpy.arange(0, 0.02, 0.01);
#    ax.set_yticks(range(len(xaxis)))
#    ax.set_yticklabels(xaxis)
#    plt.show(block=False)
#    plt.draw()

    return chromaNames, finalC


def stChromagram(signal, fs, win, step, PLOT=False):
    """
    Short-term FFT mag for spectogram estimation:
    Returns:
        a numpy array (nFFT x numOfShortTermWindows)
    ARGUMENTS:
        signal:      the input signal samples
        fs:          the sampling freq (in Hz)
        win:         the short-term window size (in samples)
        step:        the short-term window step (in samples)
        PLOT:        flag, 1 if results are to be ploted
    RETURNS:
    """
    win = int(win)
    step = int(step)
    signal = numpy.double(signal)
    signal = signal / (2.0 ** 15)
    DC = signal.mean()
    MAX = (numpy.abs(signal)).max()
    signal = (signal - DC) / (MAX - DC)

    N = len(signal)        # total number of signals
    cur_p = 0
    count_fr = 0
    nfft = int(win / 2)
    nChroma, nFreqsPerChroma = stChromaFeaturesInit(nfft, fs)
    chromaGram = numpy.array([], dtype=numpy.float64)

    while (cur_p + win - 1 < N):
        count_fr += 1
        x = signal[cur_p:cur_p + win]
        cur_p = cur_p + step
        X = abs(fft(x))
        X = X[0:nfft]
        X = X / len(X)
        chromaNames, C = stChromaFeatures(X, fs, nChroma, nFreqsPerChroma)
        C = C[:, 0]
        if count_fr == 1:
            chromaGram = C.T
        else:
            chromaGram = numpy.vstack((chromaGram, C.T))
    FreqAxis = chromaNames
    TimeAxis = [(t * step) / fs for t in range(chromaGram.shape[0])]

    if (PLOT):
        fig, ax = plt.subplots()
        chromaGramToPlot = chromaGram.transpose()[::-1, :]
        Ratio = int(chromaGramToPlot.shape[1] / (3*chromaGramToPlot.shape[0]))
        if Ratio < 1:
            Ratio = 1
        chromaGramToPlot = numpy.repeat(chromaGramToPlot, Ratio, axis=0)
        imgplot = plt.imshow(chromaGramToPlot)
        fstep = int(nfft / 5.0)
#        FreqTicks = range(0, int(nfft) + fstep, fstep)
#        FreqTicksLabels = [str(fs/2-int((f*fs) / (2*nfft))) for f in FreqTicks]
        ax.set_yticks(range(int(Ratio / 2), len(FreqAxis) * Ratio, Ratio))
        ax.set_yticklabels(FreqAxis[::-1])
        TStep = int(count_fr / 3)
        TimeTicks = range(0, count_fr, TStep)
        TimeTicksLabels = ['%.2f' % (float(t * step) / fs) for t in TimeTicks]
        ax.set_xticks(TimeTicks)
        ax.set_xticklabels(TimeTicksLabels)
        ax.set_xlabel('time (secs)')
        imgplot.set_cmap('jet')
        plt.colorbar()
        plt.show()

    return (chromaGram, TimeAxis, FreqAxis)


def phormants(x, fs):
    N = len(x)
    w = numpy.hamming(N)

    # Apply window and high pass filter.
    x1 = x * w   
    x1 = lfilter([1], [1., 0.63], x1)
    
    # Get LPC.    
    ncoeff = 2 + fs / 1000
    A, e, k = lpc(x1, ncoeff)    
    #A, e, k = lpc(x1, 8)

    # Get roots.
    rts = numpy.roots(A)
    rts = [r for r in rts if numpy.imag(r) >= 0]

    # Get angles.
    angz = numpy.arctan2(numpy.imag(rts), numpy.real(rts))

    # Get frequencies.    
    frqs = sorted(angz * (fs / (2 * math.pi)))

    return frqs
def beatExtraction(st_features, win_len, PLOT=False):
    """
    This function extracts an estimate of the beat rate for a musical signal.
    ARGUMENTS:
     - st_features:     a numpy array (n_feats x numOfShortTermWindows)
     - win_len:        window size in seconds
    RETURNS:
     - BPM:            estimates of beats per minute
     - Ratio:          a confidence measure
    """

    # Features that are related to the beat tracking task:
    toWatch = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]

    max_beat_time = int(round(2.0 / win_len))
    hist_all = numpy.zeros((max_beat_time,))
    for ii, i in enumerate(toWatch):                                        # for each feature
        DifThres = 2.0 * (numpy.abs(st_features[i, 0:-1] - st_features[i, 1::])).mean()    # dif threshold (3 x Mean of Difs)
        if DifThres<=0:
            DifThres = 0.0000000000000001        
        [pos1, _] = utilities.peakdet(st_features[i, :], DifThres)           # detect local maxima
        posDifs = []                                                        # compute histograms of local maxima changes
        for j in range(len(pos1)-1):
            posDifs.append(pos1[j+1]-pos1[j])
        [hist_times, HistEdges] = numpy.histogram(posDifs, numpy.arange(0.5, max_beat_time + 1.5))
        hist_centers = (HistEdges[0:-1] + HistEdges[1::]) / 2.0
        hist_times = hist_times.astype(float) / st_features.shape[1]
        hist_all += hist_times
        if PLOT:
            plt.subplot(9, 2, ii + 1)
            plt.plot(st_features[i, :], 'k')
            for k in pos1:
                plt.plot(k, st_features[i, k], 'k*')
            f1 = plt.gca()
            f1.axes.get_xaxis().set_ticks([])
            f1.axes.get_yaxis().set_ticks([])

    if PLOT:
        plt.show(block=False)
        plt.figure()

    # Get beat as the argmax of the agregated histogram:
    I = numpy.argmax(hist_all)
    bpms = 60 / (hist_centers * win_len)
    BPM = bpms[I]
    # ... and the beat ratio:
    Ratio = hist_all[I] / hist_all.sum()

    if PLOT:
        # filter out >500 beats from plotting:
        hist_all = hist_all[bpms < 500]
        bpms = bpms[bpms < 500]

        plt.plot(bpms, hist_all, 'k')
        plt.xlabel('Beats per minute')
        plt.ylabel('Freq Count')
        plt.show(block=True)

    return BPM, Ratio


def stSpectogram(signal, fs, win, step, PLOT=False):
    """
    Short-term FFT mag for spectogram estimation:
    Returns:
        a numpy array (nFFT x numOfShortTermWindows)
    ARGUMENTS:
        signal:      the input signal samples
        fs:          the sampling freq (in Hz)
        win:         the short-term window size (in samples)
        step:        the short-term window step (in samples)
        PLOT:        flag, 1 if results are to be ploted
    RETURNS:
    """
    win = int(win)
    step = int(step)
    signal = numpy.double(signal)
    signal = signal / (2.0 ** 15)
    DC = signal.mean()
    MAX = (numpy.abs(signal)).max()
    signal = (signal - DC) / (MAX - DC)

    N = len(signal)        # total number of signals
    cur_p = 0
    count_fr = 0
    nfft = int(win / 2)
    specgram = numpy.array([], dtype=numpy.float64)

    while (cur_p + win - 1 < N):
        count_fr += 1
        x = signal[cur_p:cur_p+win]
        cur_p = cur_p + step
        X = abs(fft(x))
        X = X[0:nfft]
        X = X / len(X)

        if count_fr == 1:
            specgram = X ** 2
        else:
            specgram = numpy.vstack((specgram, X))

    FreqAxis = [float((f + 1) * fs) / (2 * nfft) for f in range(specgram.shape[1])]
    TimeAxis = [float(t * step) / fs for t in range(specgram.shape[0])]

    if (PLOT):
        fig, ax = plt.subplots()
        imgplot = plt.imshow(specgram.transpose()[::-1, :])
        fstep = int(nfft / 5.0)
        FreqTicks = range(0, int(nfft) + fstep, fstep)
        FreqTicksLabels = [str(fs / 2 - int((f * fs) / (2 * nfft))) for f in FreqTicks]
        ax.set_yticks(FreqTicks)
        ax.set_yticklabels(FreqTicksLabels)
        TStep = int(count_fr/3)
        TimeTicks = range(0, count_fr, TStep)
        TimeTicksLabels = ['%.2f' % (float(t * step) / fs) for t in TimeTicks]
        ax.set_xticks(TimeTicks)
        ax.set_xticklabels(TimeTicksLabels)
        ax.set_xlabel('time (secs)')
        ax.set_ylabel('freq (Hz)')
        imgplot.set_cmap('jet')
        plt.colorbar()
        plt.show()

    return (specgram, TimeAxis, FreqAxis)


""" Windowing and feature extraction """


def stFeatureExtraction(signal, fs, win, step):
    """
    This function implements the shor-term windowing process. For each short-term window a set of features is extracted.
    This results to a sequence of feature vectors, stored in a numpy matrix.

    ARGUMENTS
        signal:       the input signal samples
        fs:           the sampling freq (in Hz)
        win:          the short-term window size (in samples)
        step:         the short-term window step (in samples)
    RETURNS
        st_features:   a numpy array (n_feats x numOfShortTermWindows)
    """

    win = int(win)
    step = int(step)

    # Signal normalization
    signal = numpy.double(signal)

    signal = signal / (2.0 ** 15)
    DC = signal.mean()
    MAX = (numpy.abs(signal)).max()
    signal = (signal - DC) / (MAX + 0.0000000001)

    N = len(signal)                                # total number of samples
    cur_p = 0
    count_fr = 0
    nFFT = int(win / 2)

    [fbank, freqs] = mfccInitFilterBanks(fs, nFFT)                # compute the triangular filter banks used in the mfcc calculation
    nChroma, nFreqsPerChroma = stChromaFeaturesInit(nFFT, fs)

    n_time_spectral_feats = 8
    n_harmonic_feats = 0
    n_mfcc_feats = 13
    n_chroma_feats = 13
    n_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats + n_chroma_feats
#    n_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats
    feature_names = []
    feature_names.append("zcr")
    feature_names.append("energy")
    feature_names.append("energy_entropy")
    feature_names += ["spectral_centroid", "spectral_spread"]
    feature_names.append("spectral_entropy")
    feature_names.append("spectral_flux")
    feature_names.append("spectral_rolloff")
    feature_names += ["mfcc_{0:d}".format(mfcc_i) 
                      for mfcc_i in range(1, n_mfcc_feats+1)]
    feature_names += ["chroma_{0:d}".format(chroma_i) 
                      for chroma_i in range(1, n_chroma_feats)]
    feature_names.append("chroma_std")
    st_features = []
    while (cur_p + win - 1 < N):                        # for each short-term window until the end of signal
        count_fr += 1
        x = signal[cur_p:cur_p+win]                    # get current window
        cur_p = cur_p + step                           # update window position
        X = abs(fft(x))                                  # get fft magnitude
        X = X[0:nFFT]                                    # normalize fft
        X = X / len(X)
        if count_fr == 1:
            X_prev = X.copy()                             # keep previous fft mag (used in spectral flux)
        curFV = numpy.zeros((n_total_feats, 1))
        curFV[0] = stZCR(x)                              # zero crossing rate
        curFV[1] = stEnergy(x)                           # short-term energy
        curFV[2] = stEnergyEntropy(x)                    # short-term entropy of energy
        [curFV[3], curFV[4]] = stSpectralCentroidAndSpread(X, fs)    # spectral centroid and spread
        curFV[5] = stSpectralEntropy(X)                  # spectral entropy
        curFV[6] = stSpectralFlux(X, X_prev)              # spectral flux
        curFV[7] = stSpectralRollOff(X, 0.90, fs)        # spectral rolloff
        curFV[n_time_spectral_feats:n_time_spectral_feats+n_mfcc_feats, 0] = \
            stMFCC(X, fbank, n_mfcc_feats).copy()    # MFCCs
        chromaNames, chromaF = stChromaFeatures(X, fs, nChroma, nFreqsPerChroma)
        curFV[n_time_spectral_feats + n_mfcc_feats:
              n_time_spectral_feats + n_mfcc_feats + n_chroma_feats - 1] = \
            chromaF
        curFV[n_time_spectral_feats + n_mfcc_feats + n_chroma_feats - 1] = \
            chromaF.std()
        st_features.append(curFV)
        # delta features
        '''
        if count_fr>1:
            delta = curFV - prevFV
            curFVFinal = numpy.concatenate((curFV, delta))            
        else:
            curFVFinal = numpy.concatenate((curFV, curFV))
        prevFV = curFV
        st_features.append(curFVFinal)        
        '''
        # end of delta
        X_prev = X.copy()

    st_features = numpy.concatenate(st_features, 1)
    return st_features, feature_names


def mtFeatureExtraction(signal, fs, mt_win, mt_step, st_win, st_step):
    """
    Mid-term feature extraction
    """

    mt_win_ratio = int(round(mt_win / st_step))
    mt_step_ratio = int(round(mt_step / st_step))

    mt_features = []

    st_features, f_names = stFeatureExtraction(signal, fs, st_win, st_step)
    n_feats = len(st_features)
    n_stats = 2

    mt_features, mid_feature_names = [], []
    #for i in range(n_stats * n_feats + 1):
    for i in range(n_stats * n_feats):
        mt_features.append([])
        mid_feature_names.append("")

    for i in range(n_feats):        # for each of the short-term features:
        cur_p = 0
        N = len(st_features[i])
        mid_feature_names[i] = f_names[i] + "_" + "mean"
        mid_feature_names[i + n_feats] = f_names[i] + "_" + "std"

        while (cur_p < N):
            N1 = cur_p
            N2 = cur_p + mt_win_ratio
            if N2 > N:
                N2 = N
            cur_st_feats = st_features[i][N1:N2]

            mt_features[i].append(numpy.mean(cur_st_feats))
            mt_features[i + n_feats].append(numpy.std(cur_st_feats))
            #mt_features[i+2*n_feats].append(numpy.std(cur_st_feats) / (numpy.mean(cur_st_feats)+0.00000010))
            cur_p += mt_step_ratio
    return numpy.array(mt_features), st_features, mid_feature_names


# TODO
def stFeatureSpeed(signal, fs, win, step):

    signal = numpy.double(signal)
    signal = signal / (2.0 ** 15)
    DC = signal.mean()
    MAX = (numpy.abs(signal)).max()
    signal = (signal - DC) / MAX
    # print (numpy.abs(signal)).max()

    N = len(signal)        # total number of signals
    cur_p = 0
    count_fr = 0

    lowfreq = 133.33
    linsc = 200/3.
    logsc = 1.0711703
    nlinfil = 13
    nlogfil = 27
    n_mfcc_feats = 13
    nfil = nlinfil + nlogfil
    nfft = win / 2
    if fs < 8000:
        nlogfil = 5
        nfil = nlinfil + nlogfil
        nfft = win / 2

    # compute filter banks for mfcc:
    [fbank, freqs] = mfccInitFilterBanks(fs, nfft, lowfreq, linsc, logsc, nlinfil, nlogfil)

    n_time_spectral_feats = 8
    n_harmonic_feats = 1
    n_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats
    #st_features = numpy.array([], dtype=numpy.float64)
    st_features = []

    while (cur_p + win - 1 < N):
        count_fr += 1
        x = signal[cur_p:cur_p + win]
        cur_p = cur_p + step
        X = abs(fft(x))
        X = X[0:nfft]
        X = X / len(X)
        Ex = 0.0
        El = 0.0
        X[0:4] = 0
#        M = numpy.round(0.016 * fs) - 1
#        R = numpy.correlate(frame, frame, mode='full')
        st_features.append(stHarmonic(x, fs))
#        for i in range(len(X)):
            #if (i < (len(X) / 8)) and (i > (len(X)/40)):
            #    Ex += X[i]*X[i]
            #El += X[i]*X[i]
#        st_features.append(Ex / El)
#        st_features.append(numpy.argmax(X))
#        if curFV[n_time_spectral_feats+n_mfcc_feats+1]>0:
#            print curFV[n_time_spectral_feats+n_mfcc_feats], curFV[n_time_spectral_feats+n_mfcc_feats+1]
    return numpy.array(st_features)


""" Feature Extraction Wrappers

 - The first two feature extraction wrappers are used to extract long-term averaged
   audio features for a list of WAV files stored in a given category.
   It is important to note that, one single feature is extracted per WAV file (not the whole sequence of feature vectors)

 """


def dirWavFeatureExtraction(dirName, mt_win, mt_step, st_win, st_step,
                            compute_beat=False):
    """
    This function extracts the mid-term features of the WAVE files of a particular folder.

    The resulting feature vector is extracted by long-term averaging the mid-term features.
    Therefore ONE FEATURE VECTOR is extracted for each WAV file.

    ARGUMENTS:
        - dirName:        the path of the WAVE directory
        - mt_win, mt_step:    mid-term window and step (in seconds)
        - st_win, st_step:    short-term window and step (in seconds)
    """

    all_mt_feats = numpy.array([])
    process_times = []

    types = ('*.wav', '*.aif',  '*.aiff', '*.mp3', '*.au', '*.ogg')
    wav_file_list = []
    for files in types:
        wav_file_list.extend(glob.glob(os.path.join(dirName, files)))

    wav_file_list = sorted(wav_file_list)    
    wav_file_list2, mt_feature_names = [], []
    for i, wavFile in enumerate(wav_file_list):        
        print("Analyzing file {0:d} of "
              "{1:d}: {2:s}".format(i+1,
                                    len(wav_file_list),
                                    wavFile))
        if os.stat(wavFile).st_size == 0:
            print("   (EMPTY FILE -- SKIPPING)")
            continue        
        [fs, x] = audioBasicIO.readAudioFile(wavFile)
        if isinstance(x, int):
            continue        

        t1 = time.clock()        
        x = audioBasicIO.stereo2mono(x)
        if x.shape[0]<float(fs)/5:
            print("  (AUDIO FILE TOO SMALL - SKIPPING)")
            continue
        wav_file_list2.append(wavFile)
        if compute_beat:
            [mt_term_feats, st_features, mt_feature_names] = \
                mtFeatureExtraction(x, fs, round(mt_win * fs),
                                    round(mt_step * fs),
                                    round(fs * st_win), round(fs * st_step))
            [beat, beat_conf] = beatExtraction(st_features, st_step)
        else:
            [mt_term_feats, _, mt_feature_names] = \
                mtFeatureExtraction(x, fs, round(mt_win * fs),
                                    round(mt_step * fs),
                                    round(fs * st_win), round(fs * st_step))

        mt_term_feats = numpy.transpose(mt_term_feats)
        mt_term_feats = mt_term_feats.mean(axis=0)
        # long term averaging of mid-term statistics
        if (not numpy.isnan(mt_term_feats).any()) and \
                (not numpy.isinf(mt_term_feats).any()):
            if compute_beat:
                mt_term_feats = numpy.append(mt_term_feats, beat)
                mt_term_feats = numpy.append(mt_term_feats, beat_conf)
            if len(all_mt_feats) == 0:
                # append feature vector
                all_mt_feats = mt_term_feats
            else:
                all_mt_feats = numpy.vstack((all_mt_feats, mt_term_feats))
            t2 = time.clock()
            duration = float(len(x)) / fs
            process_times.append((t2 - t1) / duration)
    if len(process_times) > 0:
        print("Feature extraction complexity ratio: "
              "{0:.1f} x realtime".format((1.0 / numpy.mean(numpy.array(process_times)))))
    return (all_mt_feats, wav_file_list2, mt_feature_names)


def dirsWavFeatureExtraction(dirNames, mt_win, mt_step, st_win, st_step, compute_beat=False):
    '''
    Same as dirWavFeatureExtraction, but instead of a single dir it
    takes a list of paths as input and returns a list of feature matrices.
    EXAMPLE:
    [features, classNames] =
           a.dirsWavFeatureExtraction(['audioData/classSegmentsRec/noise','audioData/classSegmentsRec/speech',
                                       'audioData/classSegmentsRec/brush-teeth','audioData/classSegmentsRec/shower'], 1, 1, 0.02, 0.02);

    It can be used during the training process of a classification model ,
    in order to get feature matrices from various audio classes (each stored in a seperate path)
    '''

    # feature extraction for each class:
    features = []
    classNames = []
    fileNames = []
    for i, d in enumerate(dirNames):
        [f, fn, feature_names] = dirWavFeatureExtraction(d, mt_win, mt_step,
                                                         st_win, st_step,
                                                         compute_beat=compute_beat)
        if f.shape[0] > 0:
            # if at least one audio file has been found in the provided folder:
            features.append(f)
            fileNames.append(fn)
            if d[-1] == os.sep:
                classNames.append(d.split(os.sep)[-2])
            else:
                classNames.append(d.split(os.sep)[-1])
    return features, classNames, fileNames


def dirWavFeatureExtractionNoAveraging(dirName, mt_win, mt_step, st_win, st_step):
    """
    This function extracts the mid-term features of the WAVE
    files of a particular folder without averaging each file.

    ARGUMENTS:
        - dirName:          the path of the WAVE directory
        - mt_win, mt_step:    mid-term window and step (in seconds)
        - st_win, st_step:    short-term window and step (in seconds)
    RETURNS:
        - X:                A feature matrix
        - Y:                A matrix of file labels
        - filenames:
    """

    all_mt_feats = numpy.array([])
    signal_idx = numpy.array([])
    process_times = []

    types = ('*.wav', '*.aif',  '*.aiff', '*.ogg')
    wav_file_list = []
    for files in types:
        wav_file_list.extend(glob.glob(os.path.join(dirName, files)))

    wav_file_list = sorted(wav_file_list)

    for i, wavFile in enumerate(wav_file_list):
        [fs, x] = audioBasicIO.readAudioFile(wavFile)
        if isinstance(x, int):
            continue        
        
        x = audioBasicIO.stereo2mono(x)
        [mt_term_feats, _, _] = mtFeatureExtraction(x, fs, round(mt_win * fs),
                                                    round(mt_step * fs),
                                                    round(fs * st_win),
                                                    round(fs * st_step))

        mt_term_feats = numpy.transpose(mt_term_feats)
        if len(all_mt_feats) == 0:                # append feature vector
            all_mt_feats = mt_term_feats
            signal_idx = numpy.zeros((mt_term_feats.shape[0], ))
        else:
            all_mt_feats = numpy.vstack((all_mt_feats, mt_term_feats))
            signal_idx = numpy.append(signal_idx, i * numpy.ones((mt_term_feats.shape[0], )))

    return (all_mt_feats, signal_idx, wav_file_list)


# The following two feature extraction wrappers extract features for given audio files, however
# NO LONG-TERM AVERAGING is performed. Therefore, the output for each audio file is NOT A SINGLE FEATURE VECTOR
# but a whole feature matrix.
#
# Also, another difference between the following two wrappers and the previous is that they NO LONG-TERM AVERAGING IS PERFORMED.
# In other words, the WAV files in these functions are not used as uniform samples that need to be averaged but as sequences

def mtFeatureExtractionToFile(fileName, midTermSize, midTermStep, shortTermSize, shortTermStep, outPutFile,
                              storeStFeatures=False, storeToCSV=False, PLOT=False):
    """
    This function is used as a wrapper to:
    a) read the content of a WAV file
    b) perform mid-term feature extraction on that signal
    c) write the mid-term feature sequences to a numpy file
    """
    [fs, x] = audioBasicIO.readAudioFile(fileName)
    x = audioBasicIO.stereo2mono(x)
    if storeStFeatures:
        [mtF, stF, _] = mtFeatureExtraction(x, fs,
                                         round(fs * midTermSize),
                                         round(fs * midTermStep),
                                         round(fs * shortTermSize),
                                         round(fs * shortTermStep))
    else:
        [mtF, _, _] = mtFeatureExtraction(x, fs, round(fs*midTermSize),
                                       round(fs * midTermStep),
                                       round(fs * shortTermSize),
                                       round(fs * shortTermStep))
    # save mt features to numpy file
    numpy.save(outPutFile, mtF)
    if PLOT:
        print("Mid-term numpy file: " + outPutFile + ".npy saved")
    if storeToCSV:
        numpy.savetxt(outPutFile+".csv", mtF.T, delimiter=",")
        if PLOT:
            print("Mid-term CSV file: " + outPutFile + ".csv saved")

    if storeStFeatures:
        # save st features to numpy file
        numpy.save(outPutFile+"_st", stF)
        if PLOT:
            print("Short-term numpy file: " + outPutFile + "_st.npy saved")
        if storeToCSV:
            # store st features to CSV file
            numpy.savetxt(outPutFile+"_st.csv", stF.T, delimiter=",")
            if PLOT:
                print("Short-term CSV file: " + outPutFile + "_st.csv saved")


def mtFeatureExtractionToFileDir(dirName, midTermSize, midTermStep,
                                 shortTermSize, shortTermStep,
                                 storeStFeatures=False, storeToCSV=False,
                                 PLOT=False):
    types = (dirName + os.sep + '*.wav', )
    filesToProcess = []
    for files in types:
        filesToProcess.extend(glob.glob(files))
    for f in filesToProcess:
        outPath = f
        mtFeatureExtractionToFile(f, midTermSize, midTermStep, shortTermSize,
                                  shortTermStep, outPath, storeStFeatures,
                                  storeToCSV, PLOT)
71/5:
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
import matplotlib.pyplot as plt
71/6:
[Fs, x] = audioBasicIO.readAudioFile("../music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3");
F, f_names = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);
plt.subplot(2,1,1); plt.plot(F[0,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[0]); 
plt.subplot(2,1,2); plt.plot(F[1,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[1]); plt.show()
71/7:
from os import path
from pydub import AudioSegment
71/8:
# files                                                                         
src = "../music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3"
dst = "LUXO_u_got_me_runnin_OM.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")
71/9:
# files                                                                         
src = "music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3"
dst = "LUXO_u_got_me_runnin_OM.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")
71/10:
[Fs, x] = audioBasicIO.readAudioFile("music_data/ALLFRIENDS. - Luxo - U Got Me Runnin (Original Mix).mp3");
F, f_names = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);
plt.subplot(2,1,1); plt.plot(F[0,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[0]); 
plt.subplot(2,1,2); plt.plot(F[1,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[1]); plt.show()
71/11:
from os import path
from pydub import AudioSegment
import ffprobe
71/12:
from os import path
from pydub import AudioSegment
from ffprobe import FFProbe
71/13:
from os import path
from pydub import AudioSegment
from ffprobe import FFProbe
71/14:
from os import path
from pydub import AudioSegment
from .ffprobe import FFProbe
71/15:
from os import path
from pydub import AudioSegment.converter
71/16:
from os import path
from pydub import AudioSegment
71/17: apt-get install ffmpeg libavcodec-extra-53
72/1:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
72/2:
target_array = np.unique(genre_strings)
target_array
72/3:
# Import necessary modules
import numpy as np
72/4:
target_array = np.unique(genre_strings)
target_array
72/5:
target_num_array = np.unique(beats2['Genres'])
target_num_array
72/6:
# Import necessary modules
import pandas as pd
from sklearn.decomposition import PCA
72/7:
# Create a Principle Component instance with 2 principle components
pca = PCA(n_components=2)

# Fit to standardized data
principal_components = pca.fit_transform(X, y)
72/8:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
72/9:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
72/10:
# Import necessary modules 
from sklearn.model_selection import train_test_split
72/11:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
72/12:
# Create a scaler instance
scaler = StandardScaler()
72/13:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
72/14:
# Create a scaler instance
scaler = StandardScaler()
72/15:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
72/16:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
72/17:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
72/18:
# Determine number of components in model
ml_pca.n_components_
72/19:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
72/20:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
72/21:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
72/22:
# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
72/23:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
72/24:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
72/25:
# Import necessary modules
from sklearn.linear_model import LogisticRegressionGD
72/26:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
72/27:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', n_iter=1000, random_state=1)
72/28:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
72/29:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', n_iter=1000, random_state=1)
72/30:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=1000, random_state=1)
72/31:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
72/32:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
72/33:
# Import necessary modules
import numpy as np
72/34:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
72/35:
target_array = np.unique(genre_strings)
target_array
72/36:
target_num_array = np.unique(beats2['Genres'])
target_num_array
72/37:
# Create a dataframe for the principle components
principal_df = pd.DataFrame(data=principal_components, 
                            columns=['principal_component_1', 'principal_component_2'])
principal_df.head()
72/38:
# Import necessary modules
import numpy as np
import matplotlib.pyplot as plt
72/39:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
72/40:
target_array = np.unique(genre_strings)
target_array
72/41:
target_num_array = np.unique(beats2['Genres'])
target_num_array
72/42:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
72/43:
# Import necessary modules 
from sklearn.model_selection import train_test_split
72/44:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
72/45:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
72/46:
# Create a scaler instance
scaler = StandardScaler()
72/47:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
72/48:
# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
72/49:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
72/50:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
72/51:
# Determine number of components in model
ml_pca.n_components_
72/52:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
72/53:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
72/54:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
72/55:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=1000, random_state=1)
72/56:
# Fit model to data
log_reg.fit(X_train_scal_pca, y_train)
74/1:
#Import necessary modules
import pandas as pd
import numpy as np
74/2:
# Load cvs datasets
users = pd.read_csv('takehome_users.csv')
users.head()
74/3:
# Load cvs datasets
users = pd.read_csv('takehome_users.csv').decode('utf-8')
users.head()
74/4:
# Load cvs datasets
users_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_df.head()
74/5:
# Load cvs datasets
users_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_df.head()
74/6:
# Load cvs datasets
users_df = pd.read_csv('takehome_users.csv', encoding='utf-8', errors='backslashreplace')
users_df.head()
74/7:
# Load cvs datasets
with open('takehome_users.csv') as f:
    print(f)

users_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_df.head()
74/8:
# Load cvs datasets
with open('takehome_users.csv') as f:
    print(f)

#users_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
#users_df.head()
74/9:
# Load cvs datasets
with open('takehome_users.csv') as f:
    print(f)

users_df = pd.read_csv('takehome_users.csv', encoding='UTF-8')
users_df.head()
74/10:
# Load cvs datasets
with open('takehome_users.csv') as f:
    print(f)

users_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_df.head()
74/11:
# Load cvs datasets
with open('takehome_users.csv') as f:
    print(f)

users_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_df.head()
74/12:
# Load cvs datasets
users_engage_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_engage_df.head()
74/13:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_engage_df.head()
74/14:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('../takehome_users.csv', encoding='utf-8')
users_engage_df.head()
74/15:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv', encoding='utf-8')
users_engage_df.head()
74/16:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
74/17:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
74/18:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/19:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
74/20:
#### Clean Data 
- user_engage_df
- user_login_df
74/21: users_engage_df.describe()
74/22: users_engage_df.info()
74/23: users_login_df.info()
74/24:
print(len(users_engage_df))
users_engage_df.info()
74/25:
print(len('The length of user engage dataframe is: ', users_engage_df))
users_engage_df.info()
74/26:
print('The length of user engage dataframe is: ', len(users_engage_df))
users_engage_df.info()
74/27:
print('The length of user engage dataframe is: ', len(users_engage_df))
print('   ')
users_engage_df.info()
74/28:
print('The length of user login dataframe is: ', len(users_login_df))
print('   ')
users_login_df.info()
74/29: users_engage_df[['last_session_creation_time', 'invited_by_user_id']].head()
74/30:
# Show original contents
users_engage_df[['last_session_creation_time', 'invited_by_user_id']].head()
74/31: users_engage_df['invited_by_user_id'].mode(), users_engage_df['invited_by_user_id'].mean()
74/32:
# Use general values to fill null values
# mean for last_session
users_engage_df['last_session_creation_time'].fillna((users_engage_df['last_session_creation_time'].mean()), inplace=True)
# mode for user_id
users_engage_df['invited_by_user_id'].fillna((users_engage_df['invited_by_user_id'].mode()), inplace=True)

# Test output
users_engage_df.info()
74/33:
# Use general values to fill null values
# mean for last_session
users_engage_df['last_session_creation_time'].fillna((users_engage_df['last_session_creation_time'].mean()), inplace=True)
# mode for user_id
mode_user_id = users_engage_df['invited_by_user_id'].mode()
users_engage_df['invited_by_user_id'].fillna(mode_user_id, inplace=True)

# Test output
users_engage_df.info()
74/34:
# Use general values to fill null values
# mean for last_session
users_engage_df['last_session_creation_time'].fillna((users_engage_df['last_session_creation_time'].mean()), inplace=True)
# mode for user_id
mode_user_id = users_engage_df['invited_by_user_id'].mode()
users_engage_df['invited_by_user_id'].fillna(mode_user_id, inplace=True)

# Test output
users_engage_df.info()
74/35:
# Use general values to fill null values
# mean for last_session
users_engage_df['last_session_creation_time'].fillna((users_engage_df['last_session_creation_time'].mean()), inplace=True)
74/36:
# mode for user_id
mode_user_id = users_engage_df['invited_by_user_id'].mode()
users_engage_df['invited_by_user_id'].fillna(mode_user_id, inplace=True)
74/37:
# Test output
users_engage_df.info()
74/38:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    (users_engage_df['invited_by_user_id'].mean()), inplace=True))
74/39:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    (users_engage_df['invited_by_user_id'].mean()), inplace=True)
74/40:
# Test output
users_engage_df.info()
74/41:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    (users_engage_df['invited_by_user_id'].mode()), inplace=True)
74/42:
# Test output
users_engage_df.info()
74/43:
# Import necessary modules
from scipy import stats
74/44:
# mode for user_id
mode = stats.mode(users_engage_df['invited_by_user_id'])
users_engage_df['invited_by_user_id'].fillna(
    stats.mode((users_engage_df['invited_by_user_id']), inplace=True)
74/45:
# mode for user_id
mode = stats.mode(users_engage_df['invited_by_user_id'])
users_engage_df['invited_by_user_id'].fillna(
    stats.mode(users_engage_df['invited_by_user_id'], inplace=True))
74/46:
# mode for user_id
mode = stats.mode(users_engage_df['invited_by_user_id'])
users_engage_df['invited_by_user_id'].fillna(
    stats.mode(users_engage_df['invited_by_user_id']), inplace=True)
74/47: stats.mode(users_engage_df['invited_by_user_id'])
74/48: users_engage_df['invited_by_user_id'].mode()
74/49: users_engage_df['invited_by_user_id'].mode().value
74/50: users_engage_df['invited_by_user_id'].mode().value()
74/51: users_engage_df['invited_by_user_id'].mode().values
74/52: users_engage_df['invited_by_user_id'].mode()
74/53:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
74/54:
print('The length of user engage dataframe is: ', len(users_engage_df))
print('   ')
users_engage_df.info()
74/55:
# Show sample of columns with null values
users_engage_df[['last_session_creation_time','invited_by_user_id']].head()
74/56:
# mean for last_session
users_engage_df['last_session_creation_time'].fillna(
    (users_engage_df['last_session_creation_time'].mean()), inplace=True)
74/57: users_engage_df['invited_by_user_id'].mode()
74/58:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    (users_engage_df['invited_by_user_id'].mode()), inplace=True)
74/59:
# Test output
users_engage_df.info()
74/60:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    users_engage_df['invited_by_user_id'].mode()[0])
74/61:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(users_engage_df['invited_by_user_id'].mode()[0])

# Test output
users_engage_df.info()
74/62:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(users_engage_df['invited_by_user_id'].mode()[0], inplace=True)

# Test output
users_engage_df.info()
74/63: users_engage_df['invited_by_user_id'].mode()[0]
74/64:
# Import necessary modules
from datetime import datetime
74/65:
# Show original format
user_engage_df['last_session_creation_time'].head()
74/66:
# Show original format
users_engage_df['last_session_creation_time'].head()
74/67:
# if you encounter a "year is out of range" error the timestamp
# may be in milliseconds, try `ts /= 1000` in that case
users_engage_df['last_session_creation_time2'] = datetime.utcfromtimestamp(users_engage_df['last_session_creation_time']).strftime('%Y-%m-%d %H:%M:%S'))
users_engage_df['last_session_creation_time2'].head()
74/68:
# if you encounter a "year is out of range" error the timestamp
# may be in milliseconds, try `ts /= 1000` in that case
users_engage_df['last_session_creation_time2'] = datetime.utcfromtimestamp(users_engage_df['last_session_creation_time']).strftime('%Y-%m-%d %H:%M:%S')
users_engage_df['last_session_creation_time2'].head()
74/69:
# if you encounter a "year is out of range" error the timestamp
# may be in milliseconds, try `ts /= 1000` in that case
users_engage_df['last_session_creation_time2'] = pd.to_datetime(
    users_engage_df['last_session_creation_time'], unit='s')
74/70:
# if you encounter a "year is out of range" error the timestamp
# may be in milliseconds, try `ts /= 1000` in that case
users_engage_df['last_session_creation_time2'] = pd.to_datetime(
    users_engage_df['last_session_creation_time'], unit='s')
users_engage_df['last_session_creation_time2'].head()
74/71:
# Convert column from unix timestamp to datetime
users_engage_df['last_session_creation_time'] = pd.to_datetime(
    users_engage_df['last_session_creation_time'], unit='s')

# Test output
users_engage_df['last_session_creation_time'].head()
74/72:
users_engage_df = users_engage_df.groupby(by='object_id')
users_engage_df.head()
74/73:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/74:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
74/75: users_engage_df['object_id'].unique()
74/76: len(users_engage_df['object_id'].unique())
74/77:
# Show original format
users_engage_df['last_session_creation_time'].head()
74/78:
# Convert column from unix timestamp to datetime
users_engage_df['last_session_creation_time'] = pd.to_datetime(
    users_engage_df['last_session_creation_time'], unit='s')

# Test output
users_engage_df['last_session_creation_time'].head()
74/79: user_login_df['time_stamp'].head()
74/80: users_login_df['time_stamp'].head()
74/81:
users_login_df_1D = users_login_df.resample(rule='1D').sum()
login_data_1D.head()
74/82:
users_login_df_1D = users_login_df.resample(rule='D').sum()
login_data_1D.head()
74/83: users_login_df.head()
74/84: users_login_df.resample('D').mean()
74/85:
# Set index to datetime column
users_login_df.set_index('time_stamp', inplace=True)
login_data.head()
74/86:
# Set index to datetime column
users_login_df.set_index('time_stamp', inplace=True)
users_login_df.head()
74/87:
# Set index to datetime column
users_login_df.set_index(' time_stamp', inplace=True)
users_login_df.head()
74/88:
# Set index to datetime column
users_login_df.set_index(' time_stamp ', inplace=True)
users_login_df.head()
74/89:
# Set index to datetime column
users_login_df.set_index('time_stamp', inplace=True)
users_login_df.head()
74/90: users_login_df.head()
74/91:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/92:
print('The length of user login dataframe is: ', len(users_login_df))
print('   ')
users_login_df.info()
74/93: users_login_df.head()
74/94:
# Set index to datetime column
users_login_df.set_index('time_stamp', inplace=True)
users_login_df.head()
74/95:
# Group entries by 1 day intervals
users_login_df.resample('D').mean()
74/96:
# Group entries by 1 day intervals
users_login_df.resample('D').size()
74/97:
# Group entries by 1 day intervals
users_login_df.resample('D', on='time_stamp').mean()
74/98:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/99:
# Group entries by 1 day intervals
users_login_df.resample('D', on='time_stamp').mean()
74/100:
# Set index to datetime column
users_login_df.set_index('time_stamp', inplace=True)
users_login_df.head()
74/101:
# Group entries by 1 day intervals
users_login_df.resample('D', on='time_stamp').mean()
74/102:
# Group entries by 1 day intervals
users_login_df.resample('W', on='time_stamp').mean()
74/103:
# Group entries by 1 day intervals
users_login_df.resample('W').mean()
74/104:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/105: users_login_df.head()
74/106:
# Group entries by 1 day intervals
users_login_df.groupby(users_login_df['time_stamp'].dt.week).count()
74/107:
# Group entries by 1 day intervals
users_login_df.groupby(pd.Grouper(freq='W'))
74/108:
# Group entries by 1 day intervals
users_login_df.groupby(pd.TimeGrouper(freq='W')).sum()
74/109:
# Group entries by 1 day intervals
users_login_df.groupby(pd.Grouper(freq='W')).sum()
74/110:
# Group entries by 1 week intervals
users_login_df.reset_index().set_index('time_stamp').resample('1W').sum()
74/111:
# Group entries by 1 week intervals

users_login_df['time_stamp'] = users_login_df['time_stamp'] - pd.to_timedelta(7, unit='d')
74/112: # Group entries by 1 week intervals
74/113:
# Group entries by 1 week intervals
users_login_df.groupby('time_stamp').resample(
    'W-Mon', on='time_stamp').sum().reset_index().sort_values(by='time_stamp')
74/114:
# Group entries by 1 week intervals
users_login_df.groupby('time_stamp').resample(
    'W', on='time_stamp').sum().reset_index().sort_values(by='time_stamp')
74/115:
# Group entries by 1 week intervals
test_df = users_login_df.set_index('time_stamp').groupby('user_id').resample('1W')['visited'].ffill()
74/116: test_df = users_login_df.groupby(by='user_id')
74/117:
test_df = users_login_df.groupby(by='user_id')
test_df
74/118:
test_df = users_login_df.groupby(by='user_id')
test_df.head()
74/119:
test_df = users_login_df.groupby(by='time_stamp')
test_df.head()
74/120:
test_df = users_login_df.groupby(by='time_stamp').resample('W')['visited']
test_df.head()
74/121: users_login_df['time_stamp'].dtype
74/122:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'], unit='s')

# Test output
users_login_df['time_stamp'].head()
74/123:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/124:
test_df = users_login_df.groupby(by='time_stamp').resample('W')['visited']
test_df.head()
74/125:
test_df = users_login_df.groupby(by='time_stamp').resample('W')['visited']
test_df.head()

users_login_df['time_stamp2'] = pd.to_datetime(users_login_df['time_stamp']) - pd.to_timedelta(7, unit='d')
74/126:
test_df = users_login_df.groupby(by='time_stamp').resample('W')['visited']
test_df.head()
74/127:
test_df = users_login_df.groupby(by='time_stamp')
test_df.head()
74/128:
test_df = users_login_df.groupby(by='user_id')
test_df.head()
74/129: users_login_df.groupby('user_id').resample('W-Mon', on='time_stamp').sum().reset_index().sort_values(by='user_id')
74/130:
users_login_df = users_login_df.groupby(
    'user_id').resample('W-Mon', on='time_stamp'
                       ).sum().reset_index().sort_values(by='user_id')
74/131:
users_login_df = users_login_df.groupby(
    'user_id').resample('W-Mon', on='time_stamp'
                       ).sum().reset_index()
74/132:
users_login_df['users'] = users_login_df.groupby(
    'user_id').resample('W-Mon', on='time_stamp'
                       ).sum().reset_index()



users_login_df['users'] = pd.to_datetime(users_login_df['time_stamp']) - pd.to_timedelta(7, unit='d')
test_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W-MON')])['visited'].sum().reset_index()
print(tes_df)
74/133:
users_login_df['users'] = pd.to_datetime(users_login_df['time_stamp']) - pd.to_timedelta(7, unit='d')
test_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W-MON')])['visited'].sum().reset_index()
print(tes_df)
74/134:
users_login_df['users'] = pd.to_datetime(users_login_df['time_stamp']) - pd.to_timedelta(7, unit='d')
test_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W-MON')])['visited'].sum().reset_index()
print(test_df)
74/135:
users_login_df['users'] = pd.to_datetime(users_login_df['time_stamp']) - pd.to_timedelta(7, unit='d')
test_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W')])['visited'].sum().reset_index()
print(test_df)
74/136:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W')])['visited'].sum().reset_index()
print(users_login_df.head())
74/137:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='W')])['visited'].sum().reset_index()
users_login_df.head()
74/138:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/139:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/140:
# Convert column from unix timestamp to datetime
users_engage_df['last_session_creation_time'] = pd.to_datetime(
    users_engage_df['last_session_creation_time'], unit='s')

# Test output
users_engage_df['last_session_creation_time'].head()
74/141:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/142:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().reset_index()
users_login_df.head()
74/143:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().reset_index()
users_login_df.head(), users_login_df.tail()
74/144:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().reset_index().sort_values(by='time_stamp')
users_login_df.head()
74/145:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().sort_values(by='time_stamp').reset_index()
users_login_df.head()
74/146:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/147:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/148:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().sort_values(by='time_stamp').reset_index()
users_login_df.head()
74/149:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().sort_values('time_stamp').reset_index()
users_login_df.head()
74/150:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().reset_index().sort_values(by='time_stamp')
users_login_df.head()
74/151:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().sort_values(by='time_stamp')
users_login_df.head()
74/152:
users_login_df = users_login_df.groupby(
    ['user_id', pd.Grouper(key='time_stamp', freq='7D')])['visited'].sum().reset_index()
users_login_df.head()
74/153:
# Floor by Days
users_login_df['time_stamp'] = users_login_df['time_stamp'].dt.floor('d').astype(np.int64)
users_login_df.head()
74/154:
# Sort by User then Drop Duplicates 
users_login_df = users_login_df.sort_values(['user_id', 'time_stamp']).drop_duplicates()
users_login_df.head()
74/155:
a = df.groupby('user_id')['time_stamp'].rolling(window=3)
b = pd.to_timedelta((a.max()- a.min())).dt.days
print (b)
74/156:
a = users_login_df.groupby('user_id')['time_stamp'].rolling(window=3)
b = pd.to_timedelta((a.max()- a.min())).dt.days
print (b)
74/157:
a = users_login_df.groupby('user_id')['time_stamp'].rolling(window=3)
b = pd.to_timedelta((a.max()- a.min())).dt.days
b.head()
74/158:
c = b[b == 7].index.get_level_values('user_id').tolist()
c.head()
74/159:
c = b[b == 7].index.get_level_values('user_id').tolist()
c
74/160:
c = b[b >= 7].index.get_level_values('user_id').tolist()
c
74/161:
c = b[b >= 7].index.get_level_values('user_id').unique().tolist()
c
74/162:
# Groupby with Rolling by each 3 rows
num_logins_df = users_login_df.groupby('user_id')['time_stamp'].rolling(window=3)
num_logins_df.head()
74/163:
# Groupby with Rolling by each 3 rows
num_logins_df = users_login_df.groupby('user_id')['time_stamp'].rolling(window=3)
num_logins_df
74/164:
user_login_timedelta = pd.to_timedelta((num_logins_df.max()- num_logins_df.min())).dt.days
user_login_timedelta.head()
74/165:
retained_users = user_login_timedelta[user_login_timedelta >= 7].index.get_level_values('user_id').unique().tolist()
retained_users
74/166:
retained_users = user_login_timedelta[user_login_timedelta >= 7].index.get_level_values('user_id').unique().tolist()
#retained_users
74/167:
users_login_df['wk'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
74/168:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/169:
users_login_df['wk'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
74/170: users_login_df = users_login_df.groupby(['wk','user_id'])['ime_stamp'].count().reset_index(name="freq")
74/171: users_login_df = users_login_df.groupby(['wk','user_id'])['time_stamp'].count().reset_index(name="freq")
74/172:
users_login_df = users_login_df.groupby(
    ['wk','user_id'])['time_stamp'].count().reset_index(name="freq")
users_login_df.head()
74/173:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/174:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/175:
users_login_df['wk'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
74/176:
users_login_df = users_login_df.groupby(
    ['wk','user_id'])['time_stamp'].count().reset_index(name="freq")
users_login_df.head()
74/177:
users_login_df['week'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
74/178:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
74/179:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
74/180:
users_login_df['week'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
74/181:
users_login_df = users_login_df.groupby(
    ['week','user_id'])['time_stamp'].count().reset_index(name="visited")
users_login_df.head()
74/182:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.freq >= 3].tolist()
74/183:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3].tolist()
74/184:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3]
74/185:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3]
adopted_users.head()
74/186:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3]
adopted_users = adopted_users['user_id'].tolist()
74/187:
user_adopted = []

for user in user_login_df['object_id']:
    
    if adopted_users.contains(user):
        print(True)
    else:
        print(False)
74/188:
user_adopted = []

for user in users_login_df['object_id']:
    
    if adopted_users.contains(user):
        print(True)
    else:
        print(False)
74/189: users_engage_df.head()
74/190:
user_adopted = []

for user in users_engage_df['object_id']:
    
    if adopted_users.contains(user):
        print(True)
    else:
        print(False)
74/191:
user_adopted = []

for user in users_engage_df['object_id']:
    
    if user in adopted_users:
        print(True)
    else:
        print(False)
74/192:
user_adopted = []

for user in users_engage_df['object_id']:
    
    if user in adopted_users:
        user_adopted.append(True)
    else:
        user_adopted.append(False)
74/193:
# Add adopted user information to users_engage_df
users_engage_df['user_adopted'] = pd.Series(user_adopted)
users_engage_df.tail()
75/1:
#Import necessary modules
import pandas as pd
import numpy as np
75/2:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
75/3:
print('The length of user login dataframe is: ', len(users_login_df))
print('   ')
users_login_df.info()
75/4:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
75/5:
print('The length of user engage dataframe is: ', len(users_engage_df))
print('   ')
users_engage_df.info()
75/6:
# Show sample of columns with null values
users_engage_df[['last_session_creation_time','invited_by_user_id']].head()
75/7:
# mean for last_session
users_engage_df['last_session_creation_time'].fillna(
    (users_engage_df['last_session_creation_time'].mean()), inplace=True)
75/8:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    users_engage_df['invited_by_user_id'].mode()[0], inplace=True)

# Test output
users_engage_df.info()
75/9:
# Import necessary modules
from datetime import datetime
75/10:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
75/11:
# Group timestamp by week and create new week column
users_login_df['week'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
75/12:
# Group user_login_df by ['user_id'] and ['week']
users_login_df = users_login_df.groupby(
    ['week','user_id'])['time_stamp'].count().reset_index(name="visited")
users_login_df.head()
75/13:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3]
adopted_users = adopted_users['user_id'].tolist()
75/14: users_engage_df.head()
75/15:
user_adopted = []

for user in users_engage_df['object_id']:
    
    if user in adopted_users:
        user_adopted.append(True)
    else:
        user_adopted.append(False)
75/16:
# Add adopted user information to users_engage_df
users_engage_df['user_adopted'] = pd.Series(user_adopted)
users_engage_df.tail()
75/17:
# Drop unnecessary columns
users_engage_df = users_engage_df.drop(columns=['name','email'])
75/18: %store users_engage_df
77/1:
#Import necessary modules
import pandas as pd
import numpy as np
76/1:
#Import necessary modules
import pandas as pd
import numpy as np
77/2:
# Load cvs 1 dataset
users_login_df = pd.read_csv('takehome_user_engagement.csv', encoding='utf-8')
users_login_df.head()
77/3:
print('The length of user login dataframe is: ', len(users_login_df))
print('   ')
users_login_df.info()
77/4:
# Load cvs 2 dataset
users_engage_df = pd.read_csv('takehome_users.csv')
users_engage_df.head()
77/5:
print('The length of user engage dataframe is: ', len(users_engage_df))
print('   ')
users_engage_df.info()
77/6:
# Show sample of columns with null values
users_engage_df[['last_session_creation_time','invited_by_user_id']].head()
77/7:
# mean for last_session
users_engage_df['last_session_creation_time'].fillna(
    (users_engage_df['last_session_creation_time'].mean()), inplace=True)
77/8:
# mode for user_id
users_engage_df['invited_by_user_id'].fillna(
    users_engage_df['invited_by_user_id'].mode()[0], inplace=True)

# Test output
users_engage_df.info()
77/9:
# Import necessary modules
from datetime import datetime
77/10:
# Convert column to datetime
users_login_df['time_stamp'] = pd.to_datetime(
    users_login_df['time_stamp'])

# Test output
users_login_df['time_stamp'].head()
77/11:
# Group timestamp by week and create new week column
users_login_df['week'] = users_login_df['time_stamp'].dt.week
users_login_df.head()
77/12:
# Group user_login_df by ['user_id'] and ['week']
users_login_df = users_login_df.groupby(
    ['week','user_id'])['time_stamp'].count().reset_index(name="visited")
users_login_df.head()
77/13:
# Create list of adopted_users
adopted_users = users_login_df[users_login_df.visited >= 3]
adopted_users = adopted_users['user_id'].tolist()
77/14: users_engage_df.head()
77/15:
user_adopted = []

for user in users_engage_df['object_id']:
    
    if user in adopted_users:
        user_adopted.append(1)
    else:
        user_adopted.append(0)
77/16:
# Add adopted user information to users_engage_df
users_engage_df['user_adopted'] = pd.Series(user_adopted)
users_engage_df.tail()
77/17:
# Drop unnecessary columns
users_engage_df = users_engage_df.drop(columns=['name','email'])
77/18: %store users_engage_df
76/2:
# Import data
%store -r users_engage_df
76/3: users_engage_df.head()
76/4:
for i, column in enumerate(list([str(d) for d in users_engage_df.dtypes])):
    if column == "object":
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].fillna(users_engage_df[users_engage_df.columns[i]].mode())
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].astype("category").cat.codes
    else:
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].fillna(users_engage_df[users_engage_df.columns[i]].median())
76/5:
# Test modified output
users_engage_df.head()
76/6:
# Make array of target variable: user_adopted
target = np.array(users_engage_df['user_adopted'])

# Remove target from features
features = users_engage_df.drop('user_adopted', axis = 1)

# Save feature column labels in list
feature_list = list(features.columns)

# Remove column names and place feature in combined array
features = np.array(features)
76/7:
# Import necessary modules
from sklearn.model_selection import train_test_split
76/8:
# Split data into training and testing sets
train_features, test_features, train_target, test_target = train_test_split(
    features, target, test_size = 0.25, random_state = 42)
76/9:
# Split data into training and testing sets
train_features, test_features, train_target, test_target = train_test_split(
    features, target, test_size = 0.25, random_state = 42)

print('Training Features Shape:', train_features.shape)
print('Training Labels Shape:', train_target.shape)
print('Testing Features Shape:', test_features.shape)
print('Testing Labels Shape:', test_target.shape)
76/10:
# The baseline predictions are the historical averages
baseline_predictions = test_features[:, feature_list.index('last_session_creation_time')]

# Baseline errors, and display average baseline error
baseline_errors = abs(baseline_predictions - test_target)
print('Average baseline error: ', round(np.mean(baseline_errors), 2))
Average baseline error:  5.06 degrees.
76/11:
# The baseline predictions are the historical averages
baseline_predictions = test_features[:, feature_list.index('last_session_creation_time')]

# Baseline errors, and display average baseline error
baseline_errors = abs(baseline_predictions - test_target)
print('Average baseline error: ', round(np.mean(baseline_errors), 2))
76/12:
# Import the model we are using
from sklearn.ensemble import RandomForestRegressor
76/13:
# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)

# Train the model on training data
rf.fit(train_features, train_target)
76/14:
# Use the forest's predict method on the test data
predictions = rf.predict(test_features)

# Calculate the absolute errors
errors = abs(predictions - test_target)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
76/15:
# Use the forest's predict method on the test data
predictions = rf.predict(test_features)

# Calculate the absolute errors
errors = abs(predictions - test_target)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
76/16:
# Use the forest's predict method on the test data
predictions = rf.predict(test_features)

# Calculate the absolute errors
errors = abs(predictions - test_target)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
76/17:
# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / test_target)

# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')
76/18:
# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / test_target)

# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')
76/19:
# Import tools needed for visualization
from sklearn.tree import export_graphviz
import pydot
76/20:
# Import tools needed for visualization
from sklearn.tree import export_graphviz
76/21:
# Pull out one tree from the forest
tree = rf.estimators_[5]

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')

# Write graph to a png file
graph.write_png('tree.png')
76/22:
# Import necessary modules
from sklearn.tree import export_graphviz
import pydot
76/23:
# Pull out one tree from the forest
tree = rf.estimators_[5]

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')

# Write graph to a png file
graph.write_png('tree.png')
76/24:
# Pull out one tree from the forest
tree = rf.estimators_[5]

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file='tree.dot', feature_names=feature_list, rounded=True, precision=1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')
76/25:
# Write graph to a png file
graph.write_png('tree.png')
76/26:
# Write graph to a png file
graph
76/27:
# Write graph to a png file
graph.write_png('tree.png')
76/28:
# Write graph to a png file
Image(graph.create_png())
76/29:
# Import necessary modules
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydot
76/30:
# Write graph to a png file
Image(graph.create_png())
78/1:
#Import necessary modules
import pandas as pd
import numpy as np
78/2:
# Import data
%store -r users_engage_df
78/3:
# Show original format
users_engage_df.head(2)
78/4:
# Convert categorical to numerical
for i, column in enumerate(list([str(d) for d in users_engage_df.dtypes])):
    if column == "object":
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].fillna(users_engage_df[users_engage_df.columns[i]].mode())
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].astype("category").cat.codes
    else:
        users_engage_df[users_engage_df.columns[i]] = users_engage_df[users_engage_df.columns[i]].fillna(users_engage_df[users_engage_df.columns[i]].median())
78/5:
# Test modified output
users_engage_df.head(2)
78/6:
# Make array of target variable: user_adopted
target = np.array(users_engage_df['user_adopted'])

# Remove target from features
features = users_engage_df.drop('user_adopted', axis = 1)

# Save feature column labels in list
feature_list = list(features.columns)

# Remove column names and place feature in combined array
features = np.array(features)
78/7:
# Import necessary modules
from sklearn.model_selection import train_test_split
78/8:
# Split data into training and testing sets
train_features, test_features, train_target, test_target = train_test_split(
    features, target, test_size = 0.25, random_state = 42)

print('Training Features Shape:', train_features.shape)
print('Training Labels Shape:', train_target.shape)
print('Testing Features Shape:', test_features.shape)
print('Testing Labels Shape:', test_target.shape)
78/9:
# The baseline predictions are the historical averages
baseline_predictions = test_features[:, feature_list.index('last_session_creation_time')]

# Baseline errors, and display average baseline error
baseline_errors = abs(baseline_predictions - test_target)
print('Average baseline error: ', round(np.mean(baseline_errors), 2))
78/10:
# Import the model we are using
from sklearn.ensemble import RandomForestRegressor
78/11:
# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators=1000, random_state=42)

# Train the model on training data
rf.fit(train_features, train_target)
78/12:
# Use the forest's predict method on the test data
predictions = rf.predict(test_features)

# Calculate the absolute errors
errors = abs(predictions - test_target)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
78/13:
# Import necessary modules
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydot
78/14:
# Pull out one tree from the forest
tree = rf.estimators_[5]

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file='tree.dot', feature_names=feature_list, rounded=True, precision=1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')
78/15:
# Write graph to a png file
Image(graph.create_png())
78/16:
# Pull out one tree from the forest
tree = rf.estimators_[5]

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file='tree.dot', feature_names=feature_list, rounded=True, precision=1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')
78/17:
# Write graph to a png file
Image(graph.create_png())
78/18:
# Write graph to a png file
Image(graph.create_png())
78/19:
# Write graph to a png file
Image(graph.create_png())
78/20:
# Limit depth of tree to 3 levels
rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)
rf_small.fit(train_features, train_labels)

# Extract the small tree
tree_small = rf_small.estimators_[5]

# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
78/21:
# Limit depth of tree to 3 levels
rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)
rf_small.fit(train_features, train_target)

# Extract the small tree
tree_small = rf_small.estimators_[5]

# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
78/22:
# Write graph to a png file
Image(graph.create_png())
78/23:
# Get numerical feature importances
importances = list(rf.feature_importances_)

# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]

# Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)

# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];
81/1:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGB(object):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
80/1:
# Import necessary modules
import numpy as np
import matplotlib.pyplot as plt
80/2:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
80/3:
target_array = np.unique(genre_strings)
target_array
80/4:
target_num_array = np.unique(beats2['Genres'])
target_num_array
80/5:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
80/6:
# Import necessary modules 
from sklearn.model_selection import train_test_split
80/7:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
80/8:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
80/9:
# Create a scaler instance
scaler = StandardScaler()
80/10:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
80/11:
# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
80/12:
# Make an instance of the Model
ml_pca = PCA(n_components=2)
80/13:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
80/14:
# Determine number of components in model
ml_pca.n_components_
80/15:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
80/16:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
80/17:
# Import necessary modules
from gradient_logistic_regression.ipynb import LogisticRegressionGB
80/18:
# Import necessary modules
import gradient_logistic_regression.ipynb
80/19:
# Import necessary modules
import gradient_logistic_regression
80/20:
# Import necessary modules
%run gradient_logistic_regression.ipynb
80/21:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegressionGB(solver='lbfgs', multi_class='auto', n_jobs=1000, random_state=1)
82/1:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
80/22:
# Import necessary modules
%run gradient_logistic_regression.ipynb
80/23:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
80/24:
# Default solver is incredibly slow which is why it was changed to 'lbfgs'
log_reg = LogisticRegressionGD()
80/25:
# Create a logistic regression instance that uses gradient descent
log_reg = LogisticRegressionGD()

# Add parameters
log_reg = _init_(log_reg, eta=0.5, n_iter=1000, random_state=42)
80/26:
# Create a logistic regression instance that uses gradient descent
log_reg = LogisticRegressionGD()

# Add parameters
log_reg = log_reg._init_(log_reg, eta=0.5, n_iter=1000, random_state=42)
82/2:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
80/27:
# Import necessary modules
%run gradient_logistic_regression.ipynb
80/28:
# Create a logistic regression instance that uses gradient descent
log_reg = LogisticRegressionGD()

# Add parameters
log_reg = _init_(log_reg, eta=0.5, n_iter=1000, random_state=42)
80/29:
# Import necessary modules
%run gradient_logistic_regression.ipynb
80/30:
# Create a logistic regression instance that uses gradient descent
log_reg = LogisticRegressionGD()

# Add parameters
log_reg = _init_(log_reg, eta=0.5, n_iter=1000, random_state=42)
80/31:
# Create a logistic regression instance that uses gradient descent
log_reg = LogisticRegressionGD(log_reg, eta=0.5, n_iter=1000, random_state=42)
80/32:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
80/33:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/1:
# Import necessary modules
import numpy as np
import matplotlib.pyplot as plt
83/2:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
83/3:
target_array = np.unique(genre_strings)
target_array
83/4:
target_num_array = np.unique(beats2['Genres'])
target_num_array
83/5:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Separate out the target
y = beats2['Genres'].values
83/6:
# Import necessary modules 
from sklearn.model_selection import train_test_split
83/7:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
83/8:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
83/9:
# Create a scaler instance
scaler = StandardScaler()
83/10:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
83/11:
# Show Training and Testing Data Set Shapes
print('Training (X) Features Shape:', X_train_scaled.shape)
print('Training (y) Target Shape:', y_train.shape)
print('Testing (X) Features Shape:', X_test_scaled.shape)
print('Testing (y) Target Shape:', y_test.shape)
83/12:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Save feature column labels in list
feature_list = list(X.columns)

# Separate out the target
y = beats2['Genres'].values
83/13:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Save feature column labels in list
feature_list = list(X.columns)

# Remove column names and place feature in combined array
X = np.array(features)

# Separate out the target
y = beats2['Genres'].values
83/14:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Save feature column labels in list
feature_list = list(X.columns)

# Separate out the target
y = beats2['Genres'].values
83/15:
# Import necessary modules
from sklearn.ensemble import RandomForestRegressor
83/16:
# Instantiate model with 1000 decision trees and max depth of 3
rf = RandomForestRegressor(n_estimators=1000, random_state=42, max_depth=3)

# Train the model on training data
rf.fit(X_train_scaled, y_train)
83/17:
# Use the forest's predict method on the test data
predictions = rf.predict(X_test_scaled)

# Calculate the absolute errors
errors = abs(predictions - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
83/18:
# Import necessary modules
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydot
83/19:
# Extract the small tree
tree_small = rf_small.estimators_[5]

# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
83/20:
# Extract the small tree
tree_small = rf.estimators_[5]

# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
83/21:
# Write graph to a png file
Image(graph.create_png())
83/22:
# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
83/23:
# Make an instance of the Model with 2 principal components
ml_pca = PCA(n_components=2)
83/24:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
83/25:
# Determine number of components in model
ml_pca.n_components_
83/26:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
83/27:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 2, 1), ('PCA1', 'PCA2'))
plt.show()
83/28:
# Make an instance of the Model undefined number of principle components
ml_pca = PCA()
83/29:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
83/30:
# Determine number of components in model
ml_pca.n_components_
83/31:
# Make an instance of the Model with 4 principle components, as shown in the Decision Tree
ml_pca = PCA(n_components=4)
83/32:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
83/33:
# Determine number of components in model
ml_pca.n_components_
83/34:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
83/35:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=1, height=pca3)
pl4 = plt.bar(x=1, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
83/36:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
83/37:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1, label=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 4, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4'))
plt.show()
83/38:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
print('Explained Variance Ratio:')
print('PCA1: ', pca1)
print('PCA2: ', pca2)
print('PCA3: ', pca3)
print('PCA4: ', pca4)
83/39:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/40:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD()
83/41:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD()

log_reg_gd = _init_(eta=0.5, n_iter=1000, random_state=42)
83/42:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD()

log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
83/43:
# Fit model to data
log_reg_gd.fit(X_train_scal_pca, y_train)
83/44:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/45:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/46:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/47:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD()

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
83/48:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/49:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
84/1:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/50:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/51:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
84/2:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD():
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/52:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/53:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
84/3:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD:
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/54:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/55:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
83/56:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD:
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/57:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
83/58:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/59:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
83/60:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(eta=0.05, n_iter=100, random_state=1):
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def _init_(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
84/4:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD:
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def __init_subclass__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/61:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/62:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)

# Initialize desired iterations and random state
#log_reg_gd = log_reg_gd._init_(eta=0.5, n_iter=1000, random_state=42)
84/5:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD:
    """Logistic Regression Classifier using gradient descent.
    
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    
    """
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/63:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/64:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/65:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
84/6:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/66:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/67:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/68:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/69:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
84/7:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = np.dot(X, self.w_[1:]) + self.w_[0]
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
        
        def net_input(self, X):
            """Calculate net input"""
            return np.dot(X, self.w_[1:]) + self.w_[0]
        
        def activation(self, z):
            """Compute logistic sigmoid activation"""
            return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/70:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/71:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
84/8:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self

        
        def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
83/72:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/73:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/74:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/75:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/76:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/77:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/78:
# Plot decision regions
plot_decision_regions(X=X_scal_pca_train_01_subset, y=y_train_01_subset classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/79:
# Plot decision regions
plot_decision_regions(X=X_scal_pca_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
84/9:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/80:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/81:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/82:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/83:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/84:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/85:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/86:
# Plot decision regions
plot_decision_regions(X=X_scal_pca_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/87:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/88:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/89:
# Define subsets of target and feature training data
X_scal_pca_train_01_subset = X_train_scal_pca[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/90:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/91:
# Plot decision regions
plot_decision_regions(X=X_scal_pca_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/92:
# Make an instance of the Model with 2 principle components
ml_pca2 = PCA(n_components=2)

# Map (transform) to both the training set and the test set
X_train_scal_pca2 = ml_pca2.fit_transform(X_train_scaled)
X_test_scal_pca2 = ml_pca2.transform(X_test_scaled)
83/93:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/94:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/95:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
83/96:
# Fit model to data
log_reg_gd.fit(X_scal_pca_train_01_subset, y_train_01_subset)
83/97:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/98:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/99:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/100:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/101:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/102:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/103:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/104:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/105:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/106:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
plt.legend(loc='upper left')
83/107:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/108:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg.predict(X_test_scal_pca2[0:10])
83/109:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/110:
# Get rough estimate of model accuracy score
log_reg_gd.score(X_test_scal_pca2, y_test)
83/111:
# Predict for One Observation
print(tree.predict(X_test_scaled[0].reshape(1,-1)))

# Predict for One Observation
tree.predict(X_test_scaled[0:10])
83/112:
# Predict for One Observation
print(rf.predict(X_test_scaled[0].reshape(1,-1)))

# Predict for One Observation
rf.predict(X_test_scaled[0:10])
83/113:
# Import necessary modules
from sklearn.metrics import roc_curve
83/114:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
prediction
83/115:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score
83/116:
# Get rough estimate of model accuracy score
train_score = accuracy_score(X_train_scal_pca2, y_train)
test_score = accuracy_score(X_test_scal_pca2, y_test)
train_score, test_score
84/10:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    def log_likelihood(self, X, y, weights):
        """Calculate Log-Likelihood"""
        weights = np.zeros(X.shape[1])
        scores = np.dot(X, weights)
        likelihood = np.sum(y*scores - np.log(1 + np.exp(scores)))
        return likelihood
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
84/11:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    def log_likelihood(self, X, y):
        """Calculate Log-Likelihood"""
        weights = np.zeros(X.shape[1])
        scores = np.dot(X, weights)
        likelihood = np.sum(y*scores - np.log(1 + np.exp(scores)))
        return likelihood
    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/117:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/118:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/119:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/120:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/121:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/122:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
prediction
83/123:
# Get rough estimate of model accuracy score
train_score = log_reg_gd.log_likelihood(X_train_scal_pca2, y_train)
test_score = log_reg_gd.log_likelihood(X_test_scal_pca2, y_test)
train_score, test_score
83/124:
# Computing false and true positive rates
false_pr, true_pr, _ = roc_curve(prediction, y_test, drop_intermediate=False)
83/125:
# Compute ROC curve and ROC area for each class
fpr = {}
tpr = {}
roc_auc = {}
for i in range(feature_list):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], test_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
83/126:
# Compute ROC curve and ROC area for each class
fpr = {}
tpr = {}
roc_auc = {}
for i in range(len(feature_list)):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], test_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
83/127:
# Plot ROC curve
plt.figure()
lw = 2
plt.plot(false_pr, true_pr, color='darkorange', label='ROC curve (area = %0.2f)')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
83/128:
# Computing false and true positive rates
false_pr, true_pr, _ = roc_curve(prediction, X_test, drop_intermediate=False)
83/129:
# Computing false and true positive rates
false_pr, true_pr, _ = roc_curve(prediction, y_test, drop_intermediate=False)
83/130:
# Plot ROC curve
plt.figure()
lw = 2
plt.plot(false_pr, true_pr, color='darkorange', label='ROC curve (area = %0.2f)')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
83/131:
# Get rough estimate of model accuracy score
train_score4 = log_reg_gd.log_likelihood(X_train_scal_pca, y_train)
test_score4 = log_reg_gd.log_likelihood(X_test_scal_pca, y_test)
train_score4, test_score4
83/132:
# Predict
prediction4 = log_reg_gd.predict(X_test_scal_pca)
prediction4
83/133:
# Plot ROC curve
plt.figure()
lw = 2
plt.plot(false_pr, true_pr, color='darkorange', label='ROC curve (area = %0.2f)')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
83/134:
# Computing false and true positive rates
false_pr, true_pr, thresholds = roc_curve(prediction, y_test, drop_intermediate=False)
83/135:
# Plot ROC curve
plt.figure()
lw = 2
plt.plot(false_pr, true_pr, color='darkorange', label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
83/136:
# Plot ROC curve
plt.figure()
lw = 2
plt.plot(false_pr, true_pr, color='darkorange', label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
84/12:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """
        if not hasattr(self, "coef_"):
            raise NotFittedError("Call fit before prediction")

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/137:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/138:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/139:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/140:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/141:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/142:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/143:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/144:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
prediction
83/145:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
print(prediction)

X_predit_proba = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_predit_proba)
84/13:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """
        if not hasattr(self, "coef_"):
            raise "Error: not hasattr(self, 'coef_')"

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/146:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/147:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/148:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/149:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/150:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/151:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/152:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/153:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/154:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/155:
# Import necessary modules
from sklearn.metrics import roc_curve
83/156:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
print(prediction)

X_predit_proba = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_predit_proba)
83/157:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
print(prediction)

X_predit_proba = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_predit_proba)
83/158:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/159:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/160:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/161:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/162:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/163:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/164:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/165:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/166:
# Import necessary modules
from sklearn.metrics import roc_curve
83/167:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
print(prediction)

X_predit_proba = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_predit_proba)
83/168:
# Predict
prediction = log_reg_gd.predict(X_test_scal_pca2)
print(prediction)
84/14:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
            
    from ..exceptions import NotFittedError, ConvergenceWarning, ChangedBehaviorWarning
    
    def score(self, X, y, sample_weight=None):
        """Returns the score using the `scoring` option on the given
        test data and labels.
        Parameters
        ----------
        X : array-like, shape = (n_samples, n_features)
            Test samples.
        y : array-like, shape = (n_samples,)
            True labels for X.
        sample_weight : array-like, shape = [n_samples], optional
            Sample weights.
        Returns
        -------
        score : float
            Score of self.predict(X) wrt. y.
        """

        if self.scoring is not None:
            warnings.warn("The long-standing behavior to use the "
                          "accuracy score has changed. The scoring "
                          "parameter is now used. "
                          "This warning will disappear in version 0.22.",
                          ChangedBehaviorWarning)
        scoring = self.scoring or 'accuracy'
        if isinstance(scoring, str):
            scoring = get_scorer(scoring)

        return scoring(self, X, y, sample_weight=sample_weight)

    
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
84/15: from ..exceptions import NotFittedError, ConvergenceWarning, ChangedBehaviorWarning
84/16: from exceptions import NotFittedError, ConvergenceWarning, ChangedBehaviorWarning
84/17:
"""
source code: https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/linear_model/logistic.py#L1624

The :mod:`sklearn.exceptions` module includes all custom warnings and error
classes used across scikit-learn.
"""

__all__ = ['NotFittedError',
           'ChangedBehaviorWarning',
           'ConvergenceWarning']


class NotFittedError(ValueError, AttributeError):
    """Exception class to raise if estimator is used before fitting.
    This class inherits from both ValueError and AttributeError to help with
    exception handling and backward compatibility.
    Examples
    --------
    >>> from sklearn.svm import LinearSVC
    >>> from sklearn.exceptions import NotFittedError
    >>> try:
    ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
    ... except NotFittedError as e:
    ...     print(repr(e))
    NotFittedError("This LinearSVC instance is not fitted yet. Call 'fit' with
    appropriate arguments before using this method."...)
    .. versionchanged:: 0.18
       Moved from sklearn.utils.validation.
    """


class ChangedBehaviorWarning(UserWarning):
    """Warning class used to notify the user of any change in the behavior.
    .. versionchanged:: 0.18
       Moved from sklearn.base.
    """


class ConvergenceWarning(UserWarning):
    """Custom warning to capture convergence problems
    .. versionchanged:: 0.18
       Moved from sklearn.utils.
    """
84/18: from exceptions import NotFittedError, ConvergenceWarning, ChangedBehaviorWarning
84/19:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

    def score(self, X, y, sample_weight=None):
        """Returns the score using the `scoring` option on the given
        test data and labels.
        Parameters
        ----------
        X : array-like, shape = (n_samples, n_features)
            Test samples.
        y : array-like, shape = (n_samples,)
            True labels for X.
        sample_weight : array-like, shape = [n_samples], optional
            Sample weights.
        Returns
        -------
        score : float
            Score of self.predict(X) wrt. y.
        """

        if self.scoring is not None:
            warnings.warn("The long-standing behavior to use the "
                          "accuracy score has changed. The scoring "
                          "parameter is now used. "
                          "This warning will disappear in version 0.22.",
                          ChangedBehaviorWarning)
        scoring = self.scoring or 'accuracy'
        if isinstance(scoring, str):
            scoring = get_scorer(scoring)

        return scoring(self, X, y, sample_weight=sample_weight)

    
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/169:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/170:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/171:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/172:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/173:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/174:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/175:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/176:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/177:
# Import necessary modules
from sklearn.metrics import roc_curve
83/178:
# Predict
X_test_prediction = log_reg_gd.predict(X_test_scal_pca2)
print(X_test_prediction)

X_text_probabpredict = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_text_probabpredict)
84/20:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1, multi_class=’warn’):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        self.multi_class = multi_class
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

    def score(self, X, y, sample_weight=None):
        """Returns the score using the `scoring` option on the given
        test data and labels.
        Parameters
        ----------
        X : array-like, shape = (n_samples, n_features)
            Test samples.
        y : array-like, shape = (n_samples,)
            True labels for X.
        sample_weight : array-like, shape = [n_samples], optional
            Sample weights.
        Returns
        -------
        score : float
            Score of self.predict(X) wrt. y.
        """

        if self.scoring is not None:
            warnings.warn("The long-standing behavior to use the "
                          "accuracy score has changed. The scoring "
                          "parameter is now used. "
                          "This warning will disappear in version 0.22.",
                          ChangedBehaviorWarning)
        scoring = self.scoring or 'accuracy'
        if isinstance(scoring, str):
            scoring = get_scorer(scoring)

        return scoring(self, X, y, sample_weight=sample_weight)

    
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
84/21:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn, pg. 66-68
"""

class LogisticRegressionGD(object):
    """Logistic Regression Classifier using gradient descent."""
    
    def __init__(self, eta=0.05, n_iter=100, random_state=1, multi_class='ovr'):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        self.multi_class = multi_class
        
    """
    Parameters
    ------------
    eta: float
        Learning rate (between 0.0 and 1.0)
    n_iter: int
        Passes over the training dataset.
    random_state: int
        Random number generator seed for random weight initialization. 
        
    Attributes
    ------------
    w_: 1d-array
        Weights after fitting.
    cost_: list
    
    Logistic cost function value in each epoch.
    """
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, z):
        """Compute logistic sigmoid activation"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
    
    def predict(self, X):
            """Return class label after unit step"""
            return np.where(self.net_input(X) >= 0.0, 1, 0)
            # equivalent to:
            # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

    def score(self, X, y, sample_weight=None):
        """Returns the score using the `scoring` option on the given
        test data and labels.
        Parameters
        ----------
        X : array-like, shape = (n_samples, n_features)
            Test samples.
        y : array-like, shape = (n_samples,)
            True labels for X.
        sample_weight : array-like, shape = [n_samples], optional
            Sample weights.
        Returns
        -------
        score : float
            Score of self.predict(X) wrt. y.
        """

        if self.scoring is not None:
            warnings.warn("The long-standing behavior to use the "
                          "accuracy score has changed. The scoring "
                          "parameter is now used. "
                          "This warning will disappear in version 0.22.",
                          ChangedBehaviorWarning)
        scoring = self.scoring or 'accuracy'
        if isinstance(scoring, str):
            scoring = get_scorer(scoring)

        return scoring(self, X, y, sample_weight=sample_weight)

    
    
    def predict_proba(self, X):
        """Probability estimates.
        
        The returned estimates for all classes are ordered by the
        label of classes.
        For a multi_class problem, if multi_class is set to be "multinomial"
        the softmax function is used to find the predicted probability of
        each class.
        
        Else use a one-vs-rest approach, i.e calculate the probability
        of each class assuming it to be positive using the logistic function.
        and normalize these values across all the classes.
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
        Returns
        -------
        T : array-like, shape = [n_samples, n_classes]
            Returns the probability of the sample for each class in the model,
            where classes are ordered as they are in ``self.classes_``.
        """

        ovr = (self.multi_class in ["ovr", "warn"] or
               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
                                                self.solver == 'liblinear')))
        if ovr:
            return super()._predict_proba_lr(X)
        else:
            decision = self.decision_function(X)
            if decision.ndim == 1:
                # Workaround for multi_class="multinomial" and binary outcomes
                # which requires softmax prediction with only a 1D decision.
                decision_2d = np.c_[-decision, decision]
            else:
                decision_2d = decision
            return softmax(decision_2d, copy=False)

    
    def fit(self, X, y):
        """Fit training data.
        
        Parameters
        ------------
        X: {array-like}, shape={n_samples, n_features}
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y: {array-like}, shape={n_samples}
            Target values.
            
        Returns
        ------------
        self: object
        
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic 'cost' now
            # instead of the sum of squared errors costs
            cost = (-y.dot(np.log(output))- 
                   ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            return self
83/179:
# Import necessary modules
%run gradient_logistic_regression.ipynb
83/180:
# Create a logistic regression instance that uses gradient descent
log_reg_gd = LogisticRegressionGD(eta=0.5, n_iter=1000, random_state=42)
83/181:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
83/182:
# Fit model to data
log_reg_gd.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/183:
# Import necessary modules
from matplotlib.colors import ListedColormap
83/184:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32

def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/185:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/186:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/187:
# Import necessary modules
from sklearn.metrics import roc_curve
83/188:
# Predict
X_test_prediction = log_reg_gd.predict(X_test_scal_pca2)
print(X_test_prediction)

X_text_probabpredict = log_reg_gd.predict_proba(X_test_scal_pca2)
print(X_text_probabpredict)
85/1:
# Create a General Plot Decision Regions
# Motification of original source code from'Python Machine Learning' textbook 
# Chapter: 'Training Simple Machine Learning Algorithms for Classigication', pg. 32
def plot_decision_regions(X, y, classifier, resolution=0.2):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        x=X[y == cl, 0]
        y=X[y == cl, 1]
        return x.size, y.size
        plt.scatter(x,y,
                    alpha=0.6,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/189:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/190:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/191:
# Predict for One Observation
print(log_reg_gd.predict(X_test_scal_pca2[0].reshape(1,-1)))

# Predict for One Observation
log_reg_gd.predict(X_test_scal_pca2[0:10])
83/192:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
83/193:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42)
83/194:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42)

# Fit training data to model
logreg.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/195:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs')

# Fit training data to model
logreg.fit(X_scal_pca2_train_01_subset, y_train_01_subset)
83/196:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs')

# Fit training data to model
logreg.fit(X_train_scal_pca2, y_train)
83/197:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg.fit(X_train_scal_pca2, y_train)
83/198:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 4 PCA of Audio Features')
83/199:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2, y=y_train, classifier=logreg)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scLogistic Regression of Music Genres using 4 PCA of Audio Features')
83/200:
# Plot decision regions
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scLogistic Regression of Music Genres using 4 PCA of Audio Features')
83/201:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/202:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg_2 = logreg.fit(X_train_scal_pca2, y_train)
logreg_4 = logreg.fit(X_train_scal_pca, y_train)
83/203:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg_2)
plt.subplot(221)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')

# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg_4)
plt.subplot(221)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
83/204:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg = logreg.fit(X_train_scal_pca2, y_train)
83/205:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg)
plt.subplot(221)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/206:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/207:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg2 = logreg.fit(X_train_scal_pca2, y_train)
83/208:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/209:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg2 = logreg.fit(X_train_scal_pca2, y_train) # 2 PCA
logreg4 = logreg.fit(X_train_scal_pca2, y_train) # 4 PCA
83/210:

# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg4)
plt.subplot(221)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
83/211:

# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg4)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
83/212:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/213:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/1:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/214:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/215:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
86/2:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        print(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/216:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/217:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/218: enumerate(np.unique(y_train_01_subset))
83/219:
#enumerate(np.unique(y_train_01_subset))

for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
83/220:
#enumerate(np.unique(y_train_01_subset))

for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X[y_train_01_subset == cl, 0],
                    y=X[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/221:
#enumerate(np.unique(y_train_01_subset))

for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/222:
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/223:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=c1,
                    edgecolor='black')
83/224:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=str(c1),
                    edgecolor='black')
83/225:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        print(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    
                    edgecolor='black')
86/3:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    edgecolor='black')
83/226:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/227:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/228:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/229:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/230:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        label_val = int(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=str(c1),
                    edgecolor='black')
83/231:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        label_val = int(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
86/4:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/232:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/233:
markers = ('s', 'x', 'o', '^', 'v')
colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
for idx, cl in enumerate(np.unique(y_train_01_subset)):
        label_val = int(cl)
        plt.scatter(x=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 0],
                    y=X_scal_pca2_train_01_subset[y_train_01_subset == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/234:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
86/5:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('white', 'grey', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/235:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/236:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
86/6:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('lightgreen', 'lightblue', 'lightred', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/237:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/238:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
86/7:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v')
    colors = ('lightgreen', 'lightblue', 'lightred', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/239:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/240:
# Plot decision regions
plot_decision_regions(X=X_scal_pca2_train_01_subset, y=y_train_01_subset, classifier=log_reg_gd)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('Logistic Regression using Gradient Descent of Music Genres using 2 PCA of Audio Features')
83/241:
# Import necessary modules
from sklearn.linear_model import LogisticRegression
83/242:
# Create a logreg instance with a hyperparameter of 100
logreg = LogisticRegression(C=100, random_state=42, solver='lbfgs', multi_class='auto')

# Fit training data to model
logreg2 = logreg.fit(X_train_scal_pca2, y_train) # 2 PCA
logreg4 = logreg.fit(X_train_scal_pca2, y_train) # 4 PCA
83/243:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/8:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v')
    colors = ('lightgreen', 'lightblue', 'pink', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/244:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/245:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/9:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v')
    colors = ('lightgreen', 'lightblue', 'pink', 'gray', 'cyan', 'violet', 'gold')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/246:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/247:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/248:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/10:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', '.', '-')
    colors = ('lightgreen', 'lightblue', 'pink', 'gray', 'cyan', 'violet', 'gold')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/249:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/250:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/11:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', '.', '_')
    colors = ('lightgreen', 'lightblue', 'pink', 'gray', 'cyan', 'violet', 'gold')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/251:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/252:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/253:
# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg4)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
86/12:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', '.', '_', 'H', '+', ',', '|', 'p' )
    colors = ('r', 'b', 'g', 'm', 'y', 'k', 'lightgreen', 'lightblue', 'pink', 'gray', 'cyan', 'violet', 'gold')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/254:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/255:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/256:
# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg4)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
86/13:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', '.', '_', 'H', '+', ',', '|', 'p', 'X')
    colors = ('r', 'b', 'g', 'm', 'y', 'k', 'lightgreen', 'lightblue', 'pink', 'gray', 'cyan', 'violet', 'gold', 'w')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/257:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/258:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/259:
for idx, cl in enumerate(np.unique(y_train)):
    print(idx)
86/14:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', 
               '.', '_', 'H', '+', ',',
               '|', 'p', 'X', '1', '2',
               '3', '4', '>', '<')
    colors = ('r', 'b', 'g', 'm', 'y',
              'k', 'lightgreen', 'lightblue', 'pink', 'gray', 
              'cyan', 'violet', 'gold', 'w', 'salmon',
              'olive', 'teal', 'steelblue', 'indigo')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/260:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/261:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
86/15:
"""
Python Machine Learning: Machine Learning and Deep Learning with Python
by Sebastian Raschka & Vahid Mirjalili

Chapter 2: Training Simple Machine Learning Algorithms for Classigication, pg. 32

**MODIFIED**

"""

def plot_decision_regions(X, y, classifier, resolution=0.2):
    """Create a General Plot Decision Regions"""
    
    # setup marker generator and color map
    markers = ('s', 'o', 'x', '^', 'v', 
               '.', '_', 'H', '+', ',',
               '|', 'p', 'X', '1', '2',
               '3', '4', '>', '<', '*')
    colors = ('r', 'b', 'g', 'm', 'y',
              'k', 'lightgreen', 'lightblue', 'pink', 'gray', 
              'cyan', 'violet', 'gold', 'w', 'salmon',
              'olive', 'teal', 'steelblue', 'indigo', 'lime')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                            np.arange(x2_min, x2_max, resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        label_val = int(cl)
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=label_val,
                    edgecolor='black')
83/262:
# Import necessary modules
from matplotlib.colors import ListedColormap
%run plot_decision_boundaries.ipynb
83/263:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/264:
# 4 PCA analysis
plot_decision_regions(X=X_train_scal_pca, y=y_train, classifier=logreg4)
plt.xlabel('4 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.title('scikit Logistic Regression of Music Genres using 4 PCA of Audio Features')
83/265:
# Plot decision regions

# 2 PCA analysis
plot_decision_regions(X=X_train_scal_pca2, y=y_train, classifier=logreg2)
plt.xlabel('2 PCA Component Features of Music Audio Data [standardized]')
plt.ylabel('Music Genre Target [standardized]')
plt.legend(loc='upper left')
plt.title('scikit Logistic Regression of Music Genres using 2 PCA of Audio Features')
83/266:
# Predict
X_test_prediction = log_reg.predict(X_test_scal_pca2)
print(X_test_prediction)

X_text_probabpredict = log_reg.predict_proba(X_test_scal_pca2)
print(X_text_probabpredict)
83/267:
# Predict
X_test_prediction = logreg2.predict(X_test_scal_pca2)
print(X_test_prediction)

X_text_probabpredict = logreg2.predict_proba(X_test_scal_pca2)
print(X_text_probabpredict)
83/268:
# Predict
X_test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
X_text_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, X_test_prediction)
print(class_report)
83/269:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
83/270:
# Predict
X_test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
X_text_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, X_test_prediction)
print(class_report)
83/271:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)

metrics.f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))
83/272:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)

logreg2.f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))
83/273:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, fl
83/274:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
83/275:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
logreg2.f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/276:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Get rough estimate of model accuracy score
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/277:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
f1_score = f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))
print(fl_score)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/278:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
f1score = f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))
print(flscore)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/279:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
model_f1score = f1_score(y_test, test_prediction, average='weighted', labels=np.unique(test_prediction))
print(model_f1score)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/280:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Calculate f1 score
model_f1score = f1_score(y_test, test_prediction, average='weighted', labels=np.unique(feature_list))
print(model_f1score)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/281:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/282:
# Compute the confusion_matrix to evaluate the accuracy of a classification
conf_matrix = confusion_matrix(y_test, test_prediction)
print(conf_matrix)

# Plot confusion_matrix
plt.figure(figsize = (10,7))
sns.heatmap(conf_matrix, annot=True)
83/283:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import seaborn as sns
83/284:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/285:
# Compute the confusion_matrix to evaluate the accuracy of a classification
conf_matrix = confusion_matrix(y_test, test_prediction)
print(conf_matrix)

# Plot confusion_matrix
plt.figure(figsize = (10,7))
sns.heatmap(conf_matrix, annot=True)
83/286:
# Compute the confusion_matrix to evaluate the accuracy of a classification
conf_matrix = confusion_matrix(y_test, test_prediction)

# Plot confusion_matrix
plt.figure(figsize = (10,7))
sns.heatmap(conf_matrix, annot=True)
83/287:
# Compute the confusion_matrix to evaluate the accuracy of a classification
conf_matrix = confusion_matrix(y_test, test_prediction)

# Plot confusion_matrix
plt.figure(figsize = (10,7))
sns.heatmap(conf_matrix)
83/288:
# Compute and print AUC scorey_pred_prob_scaled
print("AUC: {}".format(roc_auc_score(y_test, test_prediction)))

# Compute cross-validated AUC scores: cv_auc
cv_auc = cross_val_score(clf_log_scaled, X, y, scoring='roc_auc', cv=5)

# Print list of AUC scores
print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))
83/289:
# Import necessary modules
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
83/290:
# Predict
test_prediction = logreg2.predict(X_test_scal_pca2)

# Calculate the probability of model's ability to predict a likely value of the features
test_probability = logreg2.predict_proba(X_test_scal_pca2)

# Compute classification report (https://en.wikipedia.org/wiki/Precision_and_recall)
class_report = classification_report(y_test, test_prediction)
print(class_report)
83/291:
# Compute the confusion_matrix to evaluate the accuracy of a classification
conf_matrix = confusion_matrix(y_test, test_prediction)

# Plot confusion_matrix
plt.figure(figsize = (10,7))
sns.heatmap(conf_matrix)
83/292:
# Compute and print AUC scorey_pred_prob_scaled
print("AUC: {}".format(roc_auc_score(y_test, test_prediction)))

# Compute cross-validated AUC scores: cv_auc
cv_auc = cross_val_score(clf_log_scaled, X, y, scoring='roc_auc', cv=5)

# Print list of AUC scores
print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))
83/293:
# Computing false and true positive rates
false_pr, true_pr, thresholds = roc_curve(prediction, y_test, drop_intermediate=False)
83/294:
# Computing false and true positive rates
false_pr, true_pr, thresholds = roc_curve(prediction, y_test, drop_intermediate=False)
83/295:
# Computing false and true positive rates
false_pr, true_pr, thresholds = roc_curve(test_prediction, y_test, drop_intermediate=False)
83/296:
# Import necessary modules
import numpy as np
import matplotlib.pyplot as plt
83/297:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
83/298:
target_array = np.unique(genre_strings)
target_array
83/299:
target_num_array = np.unique(beats2['Genres'])
target_num_array
83/300:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Save feature column labels in list
feature_list = list(X.columns)

# Separate out the target
y = beats2['Genres'].values
83/301:
# Import necessary modules 
from sklearn.model_selection import train_test_split
83/302:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
83/303:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
83/304:
# Create a scaler instance
scaler = StandardScaler()
83/305:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
83/306:
# Show Training and Testing Data Set Shapes
print('Training (X) Features Shape:', X_train_scaled.shape)
print('Training (y) Target Shape:', y_train.shape)
print('Testing (X) Features Shape:', X_test_scaled.shape)
print('Testing (y) Target Shape:', y_test.shape)
83/307:
# Import necessary modules
from sklearn.ensemble import RandomForestRegressor
83/308:
# Instantiate model with 1000 decision trees and max depth of 3
rf = RandomForestRegressor(n_estimators=1000, random_state=42, max_depth=3)

# Train the model on training data
rf.fit(X_train_scaled, y_train)
83/309:
# Use the forest's predict method on the test data
predictions = rf.predict(X_test_scaled)

# Calculate the absolute errors
errors = abs(predictions - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
88/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
88/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
88/3:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
88/4:
# List column names
list(beats.columns)
88/5: beats.info()
88/6: beats.head(1)
88/7: list(beats['Genre'].unique())
88/8:
# Insertions with SQL 
import pandasql as ps
88/9:
music_query = """
select
  case 
    when Genre like '%metal%' then 'RocknRoll'
    when Genre like '%punk%' then 'RocknRoll'
    when Genre like '%rock%' then 'RocknRoll'
    when Genre like '%alternative%' then 'RocknRoll'
    when Genre like '%beat%' then 'RocknRoll'
    
    when Genre like '%trance%' then 'Electronic'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'Electronic'
    when Genre like '%dub%' then 'Electronic'
    when Genre like '%techno%' then 'Electronic'
    when Genre like '%chill%' then 'Electronic'
    when Genre like '%deep%' then 'Electronic'
    when Genre like '%rave%' then 'Electronic'
    
    when Genre like '%trap%' then 'HipHop'
    when Genre like '%hiphop%' then 'HipHop'
    
    when Genre like '%folk%' then 'Indie'
    when Genre like '%indie%' then 'Indie'
    
    when Genre like '%pop%' then 'Pop'
    
  end as Genre
from beats"""
88/10:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
88/11:
# Drop NaN values
beats = beats['Genres'].dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
88/12:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
88/13:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
88/14:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
88/15:
# List column names
list(beats.columns)
88/16: beats.info()
88/17: beats.head(1)
88/18: #list(beats['Genre'].unique())
88/19:
# Insertions with SQL 
import pandasql as ps
88/20:
music_query = """
select
  case 
    when Genre like '%metal%' then 'RocknRoll'
    when Genre like '%punk%' then 'RocknRoll'
    when Genre like '%rock%' then 'RocknRoll'
    when Genre like '%alternative%' then 'RocknRoll'
    when Genre like '%beat%' then 'RocknRoll'
    
    when Genre like '%trance%' then 'Electronic'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'Electronic'
    when Genre like '%dub%' then 'Electronic'
    when Genre like '%techno%' then 'Electronic'
    when Genre like '%chill%' then 'Electronic'
    when Genre like '%deep%' then 'Electronic'
    when Genre like '%rave%' then 'Electronic'
    
    when Genre like '%trap%' then 'HipHop'
    when Genre like '%hiphop%' then 'HipHop'
    
    when Genre like '%folk%' then 'Indie'
    when Genre like '%indie%' then 'Indie'
    
    when Genre like '%pop%' then 'Pop'
    
  end as Genre
from beats"""
88/21:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
88/22:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
88/23:
# Test output
beats.info()
88/24:
# Store as global variable
%store beats
88/25:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
88/26:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
88/27: beats.tail()
88/28:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
88/29:
# Install requirements.txt
!pip install -r requirements.txt
88/30:
# install watermark extension
!pip install --upgrade pip
!pip install watermark
88/31:
# Use a future note
%load_ext watermark
88/32: %watermark -a "Emily Schoof" -d -t -v -p numpy,pandas,seaborn,matplotlib,sklearn -g
88/33: %watermark -a "Emily Schoof" -d -t -v -p numpy,pandas,seaborn,matplotlib,sklearn
89/1:
# Import data
%store -r beats
89/2:
# Import necessary modules
import seaborn as sns; sns.set(style="darkgrid")
import matplotlib.pyplot as plt
import pylab as pl
89/3:
# View counts
ax = sns.catplot(x="Genres", data=beats, aspect=1.5, kind="count")
ax.fig.subplots_adjust(top=0.9)
ax.set_xticklabels(rotation=50)
ax.fig.suptitle('Number of Instances of Each Genre within Beats', fontsize=16)
plt.show()
89/4:
# Create a correlation dataframe
feature_corr = beats.corr()
feature_corr
89/5:
# Plot a correlation heatmap
sns.heatmap(feature_corr, square=True, cmap='RdYlGn')
89/6:
# Import necessary modules
import plotly # has packages for large datasets
import plotly.plotly as py
import plotly.graph_objs as go
import matplotlib.cm as cm
from IPython.display import IFrame
89/7:
# Define x and y variables
energy_y = abs(beats['Energy'])
loudness_y = abs(beats['Loudness'])
89/8:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/9:
# Define x and y variables
energy_y = beats['Energy']
loudness_y = beats['Loudness']
89/10:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/11:
# View graph
IFrame(src='//plot.ly/~emilyschoof/26.embed', width=600, height=500)
90/1:
# Import the relevant python libraries for the analysis
import pandas as pd
from pandas import DataFrame
import numpy as np
90/2:
# Load SongDb.tsv dataset - convert .tsv file to .csv for uploading
file_encoding = 'utf8'
input_fd = open('data/songDb.tsv', encoding=file_encoding, errors='backslashreplace')
beats = pd.read_csv(input_fd, delimiter='\t', low_memory=False)
beats.head()
90/3:
# List # of column, # of unique Genres, and total row length of dataset
len(beats.columns), len(beats.Genre.unique()), len(beats)
90/4:
# List column names
list(beats.columns)
90/5: beats.info()
90/6: beats.head(1)
90/7: list(beats['Genre'].unique())
91/1:
"""
Sourced from:
Parul Pandey
article: https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8
github: https://gist.github.com/parulnith/7f8c174e6ac099e86f0495d3d9a4c01e#file-music_genre_classification-ipynb

"""
91/2:
# feature extractoring and preprocessing data
import librosa
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import os
from PIL import Image
import pathlib
import csv

#Keras
import keras

import warnings
warnings.filterwarnings('ignore')
90/8: #list(beats['Genre'].unique())
90/9:
# Insertions with SQL 
import pandasql as ps
90/10:
music_query = """
select
  case 
    when Genre like '%metal%' then 'RocknRoll'
    when Genre like '%punk%' then 'RocknRoll'
    when Genre like '%rock%' then 'RocknRoll'
    when Genre like '%alternative%' then 'RocknRoll'
    when Genre like '%beat%' then 'RocknRoll'
    
    when Genre like '%trance%' then 'Electronic'
    when Genre like '%electro%' then 'Electronic'
    when Genre like '%house%' then 'Electronic'
    when Genre like '%dub%' then 'Electronic'
    when Genre like '%techno%' then 'Electronic'
    when Genre like '%chill%' then 'Electronic'
    when Genre like '%deep%' then 'Electronic'
    when Genre like '%rave%' then 'Electronic'
    
    when Genre like '%trap%' then 'HipHop'
    when Genre like '%hiphop%' then 'HipHop'
    
    when Genre like '%folk%' then 'Indie'
    when Genre like '%indie%' then 'Indie'
    
    when Genre like '%pop%' then 'Pop'
    
  end as Genre
from beats"""
90/11:
# Add modified Genres to beats
beats['Genres'] = ps.sqldf(music_query)
beats.tail()
90/12:
# Drop NaN values
beats = beats.dropna()

# Convert column values to numbers
beats['Tempo'] = pd.to_numeric(beats['Tempo'])
beats['time_signature'] = pd.to_numeric(beats['time_signature'])

# Drop unnecessary columns
beats = beats.drop(['Name','ID','Uri','Ref_Track','URL_features','Type', 'Genre'], axis=1)
90/13:
# Test output
beats.info()
90/14:
# Store as global variable
%store beats
90/15:
# Save order and list of beats Genre list
genre_strings = beats['Genres']
90/16:
# Convert Column value strings to a numeric value
for i, column in enumerate(list([str(d) for d in beats.dtypes])):
    if column == "object":
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].mode())
        beats[beats.columns[i]] = beats[beats.columns[i]].astype("category").cat.codes
    else:
        beats[beats.columns[i]] = beats[beats.columns[i]].fillna(beats[beats.columns[i]].median())
beats.head()
90/17: beats.tail()
90/18:
# Create copy of beats (beats2) and store as global variable
beats2 = beats.copy()
%store beats2
%store genre_strings
89/12:
# View graph
IFrame(src='//plot.ly/~emilyschoof/33.embed', width=600, height=500)
89/13:
# View graph
IFrame(src='//plot.ly/~emilyschoof/32.embed', width=600, height=500)
89/14:
# Define x and y variables
energy_y = beats['Genres']
loudness_y
89/15:
# Define x and y variables
energy_y = beats['Genres']
89/16:
# Define x and y variables
genres_y = beats['Genres']
89/17:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = energy_y,
                y = loudness_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/18:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = genres_y,
                y = abs(loudness_y),
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/19:
# View graph
IFrame(src='//plot.ly/~emilyschoof/37.embed', width=600, height=500)
89/20:
# View graph
IFrame(src='//plot.ly/~emilyschoof/36.embed', width=600, height=500)
89/21:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = genres_y,
                y = abs(energy_y),
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/22:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = genres_y,
                y = energy_y,
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/23:
# Define x and y variables
energy_y = beats['Energy']
loudness_y = beats['Loudness']
89/24:
# All graphs were plotted with the corresponding code with variations to X and Y variables
trace = go.Scattergl(x = genres_y,
                y = abs(energy_y),
                mode = 'markers',
                marker = dict(line = dict(width = 1))
                            )
data = [trace]    
layout = dict(showlegend=False)
fig=dict(data=data, layout=layout)
py.plot(fig)
89/25: IFrame(src='//plot.ly/~emilyschoof/40.embed', width=600, height=500)
92/1:
# Import necessary modules
import numpy as np
import matplotlib.pyplot as plt
92/2:
# Import data
%store -r beats2
%store -r genre_strings

beats2.head()
92/3:
target_array = np.unique(genre_strings)
target_array
92/4:
target_num_array = np.unique(beats2['Genres'])
target_num_array
92/5:
# Separate out the features
X = beats2.copy().drop(columns='Genres')

# Save feature column labels in list
feature_list = list(X.columns)

# Separate out the target
y = beats2['Genres'].values
92/6:
# Import necessary modules 
from sklearn.model_selection import train_test_split
92/7:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)
92/8:
# Import necessary modules
from sklearn.preprocessing import StandardScaler
92/9:
# Create a scaler instance
scaler = StandardScaler()
92/10:
# Fit on training features set *only*
scaler.fit(X_train)

# Apply transform to both the feature training and the test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
92/11:
# Show Training and Testing Data Set Shapes
print('Training (X) Features Shape:', X_train_scaled.shape)
print('Training (y) Target Shape:', y_train.shape)
print('Testing (X) Features Shape:', X_test_scaled.shape)
print('Testing (y) Target Shape:', y_test.shape)
92/12:
# Import necessary modules
from sklearn.ensemble import RandomForestRegressor
92/13:
# Instantiate model with 1000 decision trees and max depth of 3
rf = RandomForestRegressor(n_estimators=1000, random_state=42, max_depth=3)

# Train the model on training data
rf.fit(X_train_scaled, y_train)
92/14:
# Instantiate model with 100 decision trees and max depth of 3
rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=3)

# Train the model on training data
rf.fit(X_train_scaled, y_train)
92/15:
# Use the forest's predict method on the test data
predictions = rf.predict(X_test_scaled)

# Calculate the absolute errors
errors = abs(predictions - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))
92/16:
# Predict for One Observation
print(rf.predict(X_test_scaled[0].reshape(1,-1)))

# Predict for One Observation
rf.predict(X_test_scaled[0:10])
92/17:
# Import necessary modules
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydot
92/18:
# Extract the small tree
tree_small = rf.estimators_[5]

# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
92/19:
# Write graph to a png file
Image(graph.create_png())
92/20:
# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
92/21:
# Make an instance of the Model with 5 principle components, as shown in the Decision Tree
ml_pca = PCA(n_components=5)
92/22:
# Map (transform) to both the training set and the test set
X_train_scal_pca = ml_pca.fit_transform(X_train_scaled)
X_test_scal_pca = ml_pca.transform(X_test_scaled)
92/23:
# Determine number of components in model
ml_pca.n_components_
92/24:
# Define variable for variance - weights of PCA1 and PCA2
explained_pca = ml_pca.explained_variance_ratio_
pca1 = explained_pca[0]
pca2 = explained_pca[1]
pca3 = explained_pca[2]
pca4 = explained_pca[3]
pca5 = explained_pca[4]
print('Explained Variance Ratio:')
print('PCA1: ', pca1)
print('PCA2: ', pca2)
print('PCA3: ', pca3)
print('PCA4: ', pca4)
print('PCA5: ', pca5)
92/25:
# View bar plot
fig, ax = plt.subplots()
x = np.arange(2)
pl1 = plt.bar(x=0, height=pca1)
pl2 = plt.bar(x=1, height=pca2)
pl3 = plt.bar(x=2, height=pca3)
pl4 = plt.bar(x=3, height=pca4)
pl5 = plt.bar(x=4, height=pca5)

# Define labels
plt.title('Percentage of Variance Accounted for by Each PCA')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(0, 5, 1), ('PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'))
plt.show()
92/26:
# Make an instance of the Model with 2 principle components
ml_pca2 = PCA(n_components=2)

# Map (transform) to both the training set and the test set
X_train_scal_pca2 = ml_pca2.fit_transform(X_train_scaled)
X_test_scal_pca2 = ml_pca2.transform(X_test_scaled)
92/27:
# Define subsets of target and feature training data
X_scal_pca2_train_01_subset = X_train_scal_pca2[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

print('Subset Training (X) Features Shape:', X_scal_pca2_train_01_subset.shape)
print('Subset Training (y) Target Shape:', y_train_01_subset.shape)
92/28:
# Import necessary modules
%run gradient_logistic_regression.ipynb
   1: %history -g -f anyfilename
